# LangGraph Pre/Post Model Hooks ä½¿ç”¨æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

Pre/Post Model Hooks æ˜¯ LangGraph v2 ç‰ˆæœ¬å¼•å…¥çš„é«˜çº§ç‰¹æ€§ï¼Œå…è®¸åœ¨ LLM è°ƒç”¨å‰åæ’å…¥è‡ªå®šä¹‰å¤„ç†é€»è¾‘ã€‚æœ¬æŒ‡å—è¯¦ç»†è¯´æ˜å¦‚ä½•åœ¨ AutoAgents é¡¹ç›®ä¸­ä½¿ç”¨è¿™äº›ç‰¹æ€§æ¥ä¼˜åŒ–ä»£ç ç»“æ„å’Œæå‡å¯ç»´æŠ¤æ€§ã€‚

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### æ‰§è¡Œæµç¨‹

```
ç”¨æˆ·è¾“å…¥ 
  â†“
pre_model_hook (æ¶ˆæ¯é¢„å¤„ç†)
  â†“
Agent Node (LLM è°ƒç”¨)
  â†“
post_model_hook (å“åº”åå¤„ç†)
  â†“
å·¥å…·è°ƒç”¨ / æœ€ç»ˆè¾“å‡º
```

### ä¸¤ç§ Hook å¯¹æ¯”

| ç‰¹æ€§ | Pre-Model Hook | Post-Model Hook |
|------|----------------|-----------------|
| **æ‰§è¡Œæ—¶æœº** | LLM è°ƒç”¨å‰ | LLM è°ƒç”¨å |
| **ä¸»è¦ç”¨é€”** | æ¶ˆæ¯é¢„å¤„ç† | å“åº”åå¤„ç† |
| **å…¸å‹åœºæ™¯** | è£å‰ªã€æ‘˜è¦ã€ä¸Šä¸‹æ–‡æ³¨å…¥ | å®¡æ‰¹ã€éªŒè¯ã€æ—¥å¿— |
| **çŠ¶æ€æ›´æ–°** | messages / llm_input_messages | ä»»æ„çŠ¶æ€é”® |
| **æ”¯æŒä¸­æ–­** | âŒ | âœ… (HITL) |
| **è®¿é—® LLM å“åº”** | âŒ | âœ… |
| **ç‰ˆæœ¬è¦æ±‚** | v2 | v2 |

---

## 1ï¸âƒ£ Pre-Model Hook (å‰ç½®é’©å­)

### ç”¨é€”

åœ¨ **LLM è°ƒç”¨ä¹‹å‰** å¯¹æ¶ˆæ¯è¿›è¡Œé¢„å¤„ç†ï¼š

1. **æ¶ˆæ¯å†å²ç®¡ç†** - è£å‰ªè¿‡é•¿çš„å†å²æ¶ˆæ¯
2. **æ¶ˆæ¯æ‘˜è¦** - å¯¹å†å²å¯¹è¯è¿›è¡Œæ€»ç»“
3. **ä¸Šä¸‹æ–‡æ³¨å…¥** - åŠ¨æ€æ·»åŠ ç³»ç»Ÿæç¤ºæˆ–ç”¨æˆ·åå¥½
4. **æˆæœ¬ä¼˜åŒ–** - å‡å°‘ Token æ¶ˆè€—

### å‡½æ•°ç­¾å

```python
from typing import Dict, Any
from langchain_core.messages import BaseMessage, RemoveMessage, REMOVE_ALL_MESSAGES

def pre_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    å‚æ•°:
        state: å½“å‰ graph çŠ¶æ€
    
    è¿”å›:
        å¿…é¡»åŒ…å«ä»¥ä¸‹è‡³å°‘ä¸€ä¸ªé”®:
        - messages: æ›´æ–°çŠ¶æ€ä¸­çš„æ¶ˆæ¯åˆ—è¡¨ (æ°¸ä¹…ä¿®æ”¹)
        - llm_input_messages: ä»…ä¾› LLM ä½¿ç”¨ï¼Œä¸æ›´æ–°çŠ¶æ€ (ä¸´æ—¶ä¸Šä¸‹æ–‡)
    """
    return {
        "messages": [...],           # å¯é€‰: æ›´æ–°çŠ¶æ€
        "llm_input_messages": [...], # å¯é€‰: ä»…ä¾› LLM è¾“å…¥
        # å…¶ä»–éœ€è¦ä¼ æ’­çš„çŠ¶æ€é”®
    }
```

### ä¸¤ç§è¿”å›æ¨¡å¼

#### æ¨¡å¼ 1: æ°¸ä¹…ä¿®æ”¹çŠ¶æ€ (messages)

```python
def trim_messages_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """ä¿ç•™æœ€è¿‘ 10 æ¡æ¶ˆæ¯ï¼Œåˆ é™¤å…¶ä»–å†å²"""
    messages = state.get("messages", [])
    
    if len(messages) <= 10:
        return {}  # æ— éœ€å¤„ç†
    
    recent_messages = messages[-10:]
    
    return {
        # âš ï¸ é‡è¦: å¿…é¡»å…ˆæ¸…ç©ºå†æ·»åŠ æ–°æ¶ˆæ¯
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *recent_messages
        ]
    }
```

#### æ¨¡å¼ 2: ä¸´æ—¶ä¸Šä¸‹æ–‡æ³¨å…¥ (llm_input_messages)

```python
async def inject_user_context_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """ä¸´æ—¶æ³¨å…¥ç”¨æˆ·åå¥½ï¼Œä¸ä¿®æ”¹çŠ¶æ€"""
    messages = state.get("messages", [])
    user_id = state.get("user_id")
    
    # ä» Store è¯»å–ç”¨æˆ·åå¥½
    user_prefs = await get_user_preferences(user_id)
    
    context_message = SystemMessage(
        content=f"ç”¨æˆ·åå¥½: {user_prefs['language']}, {user_prefs['style']}"
    )
    
    return {
        # ä»…ä¾›æœ¬æ¬¡ LLM è°ƒç”¨ä½¿ç”¨
        "llm_input_messages": [context_message, *messages]
        # çŠ¶æ€ä¸­çš„ messages ä¿æŒä¸å˜
    }
```

### å®æˆ˜æ¡ˆä¾‹

#### æ¡ˆä¾‹ 1: æ™ºèƒ½æ‘˜è¦ + è£å‰ª

```python
async def smart_summarize_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¶…è¿‡ 20 æ¡æ¶ˆæ¯æ—¶ï¼Œæ‘˜è¦å‰ 10 æ¡ï¼Œä¿ç•™å 10 æ¡
    """
    messages = state.get("messages", [])
    
    if len(messages) <= 20:
        return {}
    
    # æ‘˜è¦å‰ 10 æ¡
    old_messages = messages[:-10]
    summary = await summarize_messages(old_messages)
    
    # ä¿ç•™å 10 æ¡
    recent_messages = messages[-10:]
    
    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            SystemMessage(content=f"å†å²æ‘˜è¦: {summary}"),
            *recent_messages
        ]
    }
```

#### æ¡ˆä¾‹ 2: é¡¹ç›®ä¸­çš„åº”ç”¨ (DeepResearch)

```python
# src/infrastructure/ai/graph/hooks/pre_hooks.py

from src.infrastructure.ai.graph.memory import initialize_store

async def deepresearch_pre_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    DeepResearch ä¸“ç”¨å‰ç½®å¤„ç†:
    1. æ¶ˆæ¯å†å²è£å‰ª
    2. ç”¨æˆ·åå¥½æ³¨å…¥ (ä» Store è¯»å–)
    """
    messages = state.get("messages", [])
    user_id = state.get("user_id")
    
    # 1. è£å‰ªå†å² (ä¿ç•™æœ€è¿‘ 15 æ¡)
    if len(messages) > 20:
        summary = await summarize_messages(messages[:-15])
        messages = [
            SystemMessage(content=f"å†å²æ‘˜è¦: {summary}"),
            *messages[-15:]
        ]
    
    # 2. æ³¨å…¥ç”¨æˆ·åå¥½ (ä¸´æ—¶ä¸Šä¸‹æ–‡)
    context_parts = []
    
    if user_id:
        store = initialize_store()
        prefs = await store.aget(
            namespace=("user_preferences", user_id),
            key="research_preferences"
        )
        
        if prefs and prefs.value:
            context_parts.append(
                f"ç”¨æˆ·ç ”ç©¶åå¥½: {json.dumps(prefs.value, ensure_ascii=False)}"
            )
    
    # æ·»åŠ é€šç”¨æç¤º
    context_parts.append("è¯·éµå¾ªç³»ç»ŸåŒ–ã€ç»“æ„åŒ–çš„è¾“å‡ºé£æ ¼")
    
    context_message = SystemMessage(
        content="\n".join(context_parts)
    )
    
    return {
        "llm_input_messages": [context_message, *messages]
    }
```

### âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

1. **è¦†ç›–æ¶ˆæ¯çš„æ­£ç¡®æ–¹å¼**

```python
# âŒ é”™è¯¯: ç›´æ¥è¦†ç›–ä¼šå¯¼è‡´çŠ¶æ€ä¸ä¸€è‡´
return {
    "messages": new_messages
}

# âœ… æ­£ç¡®: å¿…é¡»å…ˆæ¸…ç©ºå†æ·»åŠ 
return {
    "messages": [
        RemoveMessage(id=REMOVE_ALL_MESSAGES),
        *new_messages
    ]
}
```

2. **è‡³å°‘è¿”å›ä¸€ä¸ªæ¶ˆæ¯é”®**

```python
# âŒ é”™è¯¯: æ²¡æœ‰è¿”å› messages æˆ– llm_input_messages
return {
    "user_id": "123"
}

# âœ… æ­£ç¡®
return {
    "llm_input_messages": messages,
    "user_id": "123"
}
```

---

## 2ï¸âƒ£ Post-Model Hook (åç½®é’©å­)

### ç”¨é€”

åœ¨ **LLM è°ƒç”¨ä¹‹åã€å·¥å…·è°ƒç”¨ä¹‹å‰** è¿›è¡Œåå¤„ç†ï¼š

1. **äººæœºååŒ (HITL)** - å·¥å…·è°ƒç”¨å‰çš„äººå·¥å®¡æ‰¹
2. **å†…å®¹å®¡æŸ¥** - æ£€æµ‹æœ‰å®³/æ•æ„Ÿå†…å®¹
3. **å“åº”éªŒè¯** - ç¡®ä¿è¾“å‡ºç¬¦åˆæ ¼å¼è¦æ±‚
4. **æ—¥å¿—è®°å½•** - è®°å½• LLM å†³ç­–è¿‡ç¨‹
5. **åŠ¨æ€è·¯ç”±** - æ ¹æ® LLM è¾“å‡ºè°ƒæ•´å·¥ä½œæµ

### å‡½æ•°ç­¾å

```python
from langgraph.types import interrupt

def post_model_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    å‚æ•°:
        state: å½“å‰ graph çŠ¶æ€ (åŒ…å« LLM å“åº”)
    
    è¿”å›:
        çŠ¶æ€æ›´æ–°å­—å…¸
    """
    return {
        "messages": [...],  # å¯é€‰: ä¿®æ”¹æ¶ˆæ¯
        # å…¶ä»–çŠ¶æ€æ›´æ–°
    }
```

### å®æˆ˜æ¡ˆä¾‹

#### æ¡ˆä¾‹ 1: å·¥å…·è°ƒç”¨äººå·¥å®¡æ‰¹ (HITL)

```python
from langgraph import interrupt

def approval_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """å·¥å…·è°ƒç”¨å‰éœ€è¦äººå·¥æ‰¹å‡†"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å·¥å…·è°ƒç”¨
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        tool_calls = last_message.tool_calls
        
        # è§¦å‘ä¸­æ–­ï¼Œç­‰å¾…äººå·¥å†³ç­–
        decision = interrupt({
            "action": "approve_tools",
            "tool_calls": [
                {
                    "name": tc["name"],
                    "args": tc["args"]
                }
                for tc in tool_calls
            ],
            "message": "è¯·ç¡®è®¤æ˜¯å¦æ‰§è¡Œè¿™äº›å·¥å…·"
        })
        
        # æ ¹æ®å†³ç­–ä¿®æ”¹çŠ¶æ€
        if decision.get("approved"):
            return {}  # å…è®¸ç»§ç»­
        else:
            # æ‹’ç»å·¥å…·è°ƒç”¨
            return {
                "messages": [
                    AIMessage(content="å·¥å…·è°ƒç”¨è¢«æ‹’ç»")
                ]
            }
    
    return {}
```

#### æ¡ˆä¾‹ 2: å†…å®¹å®‰å…¨å®¡æŸ¥

```python
async def content_safety_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """æ£€æµ‹ LLM è¾“å‡ºæ˜¯å¦åŒ…å«æ•æ„Ÿå†…å®¹"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    if not last_message:
        return {}
    
    content = getattr(last_message, "content", "")
    
    # è°ƒç”¨å†…å®¹å®¡æŸ¥ API
    is_safe, reason = await check_content_safety(content)
    
    if not is_safe:
        # æ›¿æ¢ä¸ºå®‰å…¨æ¶ˆæ¯
        return {
            "messages": [
                RemoveMessage(id=last_message.id),
                AIMessage(content=f"æŠ±æ­‰ï¼Œæ— æ³•å¤„ç†è¯¥è¯·æ±‚ã€‚åŸå› : {reason}")
            ]
        }
    
    return {}
```

#### æ¡ˆä¾‹ 3: é¡¹ç›®ä¸­çš„åº”ç”¨ (ç»Ÿä¸€å·¥å…·å®¡æ‰¹)

```python
# src/infrastructure/ai/graph/hooks/post_hooks.py

def unified_tool_approval_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç»Ÿä¸€çš„å·¥å…·è°ƒç”¨å®¡æ‰¹é€»è¾‘
    æ ¹æ® auto å†³å®šæ˜¯å¦éœ€è¦äººå·¥ç¡®è®¤ï¼ˆauto=False æ‰éœ€è¦ï¼‰
    """
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    auto = state.get("auto")
    tool_mode = "copilot" if (auto is None or auto) else "interactive"
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨
    if not (hasattr(last_message, "tool_calls") and last_message.tool_calls):
        return {}
    
    tool_calls = last_message.tool_calls
    
    # Interactive æ¨¡å¼éœ€è¦å®¡æ‰¹
    if tool_mode == "interactive":
        # å±é™©å·¥å…·åˆ—è¡¨
        dangerous_tools = ["delete_file", "execute_code", "coding_tool"]
        
        needs_approval = any(
            tc["name"] in dangerous_tools 
            for tc in tool_calls
        )
        
        if needs_approval:
            decision = interrupt({
                "action": "approve_tools",
                "tool_calls": tool_calls,
                "config": {
                    "allow_accept": True,
                    "allow_reject": True,
                    "allow_edit": True
                }
            })
            
            if decision.get("type") == "reject":
                return {
                    "messages": [
                        AIMessage(content="å·¥å…·è°ƒç”¨å·²è¢«ç”¨æˆ·æ‹’ç»")
                    ]
                }
            elif decision.get("type") == "edit":
                # ä¿®æ”¹å·¥å…·å‚æ•° (éœ€è¦æ›´æ–° tool_calls)
                edited_args = decision.get("args", {})
                # å®ç°å‚æ•°ä¿®æ”¹é€»è¾‘...
                pass
    
    return {}
```

#### æ¡ˆä¾‹ 4: ç»“åˆ Writer è¾“å‡ºæ—¥å¿—

```python
from src.infrastructure.ai.graph.writer import create_stream_writer

def logging_post_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """è®°å½• LLM å†³ç­–å¹¶æµå¼è¾“å‡º"""
    writer = create_stream_writer("agent", "myagent")
    
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    if not last_message:
        return {}
    
    # è®°å½• LLM å“åº”
    content = getattr(last_message, "content", "")
    writer.thinking(content)
    
    # æ£€æŸ¥å·¥å…·è°ƒç”¨
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        for tc in last_message.tool_calls:
            writer.tool_call(
                tool_name=tc["name"],
                tool_args=tc["args"]
            )
    
    return {}  # ä¸ä¿®æ”¹çŠ¶æ€
```

---

## ğŸ”„ ç»„åˆä½¿ç”¨ Pre + Post Hooks

### å®Œæ•´ç¤ºä¾‹

```python
# src/infrastructure/ai/graph/hooks/__init__.py

from typing import Dict, Any
from langchain_core.messages import SystemMessage, AIMessage, RemoveMessage, REMOVE_ALL_MESSAGES
from langgraph.types import interrupt, Command
import json

async def unified_pre_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """ç»Ÿä¸€å‰ç½®å¤„ç†: æ¶ˆæ¯è£å‰ª + ç”¨æˆ·åå¥½æ³¨å…¥"""
    messages = state.get("messages", [])
    user_id = state.get("user_id")
    
    # 1. æ¶ˆæ¯è£å‰ª
    if len(messages) > 20:
        summary = await summarize_messages(messages[:-15])
        messages = [
            SystemMessage(content=f"å†å²æ‘˜è¦: {summary}"),
            *messages[-15:]
        ]
    
    # 2. ç”¨æˆ·åå¥½æ³¨å…¥
    user_prefs = await get_user_preferences(user_id)
    context = SystemMessage(
        content=f"ç”¨æˆ·åå¥½: {json.dumps(user_prefs, ensure_ascii=False)}"
    )
    
    return {
        "llm_input_messages": [context, *messages]
    }


def unified_post_hook(state: Dict[str, Any]) -> Dict[str, Any]:
    """ç»Ÿä¸€åç½®å¤„ç†: å·¥å…·å®¡æ‰¹ + å†…å®¹å®‰å…¨"""
    messages = state.get("messages", [])
    last_message = messages[-1] if messages else None
    
    if not last_message:
        return {}
    
    # 1. å†…å®¹å®‰å…¨æ£€æŸ¥
    content = getattr(last_message, "content", "")
    is_safe, reason = check_content_safety(content)
    
    if not is_safe:
        return {
            "messages": [
                AIMessage(content=f"æŠ±æ­‰ï¼Œæ— æ³•å¤„ç†è¯¥è¯·æ±‚ã€‚åŸå› : {reason}")
            ]
        }
    
    # 2. å·¥å…·è°ƒç”¨å®¡æ‰¹
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        auto = state.get("auto")
        tool_mode = "copilot" if (auto is None or auto) else "interactive"
        
        if tool_mode == "interactive":
            decision = interrupt({
                "action": "approve_tools",
                "tool_calls": last_message.tool_calls
            })
            
            if decision.get("type") == "reject":
                return {
                    "messages": [
                        AIMessage(content="å·¥å…·è°ƒç”¨è¢«æ‹’ç»")
                    ]
                }
    
    return {}


# å¯¼å‡º
__all__ = ["unified_pre_hook", "unified_post_hook"]
```

### åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨

```python
# src/application/services/graph/deepresearch/nodes.py

from langgraph.prebuilt import create_react_agent
from src.infrastructure.ai.graph.hooks import unified_pre_hook, unified_post_hook
from src.infrastructure.ai.graph.tools.wrapper import wrap_tools

async def react_agent_node(state: DeepResearchState) -> Dict[str, Any]:
    """ä¼˜åŒ–åçš„ React Agent èŠ‚ç‚¹ - ä½¿ç”¨ Hooks"""
    writer = create_stream_writer("react_agent", "deepresearch")
    
    # å‡†å¤‡å·¥å…·
    tools = await wrap_tools([web_search_tool], mode="copilot")
    
    # åˆ›å»ºå¸¦ hooks çš„ agent
    agent = create_react_agent(
        model=llm,
        tools=tools,
        pre_model_hook=unified_pre_hook,   # è‡ªåŠ¨æ¶ˆæ¯é¢„å¤„ç†
        post_model_hook=unified_post_hook,  # è‡ªåŠ¨å·¥å…·å®¡æ‰¹
        version="v2"  # å¿…é¡»ä½¿ç”¨ v2
    )
    
    # ç®€åŒ–çš„æ‰§è¡Œé€»è¾‘
    messages = state.get("messages", [])
    final = await writer.agent(agent, {"messages": messages})
    
    return {"answer": final}
```

---

## ğŸ“Š æœ€ä½³å®è·µ

### 1. å…³æ³¨ç‚¹åˆ†ç¦»

```python
# âœ… å¥½: æ¯ä¸ª hook ä¸“æ³¨ä¸€ä»¶äº‹
pre_hook = trim_messages_hook
post_hook = safety_check_hook

# âŒ å·®: ä¸€ä¸ª hook åšå¤ªå¤šäº‹
def mega_hook(state):
    # è£å‰ª + æ‘˜è¦ + å®¡æŸ¥ + æ—¥å¿— + ...
    pass
```

### 2. æ€§èƒ½ä¼˜åŒ–

```python
# âœ… å¥½: ä»…åœ¨å¿…è¦æ—¶å¤„ç†
def smart_pre_hook(state):
    messages = state.get("messages", [])
    if len(messages) < 10:
        return {}  # æ— éœ€å¤„ç†
    # ...

# âŒ å·®: æ¯æ¬¡éƒ½æ‰§è¡Œå¤æ‚é€»è¾‘
def heavy_hook(state):
    await expensive_operation()  # æ¯æ¬¡éƒ½è°ƒç”¨
```

### 3. é”™è¯¯å¤„ç†

```python
async def safe_hook(state):
    try:
        # Hook é€»è¾‘
        return await process(state)
    except Exception as e:
        logger.error(f"Hook failed: {e}")
        return {}  # è¿”å›ç©ºæ›´æ–°ï¼Œé¿å…ä¸­æ–­æµç¨‹
```

### 4. å¯æµ‹è¯•æ€§

```python
# hooks/tests/test_pre_hooks.py

async def test_pre_hook_message_trim():
    """æµ‹è¯•æ¶ˆæ¯è£å‰ª"""
    state = {
        "messages": [HumanMessage(content=f"msg{i}") for i in range(30)]
    }
    
    result = await unified_pre_hook(state)
    
    # éªŒè¯æ¶ˆæ¯æ•°é‡
    assert "llm_input_messages" in result
    assert len(result["llm_input_messages"]) <= 16  # æ‘˜è¦ + 15 æ¡
```

---

## ğŸ“ è¿ç§»æŒ‡å—

### ä»æ‰‹åŠ¨å¤„ç†è¿ç§»åˆ° Hooks

#### Before (æ‰‹åŠ¨å¤„ç†)

```python
async def my_node(state):
    messages = state.get("messages", [])
    
    # æ‰‹åŠ¨è£å‰ª
    if len(messages) > 20:
        messages = messages[-10:]
    
    # æ‰‹åŠ¨æ³¨å…¥ä¸Šä¸‹æ–‡
    user_prefs = await get_user_preferences(state.get("user_id"))
    context = SystemMessage(content=f"åå¥½: {user_prefs}")
    messages = [context, *messages]
    
    # è°ƒç”¨ LLM
    result = await llm.ainvoke(messages)
    
    # æ‰‹åŠ¨å®‰å…¨æ£€æŸ¥
    if not is_safe(result.content):
        return {"error": "å†…å®¹ä¸å®‰å…¨"}
    
    return {"answer": result.content}
```

#### After (ä½¿ç”¨ Hooks)

```python
async def my_node(state):
    agent = create_react_agent(
        model=llm,
        tools=tools,
        pre_model_hook=unified_pre_hook,   # è‡ªåŠ¨è£å‰ª + æ³¨å…¥
        post_model_hook=unified_post_hook,  # è‡ªåŠ¨å®‰å…¨æ£€æŸ¥
        version="v2"
    )
    
    final = await writer.agent(agent, {"messages": state.get("messages")})
    return {"answer": final}
```

### æ”¶ç›Šå¯¹æ¯”

| æŒ‡æ ‡ | Before | After | æ”¹è¿› |
|------|--------|-------|------|
| ä»£ç è¡Œæ•° | ~50 è¡Œ | ~15 è¡Œ | â¬‡ï¸ 70% |
| é‡å¤ä»£ç  | æ¯ä¸ªèŠ‚ç‚¹é‡å¤ | é›†ä¸­å¤ç”¨ | â¬†ï¸ 100% |
| å¯æµ‹è¯•æ€§ | å›°éš¾ | ç®€å• | â¬†ï¸ 80% |
| å¯ç»´æŠ¤æ€§ | ä½ | é«˜ | â¬†ï¸ 90% |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LangGraph Agents å¼€å‘æŒ‡å—](./langgraph-agents-development-guide.md)
- [LangGraph Human-in-the-Loop æŒ‡å—](./langgraph-human-in-the-loop-guide.md)
- [ä¼˜åŒ–è®°å½•](./optimized.md)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

1. **åˆ›å»º hooks æ¨¡å—**
   ```bash
   mkdir -p backend/src/infrastructure/ai/graph/hooks
   touch backend/src/infrastructure/ai/graph/hooks/__init__.py
   ```

2. **å®ç°åŸºç¡€ hooks** (å‚è€ƒä¸Šé¢çš„ç¤ºä¾‹)

3. **åœ¨èŠ‚ç‚¹ä¸­ä½¿ç”¨**
   ```python
   from src.infrastructure.ai.graph.hooks import unified_pre_hook, unified_post_hook
   ```

4. **ç¼–å†™æµ‹è¯•**
   ```bash
   mkdir -p backend/tests/unit/infrastructure/ai/graph/hooks
   ```

5. **æ›´æ–°æ–‡æ¡£**
   - åœ¨é¡¹ç›® README ä¸­æ·»åŠ  hooks ä½¿ç”¨è¯´æ˜
   - ä¸ºæ–° Graph æä¾›å¸¦ hooks çš„æ¨¡æ¿ä»£ç 

---

**æ›´æ–°è®°å½•**
- 2025-10-13: åˆ›å»ºæ–‡æ¡£ï¼Œè¯¦ç»†è¯´æ˜ Pre/Post Model Hooks çš„ä½¿ç”¨æ–¹æ³•å’Œæœ€ä½³å®è·µ
