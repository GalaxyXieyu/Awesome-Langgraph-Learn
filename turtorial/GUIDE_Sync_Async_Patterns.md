# LangGraph åŒæ­¥å¼‚æ­¥æ¨¡å¼æŒ‡å—

## âš¡ æ ¸å¿ƒå‘çŽ°

**é‡è¦ç»“è®º**: LLMçš„è°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæžœï¼ŒGraphçš„æµå¼æ¨¡å¼åªæ˜¯ä¼ è¾“ç®¡é“ã€‚

## ðŸ“Š äº”ç§ç»„åˆæ–¹å¼å¯¹æ¯”

| ç»„åˆæ–¹å¼ | å“åº”æ—¶é—´ | é¦–tokenæ—¶é—´ | æµå¼æ•ˆæžœ | æŽ¨èåº¦ |
|----------|----------|-------------|----------|--------|
| `def` + `invoke()` + `stream()` | 2.55s | - | âŒ å‡æµå¼ | ä¸æŽ¨è |
| `def` + `stream()` + `stream()` | 30.56s | 0.30s | âœ… çœŸæµå¼ | âœ… æŽ¨è |
| `def` + `invoke()` + `astream()` | 2.36s | - | âŒ å‡æµå¼ | ä¸æŽ¨è |
| `async def` + `ainvoke()` + `astream()` | 28.52s | - | âŒ éžæµå¼ | é«˜å¹¶å‘åœºæ™¯ |
| `async def` + `astream()` + `astream()` | 51.08s | 0.29s | ðŸ† å¼‚æ­¥æµå¼ | ðŸ† æœ€æŽ¨è |

## ðŸŽ¯ å†³ç­–æŒ‡å—

```
éœ€è¦æµå¼è¾“å‡ºï¼Ÿ
â”œâ”€ æ˜¯ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ ðŸ† async def + llm.astream() + graph.astream()
â”‚   â””â”€ å¦ â†’ âœ… def + llm.stream() + graph.stream()
â””â”€ å¦ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ âœ… async def + llm.ainvoke() + graph.astream()
    â””â”€ å¦ â†’ âš ï¸ def + llm.invoke() + graph.stream()
```

## ðŸ”§ å®žçŽ°æ¨¡æ¿

### ðŸ† æœ€æŽ¨èï¼šå¼‚æ­¥æµå¼è¾“å‡º
```python
async def async_streaming_node(state, config):
    llm = ChatOpenAI(...)
    
    full_response = ""
    # å¼‚æ­¥æµå¼è°ƒç”¨ - æœ€ä½³ç»„åˆ
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_response += chunk.content
            # Tokenä¼šè‡ªåŠ¨é€šè¿‡LangGraphæµå¼ä¼ è¾“
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
async for chunk in graph.astream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

### âœ… æŽ¨èï¼šåŒæ­¥æµå¼è¾“å‡º
```python
def streaming_node(state):
    llm = ChatOpenAI(...)
    
    full_response = ""
    for chunk in llm.stream(messages):  # å…³é”®ï¼šä½¿ç”¨stream()
        if chunk.content:
            full_response += chunk.content
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
for chunk in graph.stream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

### âœ… é«˜å¹¶å‘å¼‚æ­¥ï¼ˆéžæµå¼ï¼‰
```python
async def async_node(state, config):
    llm = ChatOpenAI(...)
    response = await llm.ainvoke(messages, config)  # ä¼ é€’config
    return {"result": response.content}

# è°ƒç”¨æ–¹å¼
async for chunk in graph.astream(state, stream_mode="updates"):
    print(chunk)
```

### âš ï¸ ç®€å•åŒæ­¥ï¼ˆéžæµå¼ï¼‰
```python
def sync_node(state):
    llm = ChatOpenAI(...)
    response = llm.invoke(messages)
    return {"result": response.content}

# è°ƒç”¨æ–¹å¼
for chunk in graph.stream(state, stream_mode="updates"):
    print(chunk)
```

## ðŸ’¡ æœ€ä½³å®žè·µ

### âœ… æŽ¨èåšæ³•

1. **å¼‚æ­¥ä¼˜å…ˆ**
```python
# é«˜å¹¶å‘åœºæ™¯ä½¿ç”¨å¼‚æ­¥
async def high_performance_node(state, config):
    tasks = [llm.ainvoke(msg, config) for msg in messages]
    results = await asyncio.gather(*tasks)
    return {"results": results}
```

2. **æµå¼ä¼˜å…ˆ**
```python
# éœ€è¦å®žæ—¶åé¦ˆæ—¶ä½¿ç”¨æµå¼
def real_time_node(state):
    for chunk in llm.stream(messages):
        if chunk.content:
            print(chunk.content, end="", flush=True)
```

3. **æ­£ç¡®ä¼ é€’config**
```python
# å¼‚æ­¥è°ƒç”¨å¿…é¡»ä¼ é€’config
async def correct_async_node(state, config):
    response = await llm.ainvoke(messages, config)  # âœ… æ­£ç¡®
    return {"result": response.content}
```

### âŒ é¿å…çš„åšæ³•

1. **å¼‚æ­¥çŽ¯å¢ƒä¸­çš„åŒæ­¥é˜»å¡ž**
```python
# âŒ é”™è¯¯ï¼šåŒæ­¥è°ƒç”¨é˜»å¡žå¼‚æ­¥çŽ¯å¢ƒ
def blocking_node(state):
    response = llm.invoke(messages)  # é˜»å¡žå¼‚æ­¥çŽ¯å¢ƒ
    return {"result": response.content}

# åœ¨å¼‚æ­¥çŽ¯å¢ƒä¸­ä½¿ç”¨
async for chunk in graph.astream(state):  # è¢«é˜»å¡ž
    print(chunk)
```

2. **å‡æµå¼è¾“å‡º**
```python
# âŒ é”™è¯¯ï¼šä»¥ä¸ºgraph.stream()å°±æ˜¯æµå¼
def fake_streaming_node(state):
    response = llm.invoke(messages)  # ä»ç„¶æ˜¯ä¸€æ¬¡æ€§è°ƒç”¨
    return {"result": response.content}

# å³ä½¿ä½¿ç”¨graph.stream()ï¼Œä»ç„¶ä¸æ˜¯çœŸæ­£æµå¼
for chunk in graph.stream(state):  # ç”¨æˆ·ä»éœ€ç­‰å¾…LLMå®Œæˆ
    print(chunk)
```

3. **å¿˜è®°ä¼ é€’config**
```python
# âŒ é”™è¯¯ï¼šå¼‚æ­¥è°ƒç”¨å¿˜è®°ä¼ é€’config
async def incorrect_async_node(state, config):
    response = await llm.ainvoke(messages)  # ç¼ºå°‘configå‚æ•°
    return {"result": response.content}
```

## ðŸ§ª æ€§èƒ½æµ‹è¯•

### æµ‹è¯•æµå¼æ•ˆæžœ
```python
import time

def test_streaming_effect():
    start_time = time.time()
    token_times = []
    
    for chunk in llm.stream(messages):
        if chunk.content:
            elapsed = time.time() - start_time
            token_times.append(elapsed)
            print(f"[{elapsed:.2f}s] Token: '{chunk.content}'")
    
    # æ£€æŸ¥æµå¼ç‰¹å¾
    if len(token_times) > 1:
        intervals = [token_times[i] - token_times[i-1] 
                    for i in range(1, len(token_times))]
        avg_interval = sum(intervals) / len(intervals)
        
        if avg_interval < 0.5:
            print("âœ… ç¡®è®¤ä¸ºçœŸæ­£çš„æµå¼è¾“å‡º")
        else:
            print("âš ï¸ å¯èƒ½ä¸æ˜¯çœŸæ­£çš„æµå¼è¾“å‡º")
```

### æ€§èƒ½åŸºå‡†æµ‹è¯•
```python
async def benchmark_async_vs_sync():
    # å¼‚æ­¥æµ‹è¯•
    start = time.time()
    async_result = await llm.ainvoke(messages)
    async_time = time.time() - start
    
    # åŒæ­¥æµ‹è¯•
    start = time.time()
    sync_result = llm.invoke(messages)
    sync_time = time.time() - start
    
    print(f"å¼‚æ­¥è€—æ—¶: {async_time:.2f}s")
    print(f"åŒæ­¥è€—æ—¶: {sync_time:.2f}s")
```

## ðŸŽ¯ åœºæ™¯é€‰æ‹©

### 1. èŠå¤©æœºå™¨äºº
```python
# æŽ¨èï¼šåŒæ­¥æµå¼
def chat_node(state):
    for chunk in llm.stream(messages):
        if chunk.content:
            print(chunk.content, end="", flush=True)
```

### 2. æ‰¹é‡å¤„ç†
```python
# æŽ¨èï¼šå¼‚æ­¥å¹¶å‘
async def batch_processing_node(state, config):
    tasks = [process_item(item, config) for item in state["items"]]
    results = await asyncio.gather(*tasks)
    return {"results": results}
```

### 3. å®žæ—¶å†™ä½œåŠ©æ‰‹
```python
# æŽ¨èï¼šå¼‚æ­¥æµå¼
async def writing_assistant_node(state, config):
    full_article = ""
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_article += chunk.content
            # å®žæ—¶æ˜¾ç¤ºå†™ä½œè¿›åº¦
    return {"article": full_article}
```

### 4. æ•°æ®åˆ†æž
```python
# æŽ¨èï¼šå¼‚æ­¥éžæµå¼
async def analysis_node(state, config):
    analysis = await llm.ainvoke(analysis_prompt, config)
    return {"analysis": analysis.content}
```

## ðŸ“‹ è°ƒè¯•æ£€æŸ¥æ¸…å•

- [ ] ç¡®è®¤LLMè°ƒç”¨æ–¹å¼ï¼ˆinvoke vs stream vs ainvoke vs astreamï¼‰
- [ ] éªŒè¯èŠ‚ç‚¹å‡½æ•°ç±»åž‹ï¼ˆdef vs async defï¼‰
- [ ] æ£€æŸ¥graphè°ƒç”¨æ–¹å¼ï¼ˆstream vs astreamï¼‰
- [ ] ç¡®è®¤configå‚æ•°ä¼ é€’ï¼ˆå¼‚æ­¥è°ƒç”¨å¿…éœ€ï¼‰
- [ ] æµ‹è¯•æµå¼æ•ˆæžœï¼ˆé¦–tokenæ—¶é—´ï¼‰
- [ ] éªŒè¯å¼‚æ­¥å…¼å®¹æ€§ï¼ˆæ— åŒæ­¥é˜»å¡žï¼‰

## ðŸŽ‰ æ€»ç»“

é€‰æ‹©åˆé€‚çš„åŒæ­¥å¼‚æ­¥ç»„åˆæ˜¯ä¼˜åŒ–LangGraphåº”ç”¨æ€§èƒ½å’Œç”¨æˆ·ä½“éªŒçš„å…³é”®ã€‚è®°ä½æ ¸å¿ƒåŽŸåˆ™ï¼š**LLMè°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæžœï¼Œå¼‚æ­¥è°ƒç”¨æå‡å¹¶å‘æ€§èƒ½**ã€‚
