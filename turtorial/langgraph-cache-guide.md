# LangGraph Cache ç¼“å­˜æœºåˆ¶å®Œæ•´æŒ‡å—

## æ¦‚è¿°

LangGraph çš„ç¼“å­˜ï¼ˆCacheï¼‰æœºåˆ¶æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œå…è®¸ä½ ç¼“å­˜èŠ‚ç‚¹ï¼ˆNodeï¼‰çš„è®¡ç®—ç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œè€—æ—¶æˆ–æ˜‚è´µçš„æ“ä½œã€‚é€šè¿‡åˆç†ä½¿ç”¨ç¼“å­˜ï¼Œå¯ä»¥æ˜¾è‘—æå‡åº”ç”¨çš„å“åº”é€Ÿåº¦å¹¶é™ä½ API è°ƒç”¨æˆæœ¬ã€‚

LangGraph æ”¯æŒä¸¤ç§ä¸»è¦çš„ç¼“å­˜ç±»å‹ï¼š
- **èŠ‚ç‚¹çº§ç¼“å­˜ï¼ˆNode-level Cachingï¼‰**ï¼šç¼“å­˜ç‰¹å®šèŠ‚ç‚¹çš„æ‰§è¡Œç»“æœ
- **LLM è°ƒç”¨ç¼“å­˜ï¼ˆLLM Call Cachingï¼‰**ï¼šç¼“å­˜å¤§è¯­è¨€æ¨¡å‹çš„è°ƒç”¨ç»“æœ

æœ¬æ–‡æ¡£ä¸»è¦å…³æ³¨ LangGraph çš„èŠ‚ç‚¹çº§ç¼“å­˜æœºåˆ¶ã€‚

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯èŠ‚ç‚¹çº§ç¼“å­˜ï¼Ÿ

èŠ‚ç‚¹çº§ç¼“å­˜ï¼ˆNode-level Cachingï¼‰æ˜¯ LangGraph æä¾›çš„ä¸€ç§æœºåˆ¶ï¼Œå®ƒå¯ä»¥åŸºäºèŠ‚ç‚¹çš„**è¾“å…¥**æ¥ç¼“å­˜èŠ‚ç‚¹çš„æ‰§è¡Œç»“æœã€‚å½“ç›¸åŒçš„è¾“å…¥å†æ¬¡ä¼ å…¥è¯¥èŠ‚ç‚¹æ—¶ï¼ŒLangGraph ä¼šç›´æ¥ä»ç¼“å­˜ä¸­è¿”å›ç»“æœï¼Œè€Œä¸æ˜¯é‡æ–°æ‰§è¡ŒèŠ‚ç‚¹å‡½æ•°ã€‚

### å·¥ä½œåŸç†

1. **ç¼“å­˜é”®ç”Ÿæˆ**ï¼šæ ¹æ®èŠ‚ç‚¹çš„è¾“å…¥ï¼ˆstateï¼‰ç”Ÿæˆç¼“å­˜é”®ï¼ˆé»˜è®¤ä½¿ç”¨è¾“å…¥çš„å“ˆå¸Œå€¼ï¼‰
2. **ç¼“å­˜æŸ¥æ‰¾**ï¼šæ‰§è¡ŒèŠ‚ç‚¹å‰ï¼Œå…ˆæ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å­˜åœ¨å¯¹åº”çš„ç»“æœ
3. **ç¼“å­˜å‘½ä¸­**ï¼šå¦‚æœæ‰¾åˆ°ç¼“å­˜ï¼Œç›´æ¥è¿”å›ç¼“å­˜ç»“æœ
4. **ç¼“å­˜æœªå‘½ä¸­**ï¼šå¦‚æœæœªæ‰¾åˆ°ç¼“å­˜ï¼Œæ‰§è¡ŒèŠ‚ç‚¹å‡½æ•°ï¼Œå¹¶å°†ç»“æœå­˜å…¥ç¼“å­˜
5. **ç¼“å­˜è¿‡æœŸ**ï¼šæ ¹æ®é…ç½®çš„ TTLï¼ˆTime To Liveï¼‰è‡ªåŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜

---

## ä½¿ç”¨åœºæ™¯

èŠ‚ç‚¹çº§ç¼“å­˜ç‰¹åˆ«é€‚åˆä»¥ä¸‹åœºæ™¯ï¼š

### 1. **è€—æ—¶çš„è®¡ç®—æ“ä½œ**
- å¤æ‚çš„æ•°æ®å¤„ç†å’Œåˆ†æ
- å¤§è§„æ¨¡æ•°æ®æŸ¥è¯¢
- æœºå™¨å­¦ä¹ æ¨¡å‹æ¨ç†

### 2. **æ˜‚è´µçš„ API è°ƒç”¨**
- ç¬¬ä¸‰æ–¹æœåŠ¡è°ƒç”¨ï¼ˆè®¡è´¹ APIï¼‰
- å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è°ƒç”¨
- æ•°æ®åº“å¯†é›†å‹æŸ¥è¯¢

### 3. **é‡å¤æ€§é«˜çš„æ“ä½œ**
- ç›¸åŒå‚æ•°çš„é‡å¤æŸ¥è¯¢
- é™æ€æ•°æ®çš„è·å–
- é…ç½®ä¿¡æ¯çš„åŠ è½½

### 4. **å¼€å‘å’Œæµ‹è¯•ç¯å¢ƒ**
- åŠ é€Ÿå¼€å‘è°ƒè¯•æµç¨‹
- å‡å°‘æµ‹è¯•ç¯å¢ƒçš„ API æˆæœ¬
- æé«˜å•å…ƒæµ‹è¯•æ‰§è¡Œæ•ˆç‡

---

## é…ç½®ä¸ä½¿ç”¨

### åŸºæœ¬é…ç½®

#### 1. å¯¼å…¥å¿…è¦çš„æ¨¡å—

```python
from langgraph.graph import StateGraph, START
from langgraph.checkpoint.memory import MemorySaver  # å†…å­˜ç¼“å­˜
from langgraph.types import CachePolicy
from typing_extensions import TypedDict
```

#### 2. å®šä¹‰ State

```python
class State(TypedDict):
    items: list[str]
    result: str
```

#### 3. é…ç½®ç¼“å­˜ç­–ç•¥å¹¶æ·»åŠ èŠ‚ç‚¹

```python
# åˆ›å»ºç¼“å­˜å®ä¾‹
cache = MemorySaver()

# åˆ›å»ºå›¾æ„å»ºå™¨
builder = StateGraph(State)

# å®šä¹‰èŠ‚ç‚¹å‡½æ•°ï¼ˆæ¨¡æ‹Ÿè€—æ—¶æ“ä½œï¼‰
def expensive_operation(state: State):
    """æ¨¡æ‹Ÿä¸€ä¸ªè€—æ—¶ 3 ç§’çš„æ“ä½œ"""
    import time
    time.sleep(3)
    return {"result": "å¤„ç†å®Œæˆ", "items": state["items"] + ["æ–°é¡¹ç›®"]}

# æ·»åŠ å¸¦ç¼“å­˜ç­–ç•¥çš„èŠ‚ç‚¹
builder.add_node(
    "expensive_node",
    expensive_operation,
    cache_policy=CachePolicy(ttl=120)  # TTL ä¸º 120 ç§’
)

# æ·»åŠ è¾¹
builder.add_edge(START, "expensive_node")

# ç¼–è¯‘å›¾å¹¶ä¼ å…¥ç¼“å­˜
graph = builder.compile(cache=cache)
```

#### 4. æ‰§è¡Œå›¾

```python
import time

# ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼Œè€—æ—¶çº¦ 3 ç§’ï¼‰
start = time.time()
result1 = graph.invoke({"items": ["é¡¹ç›®1"]})
print(f"ç¬¬ä¸€æ¬¡æ‰§è¡Œè€—æ—¶: {time.time() - start:.2f}ç§’")
# è¾“å‡º: ç¬¬ä¸€æ¬¡æ‰§è¡Œè€—æ—¶: 3.01ç§’

# ç¬¬äºŒæ¬¡æ‰§è¡Œï¼ˆç¼“å­˜å‘½ä¸­ï¼Œå‡ ä¹ç¬é—´è¿”å›ï¼‰
start = time.time()
result2 = graph.invoke({"items": ["é¡¹ç›®1"]})
print(f"ç¬¬äºŒæ¬¡æ‰§è¡Œè€—æ—¶: {time.time() - start:.4f}ç§’")
# è¾“å‡º: ç¬¬äºŒæ¬¡æ‰§è¡Œè€—æ—¶: 0.0050ç§’
```

---

### é«˜çº§é…ç½®

#### è‡ªå®šä¹‰ç¼“å­˜é”®å‡½æ•°ï¼ˆkey_funcï¼‰

æœ‰æ—¶ä½ å¯èƒ½éœ€è¦è‡ªå®šä¹‰ç¼“å­˜é”®çš„ç”Ÿæˆé€»è¾‘ï¼Œä¾‹å¦‚ï¼š
- å¿½ç•¥æŸäº›ä¸é‡è¦çš„å­—æ®µï¼ˆå¦‚éšæœº IDã€æ—¶é—´æˆ³ï¼‰
- åŸºäºç‰¹å®šå­—æ®µç”Ÿæˆç¼“å­˜é”®
- å®ç°è¯­ä¹‰ç›¸ä¼¼æ€§ç¼“å­˜

```python
from langgraph.graph import MessagesState
from langchain_core.messages import BaseMessage

def custom_key_func(args):
    """
    è‡ªå®šä¹‰ç¼“å­˜é”®å‡½æ•°
    åªåŸºäºæ¶ˆæ¯å†…å®¹å’Œä½ç½®ç”Ÿæˆç¼“å­˜é”®ï¼Œå¿½ç•¥æ¶ˆæ¯ ID
    """
    state = args[0]  # ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ state
    messages = state.get("messages", [])
    
    # åŸºäºæ¶ˆæ¯å†…å®¹å’Œç´¢å¼•ç”Ÿæˆé”®
    key_parts = []
    for idx, msg in enumerate(messages):
        if isinstance(msg, BaseMessage):
            key_parts.append(f"{idx}:{msg.content}")
        else:
            key_parts.append(f"{idx}:{msg}")
    
    import json
    return json.dumps(key_parts)

# ä½¿ç”¨è‡ªå®šä¹‰ç¼“å­˜é”®å‡½æ•°
builder.add_node(
    "smart_node",
    node_function,
    cache_policy=CachePolicy(
        ttl=300,  # 5 åˆ†é’Ÿ
        key_func=custom_key_func
    )
)
```

#### ä¸åŒçš„ç¼“å­˜åç«¯

LangGraph æ”¯æŒå¤šç§ç¼“å­˜åç«¯ï¼š

##### 1. å†…å­˜ç¼“å­˜ï¼ˆMemorySaverï¼‰

é€‚åˆå¼€å‘å’Œæµ‹è¯•ç¯å¢ƒï¼Œè¿›ç¨‹é‡å¯åç¼“å­˜ä¸¢å¤±ã€‚

```python
from langgraph.checkpoint.memory import MemorySaver

cache = MemorySaver()
graph = builder.compile(cache=cache)
```

##### 2. Redis ç¼“å­˜

é€‚åˆç”Ÿäº§ç¯å¢ƒï¼Œæ”¯æŒåˆ†å¸ƒå¼éƒ¨ç½²å’ŒæŒä¹…åŒ–ã€‚

```python
from langgraph_checkpoint_redis import RedisSaver
from redis import Redis

# è¿æ¥ Redis
redis_client = Redis(
    host="localhost",
    port=6379,
    db=0,
    decode_responses=True
)

# åˆ›å»º Redis ç¼“å­˜
cache = RedisSaver(redis_client)
graph = builder.compile(cache=cache)
```

##### 3. PostgreSQL ç¼“å­˜

é€‚åˆå·²ä½¿ç”¨ PostgreSQL çš„åº”ç”¨ï¼Œä¸æ•°æ®åº“é›†æˆæ›´ç´§å¯†ã€‚

```python
from langgraph.checkpoint.postgres import PostgresSaver

# åˆ›å»º PostgreSQL è¿æ¥
connection_string = "postgresql://user:password@localhost:5432/dbname"
cache = PostgresSaver.from_conn_string(connection_string)

graph = builder.compile(cache=cache)
```

---

## CachePolicy å‚æ•°è¯¦è§£

`CachePolicy` ç±»ç”¨äºé…ç½®èŠ‚ç‚¹çš„ç¼“å­˜ç­–ç•¥ï¼Œä¸»è¦å‚æ•°å¦‚ä¸‹ï¼š

### ttlï¼ˆTime To Liveï¼‰- ç¼“å­˜ç”Ÿå­˜æ—¶é—´è¯¦è§£

#### ä»€ä¹ˆæ˜¯ TTLï¼Ÿ

**TTLï¼ˆTime To Liveï¼Œç”Ÿå­˜æ—¶é—´ï¼‰** æ˜¯ç¼“å­˜æ•°æ®çš„"ä¿è´¨æœŸ"ã€‚å°±åƒé£Ÿå“æœ‰ä¿è´¨æœŸä¸€æ ·ï¼Œç¼“å­˜æ•°æ®ä¹Ÿéœ€è¦è®¾ç½®ä¸€ä¸ªæœ‰æ•ˆæœŸé™ã€‚

- **ç±»å‹**ï¼š`int`ï¼ˆç§’ï¼‰æˆ– `None`
- **å•ä½**ï¼šç§’ï¼ˆsecondsï¼‰
- **é»˜è®¤å€¼**ï¼š`None`ï¼ˆæ°¸ä¸è¿‡æœŸï¼Œä¸æ¨èï¼‰

#### TTL çš„å·¥ä½œåŸç†

```python
import time
from datetime import datetime, timedelta

# å½“ä½ è®¾ç½® TTL = 300ï¼ˆ5åˆ†é’Ÿï¼‰æ—¶ï¼š
cache_policy = CachePolicy(ttl=300)

# LangGraph ä¼šè¿™æ ·å¤„ç†ï¼š
created_at = datetime.now()  # å‡è®¾ï¼š2025-10-13 14:00:00
expires_at = created_at + timedelta(seconds=300)  # è¿‡æœŸæ—¶é—´ï¼š2025-10-13 14:05:00

# å­˜å‚¨åˆ°æ•°æ®åº“æ—¶ï¼š
cache_record = {
    "timestamp": "2025-10-13 14:00:00",
    "expires_at": "2025-10-13 14:05:00",  # 5åˆ†é’Ÿåè¿‡æœŸ
    "result": "ç¼“å­˜çš„ç»“æœ"
}

# è¯»å–ç¼“å­˜æ—¶æ£€æŸ¥ï¼š
if datetime.now() < expires_at:  # å½“å‰æ—¶é—´ < è¿‡æœŸæ—¶é—´
    return "ç¼“å­˜æœ‰æ•ˆï¼è¿”å›ç¼“å­˜ç»“æœ"
else:
    return "ç¼“å­˜å·²è¿‡æœŸï¼éœ€è¦é‡æ–°æ‰§è¡Œ"
```

#### TTL è®¾ç½®å»ºè®®

```python
# å®æ—¶æ•°æ®ï¼ˆå‡ ä¹ä¸ç¼“å­˜ï¼‰
CachePolicy(ttl=30)      # 30 ç§’ - è‚¡ç¥¨ä»·æ ¼ã€å®æ—¶å¤©æ°”

# çŸ­æœŸç¼“å­˜
CachePolicy(ttl=300)     # 5 åˆ†é’Ÿ - ç”¨æˆ·åœ¨çº¿çŠ¶æ€ã€ç®€å•æŸ¥è¯¢
CachePolicy(ttl=600)     # 10 åˆ†é’Ÿ - æ•°æ®åº“æŸ¥è¯¢ç»“æœ

# ä¸­æœŸç¼“å­˜  
CachePolicy(ttl=1800)    # 30 åˆ†é’Ÿ - LLM å¯¹è¯ç»“æœ
CachePolicy(ttl=3600)    # 1 å°æ—¶ - æ–‡ç« åˆ—è¡¨ã€å•†å“ä¿¡æ¯

# é•¿æœŸç¼“å­˜
CachePolicy(ttl=86400)   # 24 å°æ—¶ - é…ç½®ä¿¡æ¯ã€é™æ€æ•°æ®
CachePolicy(ttl=604800)  # 7 å¤© - å†å²æ•°æ®ã€å½’æ¡£ä¿¡æ¯

# æ°¸ä¹…ç¼“å­˜ï¼ˆæ…ç”¨ï¼ï¼‰
CachePolicy(ttl=None)    # æ°¸ä¸è¿‡æœŸ - ä»…ç”¨äºå®Œå…¨ä¸å˜çš„æ•°æ®
```

#### TTL è®¡ç®—åŠ©æ‰‹

```python
# ä¾¿æ·çš„æ—¶é—´è®¡ç®—
from datetime import timedelta

def calculate_ttl(**kwargs):
    """
    ä¾¿æ·è®¡ç®— TTL ç§’æ•°
    
    ç¤ºä¾‹ï¼š
        calculate_ttl(minutes=5)  # 300 ç§’
        calculate_ttl(hours=2)    # 7200 ç§’
        calculate_ttl(days=1)     # 86400 ç§’
    """
    delta = timedelta(**kwargs)
    return int(delta.total_seconds())

# ä½¿ç”¨ç¤ºä¾‹
CachePolicy(ttl=calculate_ttl(minutes=30))  # 30 åˆ†é’Ÿ
CachePolicy(ttl=calculate_ttl(hours=2))     # 2 å°æ—¶
CachePolicy(ttl=calculate_ttl(days=1))      # 1 å¤©
```

### key_funcï¼ˆç¼“å­˜é”®ç”Ÿæˆå‡½æ•°ï¼‰

- **ç±»å‹**ï¼š`Callable`
- **è¯´æ˜**ï¼šè‡ªå®šä¹‰ç¼“å­˜é”®ç”Ÿæˆé€»è¾‘
- **é»˜è®¤å€¼**ï¼šä½¿ç”¨è¾“å…¥çš„å“ˆå¸Œå€¼

```python
def my_key_func(args):
    state = args[0]
    # åªåŸºäºç‰¹å®šå­—æ®µç”Ÿæˆé”®
    return f"{state['user_id']}:{state['query']}"

CachePolicy(ttl=300, key_func=my_key_func)
```

---

## æœ€ä½³å®è·µ

### 1. åˆç†è®¾ç½® TTL

```python
# é™æ€æ•°æ®ï¼šé•¿ TTL
CachePolicy(ttl=86400)  # 24 å°æ—¶

# åŠé™æ€æ•°æ®ï¼šä¸­ç­‰ TTL
CachePolicy(ttl=3600)   # 1 å°æ—¶

# åŠ¨æ€æ•°æ®ï¼šçŸ­ TTL
CachePolicy(ttl=300)    # 5 åˆ†é’Ÿ

# å®æ—¶æ•°æ®ï¼šä¸ç¼“å­˜æˆ–æçŸ­ TTL
CachePolicy(ttl=30)     # 30 ç§’
```

### 2. é€‰æ‹©åˆé€‚çš„ç¼“å­˜åç«¯

| åœºæ™¯ | æ¨èç¼“å­˜åç«¯ | åŸå›  |
|------|------------|------|
| æœ¬åœ°å¼€å‘ | MemorySaver | ç®€å•å¿«é€Ÿ |
| ç”Ÿäº§ç¯å¢ƒï¼ˆå•æœºï¼‰ | PostgresSaver | æŒä¹…åŒ–ï¼Œä¸ä¸»æ•°æ®åº“é›†æˆ |
| ç”Ÿäº§ç¯å¢ƒï¼ˆåˆ†å¸ƒå¼ï¼‰ | RedisSaver | æ”¯æŒé›†ç¾¤ï¼Œé«˜æ€§èƒ½ |
| æµ‹è¯•ç¯å¢ƒ | MemorySaver | å¿«é€Ÿæ¸…ç†ï¼Œéš”ç¦»æ€§å¥½ |

### 3. é¿å…ç¼“å­˜çš„åœºæ™¯

ä»¥ä¸‹åœºæ™¯**ä¸å»ºè®®**ä½¿ç”¨ç¼“å­˜ï¼š

- åŒ…å«éšæœºæ€§çš„æ“ä½œï¼ˆå¦‚ç”Ÿæˆéšæœºæ•°ã€UUIDï¼‰
- ä¾èµ–å½“å‰æ—¶é—´çš„è®¡ç®—
- æœ‰å‰¯ä½œç”¨çš„æ“ä½œï¼ˆå¦‚æ•°æ®åº“å†™å…¥ã€æ–‡ä»¶ä¿®æ”¹ï¼‰
- ç”¨æˆ·ç‰¹å®šçš„æ•æ„Ÿæ“ä½œï¼ˆéœ€è¦å®æ—¶æƒé™æ£€æŸ¥ï¼‰

### 4. ç›‘æ§ç¼“å­˜æ€§èƒ½

```python
import time
import logging

def monitored_node(state):
    start = time.time()
    # æ‰§è¡Œå®é™…é€»è¾‘
    result = expensive_operation(state)
    duration = time.time() - start
    
    logging.info(f"èŠ‚ç‚¹æ‰§è¡Œè€—æ—¶: {duration:.4f}ç§’")
    return result
```

### 5. ç¼“å­˜å¤±æ•ˆç­–ç•¥

```python
from datetime import datetime

def cache_key_with_version(args):
    """å¸¦ç‰ˆæœ¬å·çš„ç¼“å­˜é”®ï¼Œæ–¹ä¾¿å¼ºåˆ¶åˆ·æ–°"""
    state = args[0]
    version = "v1.0.0"  # ä¿®æ”¹ç‰ˆæœ¬å·å¯å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å­˜
    return f"{version}:{hash(str(state))}"

CachePolicy(ttl=3600, key_func=cache_key_with_version)
```

---

## å®é™…æ¡ˆä¾‹

### æ¡ˆä¾‹ 1ï¼šç¼“å­˜ LLM è°ƒç”¨

```python
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, MessagesState, START
from langgraph.types import CachePolicy
from langgraph.checkpoint.memory import MemorySaver

# å®šä¹‰å¸¦ LLM çš„èŠ‚ç‚¹
def llm_node(state: MessagesState):
    llm = ChatOpenAI(model="gpt-4")
    response = llm.invoke(state["messages"])
    return {"messages": [response]}

# æ„å»ºå›¾
builder = StateGraph(MessagesState)
builder.add_node(
    "llm_call",
    llm_node,
    cache_policy=CachePolicy(
        ttl=1800,  # 30 åˆ†é’Ÿ
        key_func=lambda args: str(args[0]["messages"][-1].content)
    )
)
builder.add_edge(START, "llm_call")

graph = builder.compile(cache=MemorySaver())

# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼ˆå®é™…è°ƒç”¨ APIï¼‰
result1 = graph.invoke({"messages": [("user", "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ")]})

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼ˆä»ç¼“å­˜è¿”å›ï¼ŒèŠ‚çœè´¹ç”¨ï¼‰
result2 = graph.invoke({"messages": [("user", "ä»€ä¹ˆæ˜¯ LangGraphï¼Ÿ")]})
```

### æ¡ˆä¾‹ 2ï¼šç¼“å­˜æ•°æ®åº“æŸ¥è¯¢

```python
def database_query_node(state):
    """æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢"""
    import psycopg2
    conn = psycopg2.connect("dbname=mydb")
    cursor = conn.cursor()
    
    query = state["query"]
    cursor.execute(query)
    results = cursor.fetchall()
    
    cursor.close()
    conn.close()
    
    return {"results": results}

# æ·»åŠ ç¼“å­˜ç­–ç•¥
builder.add_node(
    "db_query",
    database_query_node,
    cache_policy=CachePolicy(
        ttl=600,  # 10 åˆ†é’Ÿ
        key_func=lambda args: args[0]["query"]  # åŸºäº SQL æŸ¥è¯¢ç¼“å­˜
    )
)
```

### æ¡ˆä¾‹ 3ï¼šæ¡ä»¶æ€§ç¼“å­˜

```python
def conditional_cache_key(args):
    """
    æ¡ä»¶æ€§ç¼“å­˜ï¼šåªå¯¹ç‰¹å®šç±»å‹çš„è¯·æ±‚å¯ç”¨ç¼“å­˜
    """
    state = args[0]
    
    # å¯¹åªè¯»æ“ä½œå¯ç”¨ç¼“å­˜
    if state.get("operation") == "read":
        return f"read:{state['query']}"
    
    # å¯¹å†™æ“ä½œè¿”å›å”¯ä¸€é”®ï¼ˆä¸ä¼šå‘½ä¸­ç¼“å­˜ï¼‰
    import uuid
    return str(uuid.uuid4())

CachePolicy(ttl=300, key_func=conditional_cache_key)
```

---

## ä¸ LangChain LLM Cache çš„åŒºåˆ«

LangGraph çš„èŠ‚ç‚¹çº§ç¼“å­˜ä¸ LangChain çš„ LLM Cache æ˜¯ä¸åŒçš„ï¼š

| ç‰¹æ€§ | LangGraph Node Cache | LangChain LLM Cache |
|------|---------------------|---------------------|
| ç¼“å­˜ç²’åº¦ | èŠ‚ç‚¹çº§åˆ« | LLM è°ƒç”¨çº§åˆ« |
| é…ç½®ä½ç½® | å›¾ç¼–è¯‘æ—¶ | å…¨å±€è®¾ç½® |
| ç¼“å­˜å†…å®¹ | æ•´ä¸ªèŠ‚ç‚¹çš„è¾“å‡º | LLM å“åº” |
| é€‚ç”¨èŒƒå›´ | ä»»ä½•èŠ‚ç‚¹å‡½æ•° | ä»… LLM è°ƒç”¨ |
| TTL æ”¯æŒ | âœ… | âœ… |
| è‡ªå®šä¹‰é”® | âœ… | éƒ¨åˆ†æ”¯æŒ |

ä¸¤è€…å¯ä»¥ç»“åˆä½¿ç”¨ï¼š

```python
from langchain.globals import set_llm_cache
from langchain_community.cache import RedisCache

# è®¾ç½® LangChain çš„ LLM ç¼“å­˜
set_llm_cache(RedisCache(redis_=redis_client))

# åŒæ—¶ä½¿ç”¨ LangGraph çš„èŠ‚ç‚¹ç¼“å­˜
graph = builder.compile(cache=MemorySaver())
```

---

## ç¼“å­˜è‡ªåŠ¨åˆ é™¤æœºåˆ¶è¯¦è§£

### ä»€ä¹ˆèŠ‚ç‚¹ä¼šè¢«è‡ªåŠ¨åˆ é™¤ï¼Ÿ

å¹¶ä¸æ˜¯"èŠ‚ç‚¹"è¢«åˆ é™¤ï¼Œè€Œæ˜¯**èŠ‚ç‚¹çš„ç¼“å­˜ç»“æœ**ä¼šè¢«è‡ªåŠ¨åˆ é™¤ã€‚å…·ä½“æ¥è¯´ï¼š

#### 1. **è¿‡æœŸçš„ç¼“å­˜è®°å½•**

å½“ç¼“å­˜çš„ TTL æ—¶é—´åˆ°æœŸåï¼Œè¯¥ç¼“å­˜è®°å½•ä¼šè¢«æ ‡è®°ä¸º"å·²è¿‡æœŸ"å¹¶åœ¨åç»­è¢«åˆ é™¤ã€‚

```python
# ä¾‹å­ï¼šè®¾ç½® TTL = 300 ç§’ï¼ˆ5 åˆ†é’Ÿï¼‰
builder.add_node(
    "weather_node",
    get_weather_function,
    cache_policy=CachePolicy(ttl=300)  # 5 åˆ†é’Ÿåè¿‡æœŸ
)

# æ—¶é—´çº¿ï¼š
# 14:00:00 - ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼Œç¼“å­˜ç»“æœï¼Œexpires_at = 14:05:00
# 14:03:00 - ç¬¬äºŒæ¬¡æ‰§è¡Œï¼Œç¼“å­˜æœªè¿‡æœŸï¼Œç›´æ¥è¿”å›ç¼“å­˜
# 14:06:00 - ç¬¬ä¸‰æ¬¡æ‰§è¡Œï¼Œç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°æ‰§è¡Œå¹¶æ›´æ–°ç¼“å­˜
#            æ—§çš„ç¼“å­˜è®°å½•ä¼šè¢«åˆ é™¤æˆ–è¦†ç›–
```

#### 2. **è¢«è¦†ç›–çš„ç¼“å­˜è®°å½•**

å½“ç›¸åŒçš„è¾“å…¥å†æ¬¡æ‰§è¡Œæ—¶ï¼Œæ–°çš„ç»“æœä¼šè¦†ç›–æ—§çš„ç¼“å­˜ã€‚

```python
# PostgreSQL ä¸­çš„ UPSERT æ“ä½œ
INSERT INTO checkpoints (...)
VALUES (...)
ON CONFLICT (thread_id, checkpoint_ns, checkpoint_id)
DO UPDATE SET  -- è¦†ç›–æ—§è®°å½•
    checkpoint = EXCLUDED.checkpoint,
    metadata = EXCLUDED.metadata;
```

---

### ä»€ä¹ˆæ—¶å€™ä¼šè‡ªåŠ¨æ‰§è¡Œåˆ é™¤ï¼Ÿ

ç¼“å­˜åˆ é™¤æœ‰ **3 ç§è§¦å‘æ—¶æœº**ï¼š

#### è§¦å‘æ—¶æœº 1ï¼šè¯»å–æ—¶æ£€æŸ¥ï¼ˆæƒ°æ€§åˆ é™¤ï¼‰

```python
# æ¯æ¬¡è¯»å–ç¼“å­˜æ—¶éƒ½ä¼šæ£€æŸ¥æ˜¯å¦è¿‡æœŸ
async def check_cache(cache_key, node_name):
    # ä»æ•°æ®åº“è¯»å–ç¼“å­˜
    record = await db.query(
        "SELECT * FROM checkpoints WHERE thread_id = %s",
        f"cache_{cache_key}"
    )
    
    if record:
        expires_at = record['metadata']['expires_at']
        
        # æ£€æŸ¥æ˜¯å¦è¿‡æœŸ
        if datetime.now() > datetime.fromisoformat(expires_at):
            # âœ… å‘ç°è¿‡æœŸï¼Œç«‹å³åˆ é™¤
            await db.execute(
                "DELETE FROM checkpoints WHERE thread_id = %s",
                f"cache_{cache_key}"
            )
            return None  # è¿”å›ç©ºï¼Œè§¦å‘é‡æ–°æ‰§è¡Œ
        
        return record['checkpoint']['result']  # è¿”å›ç¼“å­˜
    
    return None
```

**ç‰¹ç‚¹**ï¼š
- åªåœ¨è®¿é—®æ—¶æ£€æŸ¥
- ä¸ä¼šä¸»åŠ¨æ‰«ææ‰€æœ‰ç¼“å­˜
- æ€§èƒ½å¼€é”€å°

#### è§¦å‘æ—¶æœº 2ï¼šå®šæœŸæ¸…ç†ä»»åŠ¡ï¼ˆä¸»åŠ¨åˆ é™¤ï¼‰

```python
import asyncio
from datetime import datetime

async def scheduled_cache_cleanup(saver, interval_seconds=3600):
    """
    å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
    é»˜è®¤æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    """
    while True:
        print(f"ğŸ§¹ å¼€å§‹æ¸…ç†è¿‡æœŸç¼“å­˜ - {datetime.now()}")
        
        # æ‰¹é‡åˆ é™¤æ‰€æœ‰è¿‡æœŸè®°å½•
        result = await saver.conn.execute("""
            DELETE FROM checkpoints 
            WHERE checkpoint_ns = 'node_cache'
              AND (metadata->>'expires_at')::timestamp < NOW()
        """)
        
        deleted_count = result.split()[-1]  # æå–åˆ é™¤æ•°é‡
        print(f"âœ… æ¸…ç†å®Œæˆï¼Œåˆ é™¤äº† {deleted_count} æ¡è¿‡æœŸç¼“å­˜")
        
        # ç­‰å¾…ä¸‹æ¬¡æ¸…ç†
        await asyncio.sleep(interval_seconds)

# å¯åŠ¨æ¸…ç†ä»»åŠ¡ï¼ˆåœ¨åå°è¿è¡Œï¼‰
asyncio.create_task(scheduled_cache_cleanup(saver, interval_seconds=3600))
```

**ç‰¹ç‚¹**ï¼š
- å®šæœŸæ‰«ææ‰€æœ‰ç¼“å­˜
- æ‰¹é‡åˆ é™¤è¿‡æœŸè®°å½•
- é˜²æ­¢æ•°æ®åº“è†¨èƒ€

#### è§¦å‘æ—¶æœº 3ï¼šå†™å…¥æ—¶è¦†ç›–ï¼ˆæ›´æ–°åˆ é™¤ï¼‰

```python
# å½“æ–°çš„æ‰§è¡Œç»“æœäº§ç”Ÿæ—¶
async def store_cache(cache_key, node_name, new_result):
    # ä½¿ç”¨ UPSERT æ“ä½œ
    await db.execute("""
        INSERT INTO checkpoints (
            thread_id, checkpoint_ns, checkpoint_id,
            checkpoint, metadata
        ) VALUES (%s, %s, %s, %s, %s)
        ON CONFLICT (thread_id, checkpoint_ns, checkpoint_id)
        DO UPDATE SET
            checkpoint = EXCLUDED.checkpoint,  -- æ–°ç»“æœè¦†ç›–æ—§ç»“æœ
            metadata = EXCLUDED.metadata       -- æ–°çš„è¿‡æœŸæ—¶é—´
    """, ...)
    
    # ç»“æœï¼šæ—§ç¼“å­˜è¢«"è¦†ç›–åˆ é™¤"
```

**ç‰¹ç‚¹**ï¼š
- ç›¸åŒè¾“å…¥äº§ç”Ÿæ–°ç»“æœæ—¶è§¦å‘
- æ—§æ•°æ®è¢«æ–°æ•°æ®æ›¿æ¢
- è‡ªåŠ¨æ›´æ–°è¿‡æœŸæ—¶é—´

---

### å®Œæ•´çš„åˆ é™¤æµç¨‹å›¾

```mermaid
graph TB
    A[èŠ‚ç‚¹æ‰§è¡Œ] --> B{æ£€æŸ¥ç¼“å­˜}
    B -->|ç¼“å­˜å­˜åœ¨| C{æ£€æŸ¥TTL}
    C -->|æœªè¿‡æœŸ| D[è¿”å›ç¼“å­˜ç»“æœ]
    C -->|å·²è¿‡æœŸ| E[åˆ é™¤è¿‡æœŸç¼“å­˜]
    E --> F[é‡æ–°æ‰§è¡ŒèŠ‚ç‚¹]
    B -->|ç¼“å­˜ä¸å­˜åœ¨| F
    F --> G[å­˜å‚¨æ–°ç¼“å­˜]
    G --> H[è®¾ç½®è¿‡æœŸæ—¶é—´]
    
    I[å®šæœŸæ¸…ç†ä»»åŠ¡] --> J[æ‰«ææ‰€æœ‰ç¼“å­˜]
    J --> K{æ£€æŸ¥è¿‡æœŸæ—¶é—´}
    K -->|å·²è¿‡æœŸ| L[æ‰¹é‡åˆ é™¤]
    K -->|æœªè¿‡æœŸ| M[ä¿ç•™]
    L --> N[ä¸‹æ¬¡æ¸…ç†]
    M --> N
```

---

### å®é™…åˆ é™¤ç¤ºä¾‹

#### ç¤ºä¾‹ 1ï¼šTTL è¿‡æœŸè‡ªåŠ¨åˆ é™¤

```python
from langgraph.types import CachePolicy
import time

# è®¾ç½® 10 ç§’ TTL çš„ç¼“å­˜
builder.add_node(
    "quick_cache_node",
    expensive_function,
    cache_policy=CachePolicy(ttl=10)  # 10 ç§’è¿‡æœŸ
)

graph = builder.compile(cache=saver)

# ç¬¬ä¸€æ¬¡æ‰§è¡Œ
print("â° 14:00:00 - ç¬¬ä¸€æ¬¡æ‰§è¡Œ")
result1 = graph.invoke({"query": "test"})
print(f"ç»“æœ: {result1}")
# ç¼“å­˜åˆ›å»ºï¼šexpires_at = 14:00:10

# 5 ç§’åæ‰§è¡Œ
time.sleep(5)
print("\nâ° 14:00:05 - ç¬¬äºŒæ¬¡æ‰§è¡Œï¼ˆç¼“å­˜æœ‰æ•ˆï¼‰")
result2 = graph.invoke({"query": "test"})
print(f"âœ… ä»ç¼“å­˜è¿”å›: {result2}")
# ç¼“å­˜ä»ç„¶æœ‰æ•ˆ

# å†ç­‰ 6 ç§’ï¼ˆæ€»å…± 11 ç§’ï¼‰
time.sleep(6)
print("\nâ° 14:00:11 - ç¬¬ä¸‰æ¬¡æ‰§è¡Œï¼ˆç¼“å­˜è¿‡æœŸï¼‰")
result3 = graph.invoke({"query": "test"})
print(f"âŒ ç¼“å­˜å·²è¿‡æœŸï¼Œé‡æ–°æ‰§è¡Œ: {result3}")
# æ—§ç¼“å­˜è¢«åˆ é™¤ï¼Œåˆ›å»ºæ–°ç¼“å­˜ï¼šexpires_at = 14:00:21
```

#### ç¤ºä¾‹ 2ï¼šæ‰‹åŠ¨æ¸…ç†è¿‡æœŸç¼“å­˜

```python
# ç«‹å³æ¸…ç†æ‰€æœ‰è¿‡æœŸç¼“å­˜
async def manual_cleanup(saver):
    result = await saver.conn.execute("""
        DELETE FROM checkpoints 
        WHERE checkpoint_ns = 'node_cache'
          AND (metadata->>'expires_at')::timestamp < NOW()
        RETURNING checkpoint_id
    """)
    
    print(f"æ¸…ç†äº†ä»¥ä¸‹èŠ‚ç‚¹çš„è¿‡æœŸç¼“å­˜: {result}")

# æ‰§è¡Œæ¸…ç†
await manual_cleanup(saver)
```

#### ç¤ºä¾‹ 3ï¼šæŸ¥çœ‹å³å°†è¿‡æœŸçš„ç¼“å­˜

```python
# æŸ¥è¯¢æ¥ä¸‹æ¥ 5 åˆ†é’Ÿå†…å°†è¿‡æœŸçš„ç¼“å­˜
query = """
    SELECT 
        checkpoint_id as node_name,
        (metadata->>'expires_at')::timestamp as expires_at,
        (metadata->>'expires_at')::timestamp - NOW() as time_remaining
    FROM checkpoints 
    WHERE checkpoint_ns = 'node_cache'
      AND (metadata->>'expires_at')::timestamp BETWEEN NOW() AND NOW() + INTERVAL '5 minutes'
    ORDER BY expires_at ASC
"""

result = await saver.conn.fetch(query)
for row in result:
    print(f"èŠ‚ç‚¹ {row['node_name']} å°†åœ¨ {row['time_remaining']} åè¿‡æœŸ")
```

---

### é˜²æ­¢æ„å¤–åˆ é™¤çš„å»ºè®®

#### 1. **åˆç†è®¾ç½® TTL**
```python
# âŒ é”™è¯¯ï¼šTTL å¤ªçŸ­ï¼Œç¼“å­˜å‡ ä¹æ— ç”¨
CachePolicy(ttl=1)  # 1 ç§’å°±è¿‡æœŸ

# âœ… æ­£ç¡®ï¼šæ ¹æ®æ•°æ®æ›´æ–°é¢‘ç‡è®¾ç½®
CachePolicy(ttl=300)  # 5 åˆ†é’Ÿï¼Œé€‚åˆä¸­ç­‰åŠ¨æ€æ•°æ®
```

#### 2. **ç›‘æ§ç¼“å­˜ä½¿ç”¨æƒ…å†µ**
```python
# æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡
query = """
    SELECT 
        checkpoint_id,
        COUNT(*) as access_count,
        COUNT(CASE WHEN (metadata->>'expires_at')::timestamp > NOW() 
              THEN 1 END) as hit_count
    FROM checkpoints 
    WHERE checkpoint_ns = 'node_cache'
    GROUP BY checkpoint_id
"""
```

#### 3. **å…³é”®ç¼“å­˜ä½¿ç”¨é•¿ TTL**
```python
# å¯¹äºæ˜‚è´µçš„æ“ä½œï¼Œä½¿ç”¨æ›´é•¿çš„ TTL
builder.add_node(
    "expensive_ml_model",
    ml_inference,
    cache_policy=CachePolicy(ttl=86400)  # 24 å°æ—¶
)
```

---

## æ³¨æ„äº‹é¡¹ä¸é™åˆ¶

### 1. ç¼“å­˜ä¸€è‡´æ€§

- ç¼“å­˜çš„æ•°æ®å¯èƒ½ä¸å®é™…æ•°æ®ä¸ä¸€è‡´ï¼ˆå°¤å…¶æ˜¯é•¿ TTLï¼‰
- å¯¹å®æ—¶æ€§è¦æ±‚é«˜çš„åœºæ™¯éœ€è°¨æ…ä½¿ç”¨

### 2. å†…å­˜å ç”¨

- MemorySaver ä¼šå ç”¨åº”ç”¨å†…å­˜
- å¤§é‡ç¼“å­˜å¯èƒ½å¯¼è‡´å†…å­˜æº¢å‡º
- å»ºè®®ç”Ÿäº§ç¯å¢ƒä½¿ç”¨ Redis æˆ– PostgreSQL

### 3. ç¼“å­˜ç©¿é€

```python
# é¿å…ç¼“å­˜ç©¿é€ï¼šå¯¹ç©ºç»“æœä¹Ÿè¿›è¡Œç¼“å­˜
def safe_node(state):
    result = query_data(state["id"])
    if result is None:
        result = {"empty": True}  # ç¼“å­˜ç©ºç»“æœ
    return {"data": result}
```

### 4. ç¼“å­˜é›ªå´©

```python
import random

# æ·»åŠ éšæœºåç§»é¿å…ç¼“å­˜åŒæ—¶å¤±æ•ˆ
def random_ttl():
    base_ttl = 3600
    offset = random.randint(-300, 300)
    return base_ttl + offset

CachePolicy(ttl=random_ttl())
```

---

## è°ƒè¯•ä¸è¯Šæ–­

### å¯ç”¨ç¼“å­˜æ—¥å¿—

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger("langgraph.cache")
logger.setLevel(logging.DEBUG)
```

### æŸ¥çœ‹ç¼“å­˜å‘½ä¸­ç‡

```python
class CacheMonitor:
    def __init__(self):
        self.hits = 0
        self.misses = 0
    
    def record_hit(self):
        self.hits += 1
    
    def record_miss(self):
        self.misses += 1
    
    def get_hit_rate(self):
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

monitor = CacheMonitor()
```

---

## æ€»ç»“

LangGraph çš„ç¼“å­˜æœºåˆ¶æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ€§èƒ½ä¼˜åŒ–å·¥å…·ï¼Œé€šè¿‡åˆç†é…ç½®å¯ä»¥ï¼š

âœ… **æå‡å“åº”é€Ÿåº¦**ï¼šé¿å…é‡å¤è®¡ç®—ï¼ŒåŠ å¿«æ‰§è¡Œæ•ˆç‡  
âœ… **é™ä½æˆæœ¬**ï¼šå‡å°‘ API è°ƒç”¨æ¬¡æ•°å’Œè®¡è´¹  
âœ… **æ”¹å–„ç”¨æˆ·ä½“éªŒ**ï¼šæ›´å¿«çš„å“åº”æ—¶é—´  
âœ… **ç®€åŒ–å¼€å‘**ï¼šç»Ÿä¸€çš„ç¼“å­˜ APIï¼Œæ˜“äºé›†æˆ  

**æ ¸å¿ƒè¦ç‚¹**ï¼š
- ä½¿ç”¨ `CachePolicy` é…ç½® TTL å’Œç¼“å­˜é”®
- æ ¹æ®åœºæ™¯é€‰æ‹©åˆé€‚çš„ç¼“å­˜åç«¯
- é¿å…ç¼“å­˜æœ‰å‰¯ä½œç”¨æˆ–å®æ—¶æ€§è¦æ±‚é«˜çš„æ“ä½œ
- ç›‘æ§ç¼“å­˜æ€§èƒ½å’Œå‘½ä¸­ç‡

---

## PostgreSQL Cache å­˜å‚¨æœºåˆ¶æ·±åº¦è§£æ

### ç¼“å­˜ vs æ£€æŸ¥ç‚¹çš„æ¦‚å¿µåŒºåˆ†

åœ¨æ·±å…¥äº†è§£ PostgreSQL å­˜å‚¨æœºåˆ¶ä¹‹å‰ï¼Œé¦–å…ˆè¦æ˜ç¡®ä¸¤ä¸ªé‡è¦æ¦‚å¿µï¼š

#### 1. **Node Cachingï¼ˆèŠ‚ç‚¹ç¼“å­˜ï¼‰**
- **ç”¨é€”**ï¼šç¼“å­˜èŠ‚ç‚¹è®¡ç®—ç»“æœï¼Œé¿å…é‡å¤æ‰§è¡Œ
- **ç”Ÿå‘½å‘¨æœŸ**ï¼šåŸºäº TTLï¼Œå¯è·¨ä¼šè¯å¤ç”¨
- **å­˜å‚¨ä½ç½®**ï¼šä¸ saver/checkpointer **ç›¸åŒçš„åç«¯**
- **è§¦å‘æ¡ä»¶**ï¼šç›¸åŒè¾“å…¥çš„é‡å¤è°ƒç”¨

#### 2. **Checkpointingï¼ˆæ£€æŸ¥ç‚¹ï¼‰**
- **ç”¨é€”**ï¼šä¿å­˜å›¾æ‰§è¡Œçš„çŠ¶æ€å¿«ç…§ï¼Œæ”¯æŒæ¢å¤å’Œäººå·¥å¹²é¢„
- **ç”Ÿå‘½å‘¨æœŸ**ï¼šæŒä¹…åŒ–å­˜å‚¨ï¼Œæ”¯æŒä¼šè¯æ¢å¤
- **å­˜å‚¨ä½ç½®**ï¼šPostgreSQL æ•°æ®åº“è¡¨
- **è§¦å‘æ¡ä»¶**ï¼šæ¯æ¬¡èŠ‚ç‚¹æ‰§è¡Œåè‡ªåŠ¨ä¿å­˜

**å…³é”®ç†è§£**ï¼šNode Cache å’Œ Checkpointing **å…±äº«åŒä¸€ä¸ªå­˜å‚¨åç«¯**ï¼Œä½†æœåŠ¡äºä¸åŒçš„ç›®çš„ã€‚

---

### PostgreSQL å­˜å‚¨æ¶æ„

å½“ä½ ä½¿ç”¨ `PostgresSaver` ä½œä¸º saver æ—¶ï¼ŒLangGraph ä¼šåœ¨ PostgreSQL ä¸­åˆ›å»ºä»¥ä¸‹è¡¨ç»“æ„ï¼š

#### æ ¸å¿ƒè¡¨ç»“æ„

```sql
-- 1. è¿ç§»ç‰ˆæœ¬æ§åˆ¶è¡¨
CREATE TABLE checkpoint_migrations (
    v INTEGER NOT NULL PRIMARY KEY
);

-- 2. æ£€æŸ¥ç‚¹ä¸»è¡¨ï¼ˆå­˜å‚¨çŠ¶æ€å¿«ç…§ï¼‰
CREATE TABLE checkpoints (
    thread_id             TEXT    NOT NULL,    -- ä¼šè¯/çº¿ç¨‹ ID
    checkpoint_ns         TEXT    NOT NULL DEFAULT '', -- å‘½åç©ºé—´
    checkpoint_id         TEXT    NOT NULL,    -- æ£€æŸ¥ç‚¹ ID
    parent_checkpoint_id  TEXT,               -- çˆ¶æ£€æŸ¥ç‚¹ IDï¼ˆæ”¯æŒåˆ†æ”¯ï¼‰
    type                  TEXT,               -- æ£€æŸ¥ç‚¹ç±»å‹
    checkpoint            JSONB   NOT NULL,    -- çŠ¶æ€æ•°æ®ï¼ˆJSON æ ¼å¼ï¼‰
    metadata              JSONB   NOT NULL DEFAULT '{}', -- å…ƒæ•°æ®
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id)
);

-- 3. äºŒè¿›åˆ¶æ•°æ®è¡¨ï¼ˆå­˜å‚¨å¤§å¯¹è±¡ï¼‰
CREATE TABLE checkpoint_blobs (
    thread_id     TEXT    NOT NULL,
    checkpoint_ns TEXT    NOT NULL DEFAULT '',
    channel       TEXT    NOT NULL,           -- æ•°æ®é€šé“
    version       TEXT    NOT NULL,           -- ç‰ˆæœ¬å·
    type          TEXT    NOT NULL,           -- æ•°æ®ç±»å‹
    blob          BYTEA,                      -- äºŒè¿›åˆ¶æ•°æ®
    PRIMARY KEY (thread_id, checkpoint_ns, channel, version)
);

-- 4. å†™æ“ä½œè®°å½•è¡¨ï¼ˆäº‹åŠ¡æ—¥å¿—ï¼‰
CREATE TABLE checkpoint_writes (
    thread_id     TEXT    NOT NULL,
    checkpoint_ns TEXT    NOT NULL DEFAULT '',
    checkpoint_id TEXT    NOT NULL,
    task_id       TEXT    NOT NULL,           -- ä»»åŠ¡ ID
    task_path     TEXT    NOT NULL,           -- ä»»åŠ¡è·¯å¾„
    idx           INTEGER NOT NULL,           -- ç´¢å¼•
    channel       TEXT    NOT NULL,           -- æ•°æ®é€šé“
    type          TEXT,                       -- æ•°æ®ç±»å‹
    blob          BYTEA   NOT NULL,           -- æ“ä½œæ•°æ®
    PRIMARY KEY (thread_id, checkpoint_ns, checkpoint_id, task_id, idx)
);
```

#### è¡¨çš„ä½œç”¨è¯´æ˜

| è¡¨å | ä¸»è¦ä½œç”¨ | å­˜å‚¨å†…å®¹ |
|------|----------|----------|
| `checkpoint_migrations` | ç‰ˆæœ¬ç®¡ç† | æ•°æ®åº“æ¨¡å¼ç‰ˆæœ¬å· |
| `checkpoints` | çŠ¶æ€å¿«ç…§ | å›¾æ‰§è¡Œçš„å®Œæ•´çŠ¶æ€ï¼ˆJSONï¼‰ |
| `checkpoint_blobs` | å¤§å¯¹è±¡å­˜å‚¨ | äºŒè¿›åˆ¶æ•°æ®ï¼ˆå¦‚æ–‡ä»¶ã€æ¨¡å‹ç­‰ï¼‰ |
| `checkpoint_writes` | æ“ä½œæ—¥å¿— | æ¯ä¸ªå†™æ“ä½œçš„è¯¦ç»†è®°å½• |

---

### Cache åœ¨ PostgreSQL ä¸­çš„å­˜å‚¨å®ç°

#### ç¼“å­˜å­˜å‚¨ä½ç½®

**é‡è¦å‘ç°**ï¼šNode Caching **ä¸ä¼šåˆ›å»ºé¢å¤–çš„è¡¨**ï¼Œè€Œæ˜¯åˆ©ç”¨ç°æœ‰çš„ checkpointer åŸºç¡€è®¾æ–½ï¼

```python
# å½“ä½ è¿™æ ·é…ç½®æ—¶ï¼š
from langgraph.checkpoint.postgres import PostgresSaver

cache = PostgresSaver.from_conn_string("postgresql://...")
graph = builder.compile(cache=cache)

# cache å’Œ checkpointer æ˜¯åŒä¸€ä¸ªå¯¹è±¡ï¼
# ç¼“å­˜æ•°æ®å­˜å‚¨åœ¨ checkpoints/checkpoint_blobs è¡¨ä¸­
```

#### ç¼“å­˜é”®ç”Ÿæˆæœºåˆ¶

1. **é»˜è®¤ç¼“å­˜é”®**
```python
# æºç çº§å®ç°ï¼ˆç®€åŒ–ç‰ˆï¼‰
def generate_cache_key(node_input, cache_policy):
    if cache_policy.key_func:
        return cache_policy.key_func([node_input])
    else:
        # é»˜è®¤ï¼šä½¿ç”¨è¾“å…¥çš„å“ˆå¸Œå€¼
        import hashlib
        return hashlib.sha256(
            str(node_input).encode('utf-8')
        ).hexdigest()
```

2. **ç¼“å­˜æ•°æ®ç»“æ„**
```python
# ç¼“å­˜åœ¨ PostgreSQL ä¸­çš„å­˜å‚¨æ ¼å¼
cache_record = {
    "thread_id": f"cache_{cache_key}",  # ä½¿ç”¨ç‰¹æ®Šå‰ç¼€
    "checkpoint_ns": "node_cache",      # ç¼“å­˜å‘½åç©ºé—´
    "checkpoint_id": node_name,        # èŠ‚ç‚¹åç§°
    "checkpoint": {
        "result": node_output,          # ç¼“å­˜çš„æ‰§è¡Œç»“æœ
        "timestamp": created_at,        # åˆ›å»ºæ—¶é—´æˆ³
        "ttl": cache_policy.ttl        # TTL è®¾ç½®
    },
    "metadata": {
        "cache_key": cache_key,         # åŸå§‹ç¼“å­˜é”®
        "node_name": node_name,        # èŠ‚ç‚¹åç§°
        "expires_at": expires_timestamp # è¿‡æœŸæ—¶é—´
    }
}
```

#### ç¼“å­˜ CRUD æ“ä½œå®ç°

##### 1. **å†™å…¥ç¼“å­˜ï¼ˆCREATE/UPDATEï¼‰**
```sql
-- æ’å…¥æˆ–æ›´æ–°ç¼“å­˜è®°å½•
INSERT INTO checkpoints (
    thread_id, checkpoint_ns, checkpoint_id, 
    checkpoint, metadata
) VALUES (
    'cache_' || :cache_key,
    'node_cache',
    :node_name,
    :result_json,
    :metadata_json
) ON CONFLICT (thread_id, checkpoint_ns, checkpoint_id)
DO UPDATE SET
    checkpoint = EXCLUDED.checkpoint,
    metadata = EXCLUDED.metadata;
```

##### 2. **è¯»å–ç¼“å­˜ï¼ˆREADï¼‰**
```sql
-- æŸ¥è¯¢ç¼“å­˜è®°å½•
SELECT checkpoint, metadata
FROM checkpoints
WHERE thread_id = 'cache_' || :cache_key
  AND checkpoint_ns = 'node_cache'
  AND checkpoint_id = :node_name
  -- TTL æ£€æŸ¥
  AND (metadata->>'expires_at')::timestamp > NOW();
```

##### 3. **åˆ é™¤è¿‡æœŸç¼“å­˜ï¼ˆDELETEï¼‰**
```sql
-- æ¸…ç†è¿‡æœŸç¼“å­˜
DELETE FROM checkpoints
WHERE checkpoint_ns = 'node_cache'
  AND (metadata->>'expires_at')::timestamp < NOW();
```

##### 4. **ç¼“å­˜ç»Ÿè®¡æŸ¥è¯¢**
```sql
-- æŸ¥çœ‹ç¼“å­˜ä½¿ç”¨æƒ…å†µ
SELECT 
    COUNT(*) as total_cache_entries,
    COUNT(CASE WHEN (metadata->>'expires_at')::timestamp > NOW() 
          THEN 1 END) as active_entries,
    COUNT(CASE WHEN (metadata->>'expires_at')::timestamp <= NOW() 
          THEN 1 END) as expired_entries
FROM checkpoints
WHERE checkpoint_ns = 'node_cache';
```

---

### æºç çº§å·¥ä½œæµç¨‹

#### ç¼“å­˜å‘½ä¸­æ£€æŸ¥æµç¨‹

```python
# ç®€åŒ–çš„æºç å®ç°é€»è¾‘
class PostgresCachingSaver:
    async def check_cache(self, node_name: str, node_input: dict, 
                         cache_policy: CachePolicy):
        # 1. ç”Ÿæˆç¼“å­˜é”®
        cache_key = self._generate_cache_key(node_input, cache_policy)
        
        # 2. æ„é€ æŸ¥è¯¢
        thread_id = f"cache_{cache_key}"
        
        # 3. æŸ¥è¯¢æ•°æ®åº“
        query = """
            SELECT checkpoint, metadata 
            FROM checkpoints 
            WHERE thread_id = %s 
              AND checkpoint_ns = 'node_cache'
              AND checkpoint_id = %s
        """
        
        result = await self.conn.fetchrow(query, thread_id, node_name)
        
        if result:
            # 4. TTL æ£€æŸ¥
            metadata = result['metadata']
            expires_at = metadata.get('expires_at')
            
            if expires_at and datetime.now() < datetime.fromisoformat(expires_at):
                # ç¼“å­˜å‘½ä¸­ï¼
                return result['checkpoint']['result']
            else:
                # ç¼“å­˜è¿‡æœŸï¼Œåˆ é™¤è®°å½•
                await self._delete_expired_cache(thread_id, node_name)
        
        # ç¼“å­˜æœªå‘½ä¸­
        return None
    
    async def store_cache(self, node_name: str, node_input: dict, 
                         node_output: dict, cache_policy: CachePolicy):
        cache_key = self._generate_cache_key(node_input, cache_policy)
        thread_id = f"cache_{cache_key}"
        
        # è®¡ç®—è¿‡æœŸæ—¶é—´
        expires_at = None
        if cache_policy.ttl:
            expires_at = datetime.now() + timedelta(seconds=cache_policy.ttl)
        
        # å­˜å‚¨ç¼“å­˜
        checkpoint_data = {
            "result": node_output,
            "timestamp": datetime.now().isoformat(),
            "ttl": cache_policy.ttl
        }
        
        metadata = {
            "cache_key": cache_key,
            "node_name": node_name,
            "expires_at": expires_at.isoformat() if expires_at else None
        }
        
        await self._upsert_checkpoint(
            thread_id, "node_cache", node_name,
            checkpoint_data, metadata
        )
```

#### èŠ‚ç‚¹æ‰§è¡Œä¸ç¼“å­˜é›†æˆ

```python
# èŠ‚ç‚¹æ‰§è¡Œæ—¶çš„ç¼“å­˜é€»è¾‘
async def execute_node_with_cache(node_func, node_input, cache_policy, saver):
    # 1. æ£€æŸ¥ç¼“å­˜
    cached_result = await saver.check_cache(
        node_func.__name__, node_input, cache_policy
    )
    
    if cached_result is not None:
        print(f"âœ… ç¼“å­˜å‘½ä¸­: {node_func.__name__}")
        return cached_result
    
    # 2. æ‰§è¡ŒèŠ‚ç‚¹å‡½æ•°
    print(f"âš¡ æ‰§è¡ŒèŠ‚ç‚¹: {node_func.__name__}")
    result = await node_func(node_input)
    
    # 3. å­˜å‚¨ç¼“å­˜
    await saver.store_cache(
        node_func.__name__, node_input, result, cache_policy
    )
    
    return result
```

---

### æ€§èƒ½ä¼˜åŒ–ä¸ç›‘æ§

#### æ•°æ®åº“ç´¢å¼•ä¼˜åŒ–

```sql
-- ä¸ºç¼“å­˜æŸ¥è¯¢ä¼˜åŒ–çš„ç´¢å¼•
CREATE INDEX idx_cache_lookup 
ON checkpoints(thread_id, checkpoint_ns, checkpoint_id)
WHERE checkpoint_ns = 'node_cache';

-- ä¸º TTL æ¸…ç†ä¼˜åŒ–çš„ç´¢å¼•
CREATE INDEX idx_cache_expiry 
ON checkpoints((metadata->>'expires_at'))
WHERE checkpoint_ns = 'node_cache';
```

#### ç¼“å­˜æ¸…ç†ä»»åŠ¡

```python
# å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜
import asyncio
from datetime import datetime

async def cleanup_expired_cache(saver):
    """å®šæœŸæ¸…ç†è¿‡æœŸç¼“å­˜"""
    query = """
        DELETE FROM checkpoints 
        WHERE checkpoint_ns = 'node_cache'
          AND (metadata->>'expires_at')::timestamp < NOW()
    """
    
    result = await saver.conn.execute(query)
    print(f"æ¸…ç†äº† {result} æ¡è¿‡æœŸç¼“å­˜è®°å½•")

# æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡æ¸…ç†
async def schedule_cleanup(saver):
    while True:
        await cleanup_expired_cache(saver)
        await asyncio.sleep(3600)  # 1å°æ—¶
```

#### ç¼“å­˜ç›‘æ§æŸ¥è¯¢

```sql
-- ç¼“å­˜å‘½ä¸­ç‡åˆ†æ
WITH cache_stats AS (
    SELECT 
        checkpoint_id as node_name,
        COUNT(*) as cache_entries,
        COUNT(CASE WHEN (metadata->>'expires_at')::timestamp > NOW() 
              THEN 1 END) as active_entries,
        AVG(EXTRACT(EPOCH FROM 
            (metadata->>'expires_at')::timestamp - 
            (checkpoint->>'timestamp')::timestamp
        )) as avg_ttl_seconds
    FROM checkpoints 
    WHERE checkpoint_ns = 'node_cache'
    GROUP BY checkpoint_id
)
SELECT 
    node_name,
    cache_entries,
    active_entries,
    ROUND(active_entries::numeric / cache_entries * 100, 2) as hit_rate_percent,
    ROUND(avg_ttl_seconds) as avg_ttl_seconds
FROM cache_stats
ORDER BY cache_entries DESC;
```

---

### å®é™…åº”ç”¨å»ºè®®

#### 1. **æ•°æ®åº“è¿æ¥é…ç½®**
```python
# ç”Ÿäº§ç¯å¢ƒè¿æ¥é…ç½®
from langgraph.checkpoint.postgres import PostgresSaver

# ä½¿ç”¨è¿æ¥æ± 
saver = PostgresSaver.from_conn_string(
    "postgresql://user:pass@localhost:5432/db",
    pool_size=10,  # è¿æ¥æ± å¤§å°
    max_overflow=20,  # æœ€å¤§æº¢å‡ºè¿æ¥
)

# åˆå§‹åŒ–è¡¨ç»“æ„ï¼ˆé¦–æ¬¡ä½¿ç”¨ï¼‰
await saver.setup()
```

#### 2. **ç¼“å­˜ç­–ç•¥é…ç½®**
```python
# ä¸åŒåœºæ™¯çš„ç¼“å­˜é…ç½®
builder.add_node(
    "llm_node",
    llm_function,
    cache_policy=CachePolicy(
        ttl=1800,  # LLM è°ƒç”¨ç¼“å­˜ 30 åˆ†é’Ÿ
        key_func=lambda args: f"llm_{hash(args[0]['messages'][-1].content)}"
    )
)

builder.add_node(
    "db_query_node", 
    query_function,
    cache_policy=CachePolicy(
        ttl=600,   # æ•°æ®åº“æŸ¥è¯¢ç¼“å­˜ 10 åˆ†é’Ÿ
        key_func=lambda args: f"db_{args[0]['query']}"
    )
)
```

#### 3. **ç›‘æ§ä¸ç»´æŠ¤**
```python
# ç¼“å­˜å¥åº·æ£€æŸ¥
async def cache_health_check(saver):
    stats = await saver.conn.fetchrow("""
        SELECT 
            COUNT(*) as total,
            COUNT(CASE WHEN (metadata->>'expires_at')::timestamp > NOW() 
                  THEN 1 END) as active,
            pg_size_pretty(pg_total_relation_size('checkpoints')) as table_size
        FROM checkpoints 
        WHERE checkpoint_ns = 'node_cache'
    """)
    
    return {
        "total_entries": stats["total"],
        "active_entries": stats["active"], 
        "table_size": stats["table_size"],
        "hit_rate": stats["active"] / stats["total"] if stats["total"] > 0 else 0
    }
```

---

## å‚è€ƒèµ„æº

- [LangGraph å®˜æ–¹æ–‡æ¡£ - Node Caching](https://langchain-ai.github.io/langgraph/concepts/low_level/#node-caching)
- [LangGraph How-to Guide - Cache Expensive Nodes](https://langchain-ai.github.io/langgraphjs/how-tos/node-caching/)
- [LangChain LLM Caching](https://python.langchain.com/docs/integrations/llm_caching/)
- [PostgresSaver API Reference](https://api.python.langchain.com/en/latest/checkpoint/langchain_postgres.checkpoint.PostgresSaver.html)
- [LangGraph Checkpointing Concepts](https://langchain-ai.github.io/langgraph/concepts/persistence/)

---

**æ–‡æ¡£ç‰ˆæœ¬**: v2.0  
**æœ€åæ›´æ–°**: 2025-10-13  
**ä½œè€…**: AutoAgents Team
