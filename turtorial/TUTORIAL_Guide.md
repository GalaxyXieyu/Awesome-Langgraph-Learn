# LangGraph å®Œæ•´æ•™ç¨‹ï¼šæµå¼è¾“å‡ºã€åŒæ­¥å¼‚æ­¥ä¸ä¸­æ–­æœºåˆ¶

## ğŸ“š æ•™ç¨‹æ¦‚è¿°

æœ¬æ•™ç¨‹æ·±å…¥ç ”ç©¶äº†LangGraphçš„ä¸‰ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œé€šè¿‡ç†è®ºåˆ†æå’Œå®é™…æµ‹è¯•ï¼Œä¸ºå¼€å‘è€…æä¾›å…¨é¢çš„æŠ€æœ¯æŒ‡å—ã€‚

### ğŸ¯ ç ”ç©¶ä¸»é¢˜
1. **æµå¼è¾“å‡ºæœºåˆ¶** - å®æ—¶æ•°æ®ä¼ è¾“å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ–
2. **åŒæ­¥å¼‚æ­¥è°ƒç”¨** - æ€§èƒ½ä¼˜åŒ–å’Œå…¼å®¹æ€§åˆ†æ  
3. **ä¸­æ–­ä¸äººæœºäº¤äº’** - å·¥ä½œæµæ§åˆ¶å’Œç”¨æˆ·å‚ä¸

### ğŸ“ æ–‡ä»¶ç»“æ„
```
turtorial/
â”œâ”€â”€ TUTORIAL_Complete_Guide.md              # æœ¬æ•™ç¨‹æ–‡ä»¶
â”œâ”€â”€ TEST_Streaming_Modes.py                 # æµå¼è¾“å‡ºæµ‹è¯•
â”œâ”€â”€ TEST_Sync_Async_Performance.py          # åŒæ­¥å¼‚æ­¥æµ‹è¯•
â”œâ”€â”€ TEST_Interrupt_Mechanisms.py            # ä¸­æ–­æœºåˆ¶æµ‹è¯•
â”œâ”€â”€ DEMO_Writing_Assistant.py               # å†™ä½œåŠ©æ‰‹å®Œæ•´ç¤ºä¾‹
â”œâ”€â”€ GUIDE_Streaming_Best_Practices.md       # æµå¼è¾“å‡ºæŒ‡å—
â”œâ”€â”€ GUIDE_Sync_Async_Patterns.md            # åŒæ­¥å¼‚æ­¥æŒ‡å—
â””â”€â”€ GUIDE_Human_In_Loop.md                  # ä¸­æ–­æœºåˆ¶æŒ‡å—
```

## ğŸŒŠ ç¬¬ä¸€éƒ¨åˆ†ï¼šæµå¼è¾“å‡ºæœºåˆ¶

### æ ¸å¿ƒæ¦‚å¿µ

LangGraphæä¾›äº†å¤šç§æµå¼è¾“å‡ºæ¨¡å¼ï¼Œè®©å¼€å‘è€…å¯ä»¥å®æ—¶è·å–å·¥ä½œæµçš„æ‰§è¡ŒçŠ¶æ€å’Œç»“æœã€‚

#### ä¸»è¦æµå¼æ¨¡å¼
- **`values`**: è·å–å®Œæ•´çš„çŠ¶æ€å€¼
- **`updates`**: è·å–çŠ¶æ€æ›´æ–°å¢é‡
- **`messages`**: è·å–æ¶ˆæ¯æµï¼ˆé€‚ç”¨äºèŠå¤©åœºæ™¯ï¼‰
- **`custom`**: è‡ªå®šä¹‰äº‹ä»¶æµ

### å®é™…åº”ç”¨ç¤ºä¾‹

```python
# åŸºç¡€æµå¼è¾“å‡º
for chunk in graph.stream(initial_state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")

# è‡ªå®šä¹‰äº‹ä»¶æµ
def my_node(state):
    writer = get_stream_writer()
    writer({"progress": 50, "message": "å¤„ç†ä¸­..."})
    return state

for chunk in graph.stream(state, stream_mode="custom"):
    if "progress" in chunk:
        print(f"è¿›åº¦: {chunk['progress']}%")
```

### æœ€ä½³å®è·µ
1. **é€‰æ‹©åˆé€‚çš„æµå¼æ¨¡å¼** - æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å¼
2. **åˆç†çš„æ›´æ–°é¢‘ç‡** - é¿å…è¿‡äºé¢‘ç¹çš„çŠ¶æ€æ›´æ–°
3. **ä¸°å¯Œçš„è¿›åº¦ä¿¡æ¯** - æä¾›æœ‰æ„ä¹‰çš„è¿›åº¦åé¦ˆ
4. **é”™è¯¯å¤„ç†** - åœ¨æµå¼è¾“å‡ºä¸­åŒ…å«é”™è¯¯ä¿¡æ¯

## âš¡ ç¬¬äºŒéƒ¨åˆ†ï¼šåŒæ­¥å¼‚æ­¥è°ƒç”¨åˆ†æ

### æ ¸å¿ƒå‘ç°

é€šè¿‡æ·±å…¥æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°äº†LLMè°ƒç”¨æ–¹å¼ä¸Graphæµå¼è¾“å‡ºçš„å…³é”®å…³ç³»ï¼š

**é‡è¦ç»“è®º**: LLMçš„è°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæœï¼ŒGraphçš„æµå¼æ¨¡å¼åªæ˜¯ä¼ è¾“ç®¡é“ã€‚

### äº”ç§ç»„åˆæ–¹å¼å¯¹æ¯”

| ç»„åˆæ–¹å¼ | å“åº”æ—¶é—´ | é¦–tokenæ—¶é—´ | æµå¼æ•ˆæœ | æ¨èåº¦ |
|----------|----------|-------------|----------|--------|
| `def` + `invoke()` + `stream()` | 2.55s | - | âŒ å‡æµå¼ | ä¸æ¨è |
| `def` + `stream()` + `stream()` | 30.56s | 0.30s | âœ… çœŸæµå¼ | âœ… æ¨è |
| `def` + `invoke()` + `astream()` | 2.36s | - | âŒ å‡æµå¼ | ä¸æ¨è |
| `async def` + `ainvoke()` + `astream()` | 28.52s | - | âŒ éæµå¼ | é«˜å¹¶å‘åœºæ™¯ |
| `async def` + `astream()` + `astream()` | 51.08s | 0.29s | ğŸ† å¼‚æ­¥æµå¼ | ğŸ† æœ€æ¨è |

### æœ€ä½³å®è·µä»£ç 

#### ğŸ† æœ€æ¨èï¼šå¼‚æ­¥æµå¼è¾“å‡º
```python
async def async_streaming_node(state, config):
    llm = ChatOpenAI(...)
    
    full_response = ""
    # å¼‚æ­¥æµå¼è°ƒç”¨ - æœ€ä½³ç»„åˆ
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_response += chunk.content
            # Tokenä¼šè‡ªåŠ¨é€šè¿‡LangGraphæµå¼ä¼ è¾“
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
async for chunk in graph.astream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

#### âœ… æ¨èï¼šåŒæ­¥æµå¼è¾“å‡º
```python
def streaming_node(state):
    llm = ChatOpenAI(...)
    
    full_response = ""
    for chunk in llm.stream(messages):  # å…³é”®ï¼šä½¿ç”¨stream()
        if chunk.content:
            full_response += chunk.content
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
for chunk in graph.stream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

### å†³ç­–æŒ‡å—

```
éœ€è¦æµå¼è¾“å‡ºï¼Ÿ
â”œâ”€ æ˜¯ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ ğŸ† async def + llm.astream() + graph.astream()
â”‚   â””â”€ å¦ â†’ âœ… def + llm.stream() + graph.stream()
â””â”€ å¦ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ âœ… async def + llm.ainvoke() + graph.astream()
    â””â”€ å¦ â†’ âš ï¸ def + llm.invoke() + graph.stream()
```

## ğŸ”„ ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸­æ–­ä¸äººæœºäº¤äº’

### ä¸­æ–­æœºåˆ¶æ¦‚è¿°

LangGraphçš„ä¸­æ–­æœºåˆ¶å…è®¸åœ¨å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœå¹¶ç­‰å¾…å¤–éƒ¨è¾“å…¥ï¼Œå®ç°çœŸæ­£çš„äººæœºåä½œã€‚

### ä¸‰ç§ä¸­æ–­æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | `interrupt` (åŠ¨æ€) | `interrupt_before` | `interrupt_after` |
|------|-------------------|-------------------|------------------|
| **è§¦å‘æ—¶æœº** | ä»£ç æ‰§è¡Œæ—¶ | èŠ‚ç‚¹æ‰§è¡Œå‰ | èŠ‚ç‚¹æ‰§è¡Œå |
| **æ•°æ®ä¼ é€’** | âœ… ä¸°å¯Œä¸Šä¸‹æ–‡ | âŒ ä»…åŸºç¡€çŠ¶æ€ | âœ… æ‰§è¡Œç»“æœ |
| **ç”Ÿäº§ç¯å¢ƒ** | ğŸ† **æ¨è** | âŒ ä»…è°ƒè¯•ç”¨ | âŒ ä»…è°ƒè¯•ç”¨ |
| **ç”¨æˆ·äº¤äº’** | âœ… å¤æ‚å†³ç­– | âš ï¸ ç®€å•ç»§ç»­/åœæ­¢ | âš ï¸ ç®€å•ç»§ç»­/åœæ­¢ |

### åŠ¨æ€ä¸­æ–­æœ€ä½³å®è·µ

```python
def approval_node(state):
    # ç”Ÿæˆå†…å®¹
    content = generate_content(state)
    
    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    user_decision = interrupt({
        "type": "content_approval",
        "message": "è¯·å®¡æ ¸ç”Ÿæˆçš„å†…å®¹ï¼š",
        "content": content,
        "options": {
            "approve": "æ‰¹å‡†å†…å®¹",
            "edit": "ç¼–è¾‘å†…å®¹",
            "regenerate": "é‡æ–°ç”Ÿæˆ"
        },
        "ui_hints": {
            "show_preview": True,
            "allow_inline_edit": True
        }
    })
    
    # å¤„ç†ç”¨æˆ·å†³ç­–
    if user_decision.get("action") == "approve":
        state["status"] = "approved"
    elif user_decision.get("action") == "edit":
        state["content"] = user_decision.get("edited_content")
        state["status"] = "edited"
    else:  # regenerate
        state["status"] = "regeneration_needed"
    
    return state

# æ¢å¤æ‰§è¡Œ
result = graph.invoke(Command(resume={"action": "approve"}), config)
```

### é™æ€ä¸­æ–­ç”¨æ³•

```python
# å¼€å‘è°ƒè¯•æ—¶ä½¿ç”¨
graph = workflow.compile(
    checkpointer=InMemorySaver(),
    interrupt_before=["critical_node"],  # åœ¨å…³é”®èŠ‚ç‚¹å‰æš‚åœ
    interrupt_after=["validation_node"]   # åœ¨éªŒè¯èŠ‚ç‚¹åæš‚åœ
)

# ç»§ç»­æ‰§è¡Œ
result = graph.invoke(None, config)  # ä¼ å…¥Noneç»§ç»­æ‰§è¡Œ
```

## ğŸš€ ç¬¬å››éƒ¨åˆ†ï¼šå†™ä½œåŠ©æ‰‹å®Œæ•´ç¤ºä¾‹

### å·¥ä½œæµè®¾è®¡

å†™ä½œåŠ©æ‰‹é¡¹ç›®é›†æˆäº†æ‰€æœ‰ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼Œå±•ç¤ºäº†å®Œæ•´çš„äººæœºåä½œå·¥ä½œæµï¼š

```
ç”¨æˆ·è¾“å…¥ä¸»é¢˜
    â†“
ç”Ÿæˆå¤§çº² â†’ [ä¸­æ–­ç¡®è®¤] â†’ æœç´¢èµ„æ–™ â†’ [ä¸­æ–­ç­›é€‰] 
    â†“
ç”Ÿæˆæ–‡ç«  â†’ [ä¸­æ–­å®¡æ ¸] â†’ æœ€ç»ˆç¡®è®¤ â†’ [ä¸­æ–­å‘å¸ƒ]
    â†“
å‘å¸ƒå®Œæˆ
```

### å…³é”®é›†æˆç‚¹

#### 1. å¤§çº²ç”Ÿæˆï¼ˆæµå¼ + ä¸­æ–­ï¼‰
```python
async def generate_outline_with_approval(state):
    # å¼‚æ­¥æµå¼ç”Ÿæˆå¤§çº²
    outline = await generate_outline_async(state["topic"])
    
    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    user_decision = interrupt({
        "type": "outline_approval",
        "outline": outline,
        "options": ["approve", "modify", "regenerate"]
    })
    
    return handle_outline_decision(state, user_decision)
```

#### 2. æ–‡ç« ç”Ÿæˆï¼ˆå¼‚æ­¥æµå¼è¾“å‡ºï¼‰
```python
async def generate_article_streaming(state, config):
    llm = ChatOpenAI(...)
    
    full_article = ""
    # ä½¿ç”¨æœ€ä½³ç»„åˆï¼šå¼‚æ­¥æµå¼
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_article += chunk.content
            # å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
    
    return {"article": full_article}
```

#### 3. å®Œæ•´å·¥ä½œæµ
```python
def create_writing_assistant():
    workflow = StateGraph(WritingState)
    
    # æ·»åŠ å¸¦ä¸­æ–­çš„èŠ‚ç‚¹
    workflow.add_node("outline", generate_outline_with_approval)
    workflow.add_node("search", search_and_select_sources)
    workflow.add_node("article", generate_article_streaming)
    workflow.add_node("publish", final_publish_confirmation)
    
    # è®¾ç½®æµç¨‹
    workflow.set_entry_point("outline")
    workflow.add_edge("outline", "search")
    workflow.add_edge("search", "article")
    workflow.add_edge("article", "publish")
    workflow.add_edge("publish", END)
    
    return workflow.compile(checkpointer=InMemorySaver())
```

## ğŸ’¡ ç»¼åˆæœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
- **å¼‚æ­¥ä¼˜å…ˆ**: é«˜å¹¶å‘åœºæ™¯ä½¿ç”¨å¼‚æ­¥èŠ‚ç‚¹å’Œå¼‚æ­¥LLMè°ƒç”¨
- **æµå¼è¾“å‡º**: éœ€è¦å®æ—¶åé¦ˆæ—¶ä½¿ç”¨`llm.stream()`æˆ–`llm.astream()`
- **åˆç†ç¼“å­˜**: é¿å…é‡å¤çš„æ˜‚è´µæ“ä½œ

### 2. ç”¨æˆ·ä½“éªŒ
- **åŠæ—¶åé¦ˆ**: ä½¿ç”¨æµå¼è¾“å‡ºæä¾›å®æ—¶è¿›åº¦
- **æ¸…æ™°æç¤º**: ä¸­æ–­æ—¶æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **çµæ´»æ§åˆ¶**: ç»™ç”¨æˆ·å¤šç§å†³ç­–é€‰é¡¹

### 3. é”™è¯¯å¤„ç†
- **ä¼˜é›…é™çº§**: æµå¼è¾“å‡ºå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
- **çŠ¶æ€æ¢å¤**: ä¸­æ–­åçš„çŠ¶æ€ä¸€è‡´æ€§ä¿è¯
- **å¼‚å¸¸æ•è·**: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶

### 4. å¼€å‘è°ƒè¯•
- **é™æ€ä¸­æ–­**: å¼€å‘é˜¶æ®µä½¿ç”¨interrupt_before/afterè°ƒè¯•
- **æ—¥å¿—è®°å½•**: è¯¦ç»†è®°å½•ç”¨æˆ·å†³ç­–å’Œæ‰§è¡Œå†å²
- **çŠ¶æ€ç›‘æ§**: ç›‘æ§å·¥ä½œæµçš„æ‰§è¡ŒçŠ¶æ€

## ğŸ¯ æ€»ç»“

é€šè¿‡æœ¬æ•™ç¨‹çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬æŒæ¡äº†LangGraphçš„ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š

1. **æµå¼è¾“å‡º** - æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®æŠ€æœ¯
2. **åŒæ­¥å¼‚æ­¥** - æ€§èƒ½ä¼˜åŒ–çš„é‡è¦é€‰æ‹©
3. **ä¸­æ–­æœºåˆ¶** - äººæœºåä½œçš„å¼ºå¤§å·¥å…·

è¿™äº›åŠŸèƒ½çš„åˆç†ç»„åˆå¯ä»¥æ„å»ºå‡ºæ—¢é«˜æ•ˆåˆç”¨æˆ·å‹å¥½çš„AIåº”ç”¨ç³»ç»Ÿã€‚

### ğŸ”— ç›¸å…³æ–‡ä»¶
- `TEST_Streaming_Modes.py` - æµå¼è¾“å‡ºåŠŸèƒ½æµ‹è¯•
- `TEST_Sync_Async_Performance.py` - åŒæ­¥å¼‚æ­¥æ€§èƒ½æµ‹è¯•
- `TEST_Interrupt_Mechanisms.py` - ä¸­æ–­æœºåˆ¶åŠŸèƒ½æµ‹è¯•
- `DEMO_Writing_Assistant.py` - å®Œæ•´åº”ç”¨ç¤ºä¾‹

### ğŸ“š è¯¦ç»†æŒ‡å—
- `GUIDE_Streaming_Best_Practices.md` - æµå¼è¾“å‡ºè¯¦ç»†æŒ‡å—
- `GUIDE_Sync_Async_Patterns.md` - åŒæ­¥å¼‚æ­¥è¯¦ç»†æŒ‡å—
- `GUIDE_Human_In_Loop.md` - ä¸­æ–­æœºåˆ¶è¯¦ç»†æŒ‡å—

---

**ğŸ‰ æ­å–œï¼ä½ ç°åœ¨å·²ç»æŒæ¡äº†LangGraphçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå¯ä»¥æ„å»ºå¼ºå¤§çš„äººæœºåä½œAIåº”ç”¨äº†ï¼**
