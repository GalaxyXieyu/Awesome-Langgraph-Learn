# LangGraph å®Œæ•´æ•™ç¨‹ï¼šæµå¼è¾“å‡ºã€åŒæ­¥å¼‚æ­¥ä¸ä¸­æ–­æœºåˆ¶

## ğŸ“š æ•™ç¨‹æ¦‚è¿°

æœ¬æ•™ç¨‹æ·±å…¥ç ”ç©¶äº†LangGraphçš„ä¸‰ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œé€šè¿‡ç†è®ºåˆ†æå’Œå®é™…æµ‹è¯•ï¼Œä¸ºå¼€å‘è€…æä¾›å…¨é¢çš„æŠ€æœ¯æŒ‡å—ã€‚

### ğŸ¯ ç ”ç©¶ä¸»é¢˜
1. **æµå¼è¾“å‡ºæœºåˆ¶** - å®æ—¶æ•°æ®ä¼ è¾“å’Œç”¨æˆ·ä½“éªŒä¼˜åŒ–
2. **åŒæ­¥å¼‚æ­¥è°ƒç”¨** - æ€§èƒ½ä¼˜åŒ–å’Œå…¼å®¹æ€§åˆ†æ  
3. **ä¸­æ–­ä¸äººæœºäº¤äº’** - å·¥ä½œæµæ§åˆ¶å’Œç”¨æˆ·å‚ä¸

### ğŸ“ æ–‡ä»¶ç»“æ„
```
turtorial/
â”œâ”€â”€ TUTORIAL_Guide.md                       # æœ¬æ•™ç¨‹æ–‡ä»¶
â”œâ”€â”€ TEST_Streaming_Modes.py                 # æµå¼è¾“å‡ºæµ‹è¯•
â”œâ”€â”€ TEST_Sync_Async_Performance.py          # åŒæ­¥å¼‚æ­¥æµ‹è¯•
â”œâ”€â”€ TEST_Interrupt_Mechanisms.py            # ä¸­æ–­æœºåˆ¶æµ‹è¯•
â”œâ”€â”€ DEMO_Writing_Assistant.py               # å†™ä½œåŠ©æ‰‹å®Œæ•´ç¤ºä¾‹
â”œâ”€â”€ GUIDE_Streaming_Best_Practices.md       # æµå¼è¾“å‡ºæŒ‡å—
â”œâ”€â”€ GUIDE_Sync_Async_Patterns.md            # åŒæ­¥å¼‚æ­¥æŒ‡å—
â””â”€â”€ GUIDE_Human_In_Loop.md                  # ä¸­æ–­æœºåˆ¶æŒ‡å—
```

## ğŸŒŠ ç¬¬ä¸€éƒ¨åˆ†ï¼šæµå¼è¾“å‡ºæœºåˆ¶

### æ ¸å¿ƒæ¦‚å¿µ

LangGraphæä¾›äº†å¤šç§æµå¼è¾“å‡ºæ¨¡å¼ï¼Œè®©å¼€å‘è€…å¯ä»¥å®æ—¶è·å–å·¥ä½œæµçš„æ‰§è¡ŒçŠ¶æ€å’Œç»“æœã€‚

#### ä¸»è¦æµå¼æ¨¡å¼
- **`values`**: è·å–å®Œæ•´çš„çŠ¶æ€å€¼
- **`updates`**: è·å–çŠ¶æ€æ›´æ–°å¢é‡
- **`messages`**: è·å–æ¶ˆæ¯æµï¼ˆé€‚ç”¨äºèŠå¤©åœºæ™¯ï¼‰
- **`custom`**: è‡ªå®šä¹‰äº‹ä»¶æµ

### å®é™…åº”ç”¨ç¤ºä¾‹

```python
# åŸºç¡€æµå¼è¾“å‡º
for chunk in graph.stream(initial_state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")

# è‡ªå®šä¹‰äº‹ä»¶æµ
def my_node(state):
    writer = get_stream_writer()
    writer({"progress": 50, "message": "å¤„ç†ä¸­..."})
    return state

for chunk in graph.stream(state, stream_mode="custom"):
    if "progress" in chunk:
        print(f"è¿›åº¦: {chunk['progress']}%")
```

### æœ€ä½³å®è·µ
1. **é€‰æ‹©åˆé€‚çš„æµå¼æ¨¡å¼** - æ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å¼
2. **åˆç†çš„æ›´æ–°é¢‘ç‡** - é¿å…è¿‡äºé¢‘ç¹çš„çŠ¶æ€æ›´æ–°
3. **ä¸°å¯Œçš„è¿›åº¦ä¿¡æ¯** - æä¾›æœ‰æ„ä¹‰çš„è¿›åº¦åé¦ˆ
4. **é”™è¯¯å¤„ç†** - åœ¨æµå¼è¾“å‡ºä¸­åŒ…å«é”™è¯¯ä¿¡æ¯

## âš¡ ç¬¬äºŒéƒ¨åˆ†ï¼šåŒæ­¥å¼‚æ­¥è°ƒç”¨åˆ†æ

### æ ¸å¿ƒå‘ç°

é€šè¿‡æ·±å…¥æµ‹è¯•ï¼Œæˆ‘ä»¬å‘ç°äº†LLMè°ƒç”¨æ–¹å¼ä¸Graphæµå¼è¾“å‡ºçš„å…³é”®å…³ç³»ï¼š

**é‡è¦ç»“è®º**: LLMçš„è°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæœï¼ŒGraphçš„æµå¼æ¨¡å¼åªæ˜¯ä¼ è¾“ç®¡é“ã€‚

### äº”ç§ç»„åˆæ–¹å¼å¯¹æ¯”

| ç»„åˆæ–¹å¼ | å“åº”æ—¶é—´ | é¦–tokenæ—¶é—´ | æµå¼æ•ˆæœ | æ¨èåº¦ |
|----------|----------|-------------|----------|--------|
| `def` + `invoke()` + `stream()` | 2.55s | - | âŒ å‡æµå¼ | ä¸æ¨è |
| `def` + `stream()` + `stream()` | 30.56s | 0.30s | âœ… çœŸæµå¼ | âœ… æ¨è |
| `def` + `invoke()` + `astream()` | 2.36s | - | âŒ å‡æµå¼ | ä¸æ¨è |
| `async def` + `ainvoke()` + `astream()` | 28.52s | - | âŒ éæµå¼ | é«˜å¹¶å‘åœºæ™¯ |
| `async def` + `astream()` + `astream()` | 51.08s | 0.29s | ğŸ† å¼‚æ­¥æµå¼ | ğŸ† æœ€æ¨è |

### æœ€ä½³å®è·µä»£ç 

#### ğŸ† æœ€æ¨èï¼šå¼‚æ­¥æµå¼è¾“å‡º
```python
async def async_streaming_node(state, config):
    llm = ChatOpenAI(...)
    
    full_response = ""
    # å¼‚æ­¥æµå¼è°ƒç”¨ - æœ€ä½³ç»„åˆ
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_response += chunk.content
            # Tokenä¼šè‡ªåŠ¨é€šè¿‡LangGraphæµå¼ä¼ è¾“
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
async for chunk in graph.astream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

#### âœ… æ¨èï¼šåŒæ­¥æµå¼è¾“å‡º
```python
def streaming_node(state):
    llm = ChatOpenAI(...)
    
    full_response = ""
    for chunk in llm.stream(messages):  # å…³é”®ï¼šä½¿ç”¨stream()
        if chunk.content:
            full_response += chunk.content
    
    return {"result": full_response}

# è°ƒç”¨æ–¹å¼
for chunk in graph.stream(state, stream_mode="updates"):
    print(f"èŠ‚ç‚¹æ›´æ–°: {chunk}")
```

### å†³ç­–æŒ‡å—

```
éœ€è¦æµå¼è¾“å‡ºï¼Ÿ
â”œâ”€ æ˜¯ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
â”‚   â”œâ”€ æ˜¯ â†’ ğŸ† async def + llm.astream() + graph.astream()
â”‚   â””â”€ å¦ â†’ âœ… def + llm.stream() + graph.stream()
â””â”€ å¦ â†’ éœ€è¦é«˜å¹¶å‘ï¼Ÿ
    â”œâ”€ æ˜¯ â†’ âœ… async def + llm.ainvoke() + graph.astream()
    â””â”€ å¦ â†’ âš ï¸ def + llm.invoke() + graph.stream()
```

## ğŸ”„ ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸­æ–­ä¸äººæœºäº¤äº’

### ä¸­æ–­æœºåˆ¶æ¦‚è¿°

LangGraphçš„ä¸­æ–­æœºåˆ¶å…è®¸åœ¨å·¥ä½œæµæ‰§è¡Œè¿‡ç¨‹ä¸­æš‚åœå¹¶ç­‰å¾…å¤–éƒ¨è¾“å…¥ï¼Œå®ç°çœŸæ­£çš„äººæœºåä½œã€‚

### ä¸‰ç§ä¸­æ–­æ–¹å¼å¯¹æ¯”

| ç‰¹æ€§ | `interrupt` (åŠ¨æ€) | `interrupt_before` | `interrupt_after` |
|------|-------------------|-------------------|------------------|
| **è§¦å‘æ—¶æœº** | ä»£ç æ‰§è¡Œæ—¶ | èŠ‚ç‚¹æ‰§è¡Œå‰ | èŠ‚ç‚¹æ‰§è¡Œå |
| **æ•°æ®ä¼ é€’** | âœ… ä¸°å¯Œä¸Šä¸‹æ–‡ | âŒ ä»…åŸºç¡€çŠ¶æ€ | âœ… æ‰§è¡Œç»“æœ |
| **ç”Ÿäº§ç¯å¢ƒ** | ğŸ† **æ¨è** | âŒ ä»…è°ƒè¯•ç”¨ | âŒ ä»…è°ƒè¯•ç”¨ |
| **ç”¨æˆ·äº¤äº’** | âœ… å¤æ‚å†³ç­– | âš ï¸ ç®€å•ç»§ç»­/åœæ­¢ | âš ï¸ ç®€å•ç»§ç»­/åœæ­¢ |

### åŠ¨æ€ä¸­æ–­æœ€ä½³å®è·µ

```python
def approval_node(state):
    # ç”Ÿæˆå†…å®¹
    content = generate_content(state)
    
    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    user_decision = interrupt({
        "type": "content_approval",
        "message": "è¯·å®¡æ ¸ç”Ÿæˆçš„å†…å®¹ï¼š",
        "content": content,
        "options": {
            "approve": "æ‰¹å‡†å†…å®¹",
            "edit": "ç¼–è¾‘å†…å®¹",
            "regenerate": "é‡æ–°ç”Ÿæˆ"
        },
        "ui_hints": {
            "show_preview": True,
            "allow_inline_edit": True
        }
    })
    
    # å¤„ç†ç”¨æˆ·å†³ç­–
    if user_decision.get("action") == "approve":
        state["status"] = "approved"
    elif user_decision.get("action") == "edit":
        state["content"] = user_decision.get("edited_content")
        state["status"] = "edited"
    else:  # regenerate
        state["status"] = "regeneration_needed"
    
    return state

# æ¢å¤æ‰§è¡Œ
result = graph.invoke(Command(resume={"action": "approve"}), config)
```

### é™æ€ä¸­æ–­ç”¨æ³•

```python
# å¼€å‘è°ƒè¯•æ—¶ä½¿ç”¨
graph = workflow.compile(
    checkpointer=InMemorySaver(),
    interrupt_before=["critical_node"],  # åœ¨å…³é”®èŠ‚ç‚¹å‰æš‚åœ
    interrupt_after=["validation_node"]   # åœ¨éªŒè¯èŠ‚ç‚¹åæš‚åœ
)

# ç»§ç»­æ‰§è¡Œ
result = graph.invoke(None, config)  # ä¼ å…¥Noneç»§ç»­æ‰§è¡Œ
```

## âš ï¸ é‡è¦å‘ç°ï¼šInterrupt èŠ‚ç‚¹é‡å¤æ‰§è¡Œé—®é¢˜

### ğŸ” é—®é¢˜æè¿°

**æ ¸å¿ƒé—®é¢˜**: åŒ…å« `interrupt()` çš„èŠ‚ç‚¹åœ¨ç”¨æˆ·è¾“å…¥åä¼š**å®Œå…¨é‡æ–°æ‰§è¡Œ**ï¼Œè¿™ä¼šå¯¼è‡´ï¼š

1. **å¤§æ¨¡å‹é‡å¤è°ƒç”¨** - æ— è®ºä½¿ç”¨ `invoke()` è¿˜æ˜¯ `astream()`
2. **é¢å¤–çš„APIæˆæœ¬** - æ¯æ¬¡ç”¨æˆ·äº¤äº’éƒ½ä¼šé‡å¤è°ƒç”¨
3. **ç”¨æˆ·è¾“å…¥å¤„ç†å»¶è¿Ÿ** - éœ€è¦ç­‰å¾…èŠ‚ç‚¹é‡æ–°æ‰§è¡Œå®Œæˆ
4. **æ½œåœ¨çš„ç»“æœä¸ä¸€è‡´** - é‡æ–°è°ƒç”¨å¯èƒ½äº§ç”Ÿä¸åŒç»“æœ

### ğŸ“Š éªŒè¯æµ‹è¯•ç»“æœ

é€šè¿‡è¯¦ç»†æµ‹è¯•éªŒè¯äº†ä»¥ä¸‹å…³é”®å‘ç°ï¼š

| æµ‹è¯•åœºæ™¯ | å¤§æ¨¡å‹è°ƒç”¨æ¬¡æ•° | èŠ‚ç‚¹æ‰§è¡Œæ¬¡æ•° | é—®é¢˜ä¸¥é‡ç¨‹åº¦ |
|----------|---------------|-------------|-------------|
| `invoke()` + `interrupt()` | **2æ¬¡** | 2æ¬¡ | ğŸ”´ é«˜æˆæœ¬ |
| `astream()` + `interrupt()` | **2æ¬¡** | 2æ¬¡ | ğŸ”´ é«˜æˆæœ¬ |
| æ— å¤§æ¨¡å‹ + `interrupt()` | 0æ¬¡ | 2æ¬¡ | ğŸŸ¡ æ€§èƒ½å½±å“ |

**ç»“è®º**: è¿™ä¸æ˜¯ `astream` ç‰¹æœ‰çš„é—®é¢˜ï¼Œè€Œæ˜¯ **LangGraph interrupt æœºåˆ¶çš„è®¾è®¡ç‰¹æ€§**ã€‚

### ğŸ› ï¸ è§£å†³æ–¹æ¡ˆå¯¹æ¯”

#### âŒ é—®é¢˜ä»£ç ç¤ºä¾‹
```python
async def problematic_node(state):
    """é—®é¢˜ï¼šå¤§æ¨¡å‹ä¼šè¢«è°ƒç”¨2æ¬¡"""
    print("ğŸ”„ èŠ‚ç‚¹å¼€å§‹æ‰§è¡Œ")

    # ç¬¬ä¸€æ¬¡æ‰§è¡Œï¼šè°ƒç”¨å¤§æ¨¡å‹
    # ç”¨æˆ·è¾“å…¥åï¼šé‡æ–°æ‰§è¡Œï¼Œå†æ¬¡è°ƒç”¨å¤§æ¨¡å‹ï¼
    result = await llm.ainvoke("ç”Ÿæˆå†…å®¹")  # æˆ– llm.invoke()

    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·è¾“å…¥
    user_input = interrupt({
        "question": "æ»¡æ„è¿™ä¸ªç»“æœå—ï¼Ÿ",
        "result": result
    })

    print(f"âœ… ç”¨æˆ·è¾“å…¥: {user_input}")
    return {"output": result, "feedback": user_input}
```

#### âœ… è§£å†³æ–¹æ¡ˆ1ï¼šåˆ†ç¦»å¤§æ¨¡å‹è°ƒç”¨å’Œä¸­æ–­ï¼ˆæ¨èï¼‰
```python
async def llm_generation_node(state):
    """ä¸“é—¨è´Ÿè´£å¤§æ¨¡å‹è°ƒç”¨"""
    if not state.get("llm_completed"):
        print("ğŸ¤– è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå†…å®¹")
        result = await llm.ainvoke("ç”Ÿæˆå†…å®¹")
        return {
            "llm_output": result,
            "llm_completed": True
        }
    return {}  # å·²å®Œæˆï¼Œè·³è¿‡

async def user_interaction_node(state):
    """ä¸“é—¨è´Ÿè´£ç”¨æˆ·äº¤äº’"""
    print("ğŸ’¬ ç­‰å¾…ç”¨æˆ·åé¦ˆ")
    user_input = interrupt({
        "question": "æ»¡æ„è¿™ä¸ªç»“æœå—ï¼Ÿ",
        "result": state["llm_output"]
    })

    return {"user_feedback": user_input}

# å›¾ç»“æ„
workflow.add_node("llm_gen", llm_generation_node)
workflow.add_node("user_input", user_interaction_node)
workflow.add_edge("llm_gen", "user_input")
```

#### âœ… è§£å†³æ–¹æ¡ˆ2ï¼šç¼“å­˜æœºåˆ¶ï¼ˆæŠ˜ä¸­æ–¹æ¡ˆï¼‰
```python
async def cached_node_with_interrupt(state):
    """ä½¿ç”¨ç¼“å­˜é¿å…é‡å¤è°ƒç”¨"""

    # æ£€æŸ¥ç¼“å­˜ï¼Œé¿å…é‡å¤è°ƒç”¨
    if not state.get("llm_cache"):
        print("ğŸ¤– é¦–æ¬¡è°ƒç”¨å¤§æ¨¡å‹")
        result = await llm.ainvoke("ç”Ÿæˆå†…å®¹")
        # ç¼“å­˜ç»“æœ
        state["llm_cache"] = result
    else:
        print("âœ… ä½¿ç”¨ç¼“å­˜ç»“æœ")
        result = state["llm_cache"]

    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·è¾“å…¥
    user_input = interrupt({
        "question": "æ»¡æ„è¿™ä¸ªç»“æœå—ï¼Ÿ",
        "result": result
    })

    return {
        "output": result,
        "feedback": user_input,
        "llm_cache": result  # ä¿æŒç¼“å­˜
    }
```

#### âš ï¸ è§£å†³æ–¹æ¡ˆ3ï¼šæ¥å—é‡å¤è°ƒç”¨ï¼ˆç®€å•ä½†æœ‰æˆæœ¬ï¼‰
```python
async def simple_but_costly_node(state):
    """ç®€å•å®ç°ï¼Œä½†ä¼šäº§ç”Ÿé¢å¤–æˆæœ¬"""
    print("âš ï¸  æ¥å—å¤§æ¨¡å‹ä¼šè¢«è°ƒç”¨2æ¬¡çš„äº‹å®")

    # æ¯æ¬¡ç”¨æˆ·äº¤äº’éƒ½ä¼šé‡æ–°è°ƒç”¨ï¼Œäº§ç”Ÿé¢å¤–æˆæœ¬
    result = await llm.ainvoke("ç”Ÿæˆå†…å®¹")

    user_input = interrupt({
        "question": "æ»¡æ„è¿™ä¸ªç»“æœå—ï¼Ÿ",
        "result": result
    })

    return {"output": result, "feedback": user_input}
```

### ğŸ¯ æœ€ä½³å®è·µå»ºè®®

#### ç”Ÿäº§ç¯å¢ƒæ¨è
```python
# ğŸ† æœ€ä½³å®è·µï¼šå®Œå…¨åˆ†ç¦»å…³æ³¨ç‚¹
class OptimizedWorkflow:
    def create_graph(self):
        workflow = StateGraph(State)

        # åˆ†ç¦»çš„èŠ‚ç‚¹è®¾è®¡
        workflow.add_node("content_generation", self.generate_content)
        workflow.add_node("user_approval", self.get_user_approval)
        workflow.add_node("content_revision", self.revise_content)
        workflow.add_node("final_processing", self.final_process)

        # æ™ºèƒ½è·¯ç”±
        workflow.add_conditional_edges(
            "user_approval",
            self.route_based_on_feedback,
            {
                "approved": "final_processing",
                "revise": "content_revision",
                "regenerate": "content_generation"  # åªæœ‰æ˜ç¡®è¦æ±‚æ‰é‡æ–°ç”Ÿæˆ
            }
        )

        return workflow.compile(checkpointer=InMemorySaver())

    async def generate_content(self, state):
        """åªè´Ÿè´£å†…å®¹ç”Ÿæˆï¼Œä¸å¤„ç†ç”¨æˆ·äº¤äº’"""
        if state.get("regenerate_requested"):
            # æ¸…é™¤ç¼“å­˜ï¼Œé‡æ–°ç”Ÿæˆ
            state.pop("content_cache", None)

        if not state.get("content_cache"):
            content = await self.llm.ainvoke(state["prompt"])
            state["content_cache"] = content

        return {"content": state["content_cache"]}

    async def get_user_approval(self, state):
        """åªè´Ÿè´£ç”¨æˆ·äº¤äº’ï¼Œä¸è°ƒç”¨å¤§æ¨¡å‹"""
        feedback = interrupt({
            "type": "content_approval",
            "content": state["content"],
            "options": ["approved", "revise", "regenerate"]
        })

        return {"user_feedback": feedback}
```

### ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | å¤§æ¨¡å‹è°ƒç”¨æ¬¡æ•° | å¼€å‘å¤æ‚åº¦ | è¿è¡Œæˆæœ¬ | æ¨èåœºæ™¯ |
|------|---------------|------------|----------|----------|
| **åˆ†ç¦»æ–¹æ¡ˆ** | 1æ¬¡ï¼ˆæœ€ä¼˜ï¼‰ | ä¸­ç­‰ | ğŸ’° æœ€ä½ | ğŸ† ç”Ÿäº§ç¯å¢ƒ |
| **ç¼“å­˜æ–¹æ¡ˆ** | 1æ¬¡ | ä½ | ğŸ’°ğŸ’° ä½ | å¿«é€ŸåŸå‹ |
| **æ¥å—é‡å¤** | 2æ¬¡ | æœ€ä½ | ğŸ’°ğŸ’°ğŸ’° é«˜ | ä»…æµ‹è¯•ç”¨ |

### ğŸ”§ è°ƒè¯•æŠ€å·§

```python
# æ·»åŠ æ‰§è¡Œè®¡æ•°å™¨æ¥ç›‘æ§é‡å¤æ‰§è¡Œ
execution_counter = {}

async def debug_node(state):
    node_name = "debug_node"
    execution_counter[node_name] = execution_counter.get(node_name, 0) + 1

    print(f"ğŸ” {node_name} æ‰§è¡Œæ¬¡æ•°: {execution_counter[node_name]}")

    if execution_counter[node_name] > 1:
        print("âš ï¸  æ£€æµ‹åˆ°èŠ‚ç‚¹é‡å¤æ‰§è¡Œï¼")

    # ä½ çš„èŠ‚ç‚¹é€»è¾‘...
    return state
```

### ğŸ’¡ å…³é”®è¦ç‚¹æ€»ç»“

1. **é—®é¢˜æ™®éæ€§**: `invoke()` å’Œ `astream()` éƒ½ä¼šé‡å¤è°ƒç”¨
2. **æ ¹æœ¬åŸå› **: LangGraph interrupt æœºåˆ¶ä¼šé‡æ–°æ‰§è¡Œæ•´ä¸ªèŠ‚ç‚¹
3. **æœ€ä½³è§£å†³æ–¹æ¡ˆ**: åˆ†ç¦»å¤§æ¨¡å‹è°ƒç”¨å’Œç”¨æˆ·äº¤äº’é€»è¾‘
4. **æˆæœ¬å½±å“**: é‡å¤è°ƒç”¨ä¼šå¯¼è‡´APIæˆæœ¬ç¿»å€
5. **å¼€å‘å»ºè®®**: ç”Ÿäº§ç¯å¢ƒå¿…é¡»è€ƒè™‘è¿™ä¸ªé—®é¢˜

## ğŸš€ ç¬¬å››éƒ¨åˆ†ï¼šå†™ä½œåŠ©æ‰‹å®Œæ•´ç¤ºä¾‹

### å·¥ä½œæµè®¾è®¡

å†™ä½œåŠ©æ‰‹é¡¹ç›®é›†æˆäº†æ‰€æœ‰ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼Œå±•ç¤ºäº†å®Œæ•´çš„äººæœºåä½œå·¥ä½œæµï¼š

```
ç”¨æˆ·è¾“å…¥ä¸»é¢˜
    â†“
ç”Ÿæˆå¤§çº² â†’ [ä¸­æ–­ç¡®è®¤] â†’ æœç´¢èµ„æ–™ â†’ [ä¸­æ–­ç­›é€‰] 
    â†“
ç”Ÿæˆæ–‡ç«  â†’ [ä¸­æ–­å®¡æ ¸] â†’ æœ€ç»ˆç¡®è®¤ â†’ [ä¸­æ–­å‘å¸ƒ]
    â†“
å‘å¸ƒå®Œæˆ
```

### å…³é”®é›†æˆç‚¹

#### 1. å¤§çº²ç”Ÿæˆï¼ˆæµå¼ + ä¸­æ–­ï¼‰
```python
async def generate_outline_with_approval(state):
    # å¼‚æ­¥æµå¼ç”Ÿæˆå¤§çº²
    outline = await generate_outline_async(state["topic"])
    
    # ä¸­æ–­ç­‰å¾…ç”¨æˆ·ç¡®è®¤
    user_decision = interrupt({
        "type": "outline_approval",
        "outline": outline,
        "options": ["approve", "modify", "regenerate"]
    })
    
    return handle_outline_decision(state, user_decision)
```

#### 2. æ–‡ç« ç”Ÿæˆï¼ˆå¼‚æ­¥æµå¼è¾“å‡ºï¼‰
```python
async def generate_article_streaming(state, config):
    llm = ChatOpenAI(...)
    
    full_article = ""
    # ä½¿ç”¨æœ€ä½³ç»„åˆï¼šå¼‚æ­¥æµå¼
    async for chunk in llm.astream(messages, config):
        if chunk.content:
            full_article += chunk.content
            # å®æ—¶æ˜¾ç¤ºç”Ÿæˆè¿›åº¦
    
    return {"article": full_article}
```

#### 3. å®Œæ•´å·¥ä½œæµ
```python
def create_writing_assistant():
    workflow = StateGraph(WritingState)
    
    # æ·»åŠ å¸¦ä¸­æ–­çš„èŠ‚ç‚¹
    workflow.add_node("outline", generate_outline_with_approval)
    workflow.add_node("search", search_and_select_sources)
    workflow.add_node("article", generate_article_streaming)
    workflow.add_node("publish", final_publish_confirmation)
    
    # è®¾ç½®æµç¨‹
    workflow.set_entry_point("outline")
    workflow.add_edge("outline", "search")
    workflow.add_edge("search", "article")
    workflow.add_edge("article", "publish")
    workflow.add_edge("publish", END)
    
    return workflow.compile(checkpointer=InMemorySaver())
```

## ğŸ’¡ ç»¼åˆæœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
- **å¼‚æ­¥ä¼˜å…ˆ**: é«˜å¹¶å‘åœºæ™¯ä½¿ç”¨å¼‚æ­¥èŠ‚ç‚¹å’Œå¼‚æ­¥LLMè°ƒç”¨
- **æµå¼è¾“å‡º**: éœ€è¦å®æ—¶åé¦ˆæ—¶ä½¿ç”¨`llm.stream()`æˆ–`llm.astream()`
- **åˆç†ç¼“å­˜**: é¿å…é‡å¤çš„æ˜‚è´µæ“ä½œ

### 2. ç”¨æˆ·ä½“éªŒ
- **åŠæ—¶åé¦ˆ**: ä½¿ç”¨æµå¼è¾“å‡ºæä¾›å®æ—¶è¿›åº¦
- **æ¸…æ™°æç¤º**: ä¸­æ–­æ—¶æä¾›è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡ä¿¡æ¯
- **çµæ´»æ§åˆ¶**: ç»™ç”¨æˆ·å¤šç§å†³ç­–é€‰é¡¹

### 3. é”™è¯¯å¤„ç†
- **ä¼˜é›…é™çº§**: æµå¼è¾“å‡ºå¤±è´¥æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆ
- **çŠ¶æ€æ¢å¤**: ä¸­æ–­åçš„çŠ¶æ€ä¸€è‡´æ€§ä¿è¯
- **å¼‚å¸¸æ•è·**: å®Œå–„çš„å¼‚å¸¸å¤„ç†æœºåˆ¶

### 4. å¼€å‘è°ƒè¯•
- **é™æ€ä¸­æ–­**: å¼€å‘é˜¶æ®µä½¿ç”¨interrupt_before/afterè°ƒè¯•
- **æ—¥å¿—è®°å½•**: è¯¦ç»†è®°å½•ç”¨æˆ·å†³ç­–å’Œæ‰§è¡Œå†å²
- **çŠ¶æ€ç›‘æ§**: ç›‘æ§å·¥ä½œæµçš„æ‰§è¡ŒçŠ¶æ€

## ğŸ¯ æ€»ç»“

é€šè¿‡æœ¬æ•™ç¨‹çš„æ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬æŒæ¡äº†LangGraphçš„ä¸‰ä¸ªæ ¸å¿ƒåŠŸèƒ½ï¼š

1. **æµå¼è¾“å‡º** - æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®æŠ€æœ¯
2. **åŒæ­¥å¼‚æ­¥** - æ€§èƒ½ä¼˜åŒ–çš„é‡è¦é€‰æ‹©
3. **ä¸­æ–­æœºåˆ¶** - äººæœºåä½œçš„å¼ºå¤§å·¥å…·

### ğŸ” é‡è¦å‘ç°

ç‰¹åˆ«å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæˆ‘ä»¬é€šè¿‡è¯¦ç»†éªŒè¯æµ‹è¯•å‘ç°äº†ä¸€ä¸ª**å½±å“ç”Ÿäº§ç¯å¢ƒçš„å…³é”®é—®é¢˜**ï¼š

**Interrupt èŠ‚ç‚¹é‡å¤æ‰§è¡Œé—®é¢˜** - åŒ…å« `interrupt()` çš„èŠ‚ç‚¹åœ¨ç”¨æˆ·è¾“å…¥åä¼šå®Œå…¨é‡æ–°æ‰§è¡Œï¼Œå¯¼è‡´ï¼š
- å¤§æ¨¡å‹é‡å¤è°ƒç”¨ï¼ˆæ— è®ºä½¿ç”¨ `invoke()` è¿˜æ˜¯ `astream()`ï¼‰
- APIæˆæœ¬å¯èƒ½ç¿»å€
- ç”¨æˆ·è¾“å…¥å¤„ç†å»¶è¿Ÿ

**è§£å†³æ–¹æ¡ˆ**: åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å¿…é¡»é‡‡ç”¨åˆ†ç¦»å¤§æ¨¡å‹è°ƒç”¨å’Œç”¨æˆ·äº¤äº’çš„è®¾è®¡æ¨¡å¼ï¼Œæˆ–ä½¿ç”¨ç¼“å­˜æœºåˆ¶æ¥é¿å…ä¸å¿…è¦çš„é‡å¤è°ƒç”¨ã€‚

è¿™äº›åŠŸèƒ½çš„åˆç†ç»„åˆï¼ŒåŠ ä¸Šå¯¹æ½œåœ¨é—®é¢˜çš„æ·±å…¥ç†è§£ï¼Œå¯ä»¥æ„å»ºå‡ºæ—¢é«˜æ•ˆåˆç”¨æˆ·å‹å¥½çš„AIåº”ç”¨ç³»ç»Ÿã€‚

### ğŸ”— ç›¸å…³æ–‡ä»¶
- `TEST_Streaming_Modes.py` - æµå¼è¾“å‡ºåŠŸèƒ½æµ‹è¯•
- `TEST_Sync_Async_Performance.py` - åŒæ­¥å¼‚æ­¥æ€§èƒ½æµ‹è¯•
- `TEST_Interrupt_Mechanisms.py` - ä¸­æ–­æœºåˆ¶åŠŸèƒ½æµ‹è¯•
- `DEMO_Writing_Assistant.py` - å®Œæ•´åº”ç”¨ç¤ºä¾‹

### ğŸ§ª éªŒè¯æµ‹è¯•æ–‡ä»¶
- `test_interrupt_with_llm.py` - éªŒè¯å¤§æ¨¡å‹é‡å¤è°ƒç”¨é—®é¢˜çš„è¯¦ç»†æµ‹è¯•
- `test_interrupt_user_input.py` - éªŒè¯ç”¨æˆ·è¾“å…¥å¤„ç†å»¶è¿Ÿé—®é¢˜
- `test_invoke_vs_astream.py` - å¯¹æ¯”invokeå’Œastreamåœ¨interruptä¸­çš„è¡Œä¸º
- `optimized_interrupt_solution.py` - å±•ç¤ºä¼˜åŒ–è§£å†³æ–¹æ¡ˆçš„å®Œæ•´ç¤ºä¾‹

### ğŸ“š è¯¦ç»†æŒ‡å—
- `GUIDE_Streaming_Best_Practices.md` - æµå¼è¾“å‡ºè¯¦ç»†æŒ‡å—
- `GUIDE_Sync_Async_Patterns.md` - åŒæ­¥å¼‚æ­¥è¯¦ç»†æŒ‡å—
- `GUIDE_Human_In_Loop.md` - ä¸­æ–­æœºåˆ¶è¯¦ç»†æŒ‡å—

---

**ğŸ‰ æ­å–œï¼ä½ ç°åœ¨å·²ç»æŒæ¡äº†LangGraphçš„æ ¸å¿ƒåŠŸèƒ½ï¼Œå¯ä»¥æ„å»ºå¼ºå¤§çš„äººæœºåä½œAIåº”ç”¨äº†ï¼**
