#!/usr/bin/env python3
"""
TEST_Sync_Async_Performance.py - LangGraphåŒæ­¥å¼‚æ­¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•
æµ‹è¯•ä¸åŒç»„åˆæ–¹å¼çš„å…¼å®¹æ€§ã€æ€§èƒ½å’Œæµå¼æ•ˆæœ
é‡è¦å‘ç°: LLMè°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæœï¼ŒGraphæµå¼æ¨¡å¼åªæ˜¯ä¼ è¾“ç®¡é“
"""

import time
import asyncio
from typing import Dict, Any, TypedDict
from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver


class PerformanceTestState(TypedDict):
    """æ€§èƒ½æµ‹è¯•çŠ¶æ€å®šä¹‰"""
    input_text: str
    result: str
    current_step: str
    performance_data: Dict[str, Any]


def create_mock_llm_response(text: str, delay: float = 1.0) -> str:
    """æ¨¡æ‹ŸLLMå“åº”"""
    time.sleep(delay)
    return f"å¤„ç†ç»“æœ: {text} (è€—æ—¶ {delay}s)"


async def create_mock_llm_response_async(text: str, delay: float = 1.0) -> str:
    """æ¨¡æ‹Ÿå¼‚æ­¥LLMå“åº”"""
    await asyncio.sleep(delay)
    return f"å¼‚æ­¥å¤„ç†ç»“æœ: {text} (è€—æ—¶ {delay}s)"


def mock_llm_stream(text: str, chunk_count: int = 5):
    """æ¨¡æ‹ŸLLMæµå¼å“åº”"""
    words = text.split()
    chunk_size = max(1, len(words) // chunk_count)
    
    for i in range(0, len(words), chunk_size):
        chunk_words = words[i:i + chunk_size]
        chunk_text = " ".join(chunk_words)
        time.sleep(0.3)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
        yield chunk_text


async def mock_llm_astream(text: str, chunk_count: int = 5):
    """æ¨¡æ‹Ÿå¼‚æ­¥LLMæµå¼å“åº”"""
    words = text.split()
    chunk_size = max(1, len(words) // chunk_count)
    
    for i in range(0, len(words), chunk_size):
        chunk_words = words[i:i + chunk_size]
        chunk_text = " ".join(chunk_words)
        await asyncio.sleep(0.3)  # æ¨¡æ‹Ÿç”Ÿæˆå»¶è¿Ÿ
        yield chunk_text


# åœºæ™¯1: def + invoke + stream (å‡æµå¼)
def sync_node_with_invoke(state: PerformanceTestState) -> PerformanceTestState:
    """åŒæ­¥èŠ‚ç‚¹ + æ¨¡æ‹Ÿinvokeè°ƒç”¨"""
    start_time = time.time()
    
    # æ¨¡æ‹Ÿinvokeè°ƒç”¨
    response = create_mock_llm_response(state["input_text"], 1.0)
    
    end_time = time.time()
    
    state["result"] = response
    state["current_step"] = "sync_invoke_completed"
    state["performance_data"] = {
        "method": "sync_invoke",
        "response_time": end_time - start_time,
        "streaming": False,
        "user_experience": "å·® - éœ€è¦ç­‰å¾…å®Œæˆ"
    }
    
    return state


# åœºæ™¯2: def + stream + stream (çœŸæµå¼)
def sync_node_with_stream(state: PerformanceTestState) -> PerformanceTestState:
    """åŒæ­¥èŠ‚ç‚¹ + æ¨¡æ‹Ÿstreamè°ƒç”¨"""
    start_time = time.time()
    first_chunk_time = None
    full_response = ""
    
    # æ¨¡æ‹Ÿstreamè°ƒç”¨
    for chunk in mock_llm_stream(state["input_text"]):
        if first_chunk_time is None:
            first_chunk_time = time.time()
        full_response += chunk + " "
    
    end_time = time.time()
    
    state["result"] = full_response.strip()
    state["current_step"] = "sync_stream_completed"
    state["performance_data"] = {
        "method": "sync_stream",
        "total_time": end_time - start_time,
        "first_chunk_time": first_chunk_time - start_time if first_chunk_time else 0,
        "streaming": True,
        "user_experience": "å¥½ - å®æ—¶çœ‹åˆ°è¾“å‡º"
    }
    
    return state


# åœºæ™¯3: async def + ainvoke + astream (å¼‚æ­¥éæµå¼)
async def async_node_with_ainvoke(state: PerformanceTestState, config=None) -> PerformanceTestState:
    """å¼‚æ­¥èŠ‚ç‚¹ + æ¨¡æ‹Ÿainvokeè°ƒç”¨"""
    start_time = time.time()
    
    # æ¨¡æ‹Ÿainvokeè°ƒç”¨
    response = await create_mock_llm_response_async(state["input_text"], 1.0)
    
    end_time = time.time()
    
    state["result"] = response
    state["current_step"] = "async_invoke_completed"
    state["performance_data"] = {
        "method": "async_invoke",
        "response_time": end_time - start_time,
        "streaming": False,
        "user_experience": "å¥½ - å¼‚æ­¥æ€§èƒ½ä¼˜ç§€"
    }
    
    return state


# åœºæ™¯4: async def + astream + astream (å¼‚æ­¥æµå¼ - æœ€ä½³)
async def async_node_with_astream(state: PerformanceTestState, config=None) -> PerformanceTestState:
    """å¼‚æ­¥èŠ‚ç‚¹ + æ¨¡æ‹Ÿastreamè°ƒç”¨"""
    start_time = time.time()
    first_chunk_time = None
    full_response = ""
    
    # æ¨¡æ‹Ÿastreamè°ƒç”¨
    async for chunk in mock_llm_astream(state["input_text"]):
        if first_chunk_time is None:
            first_chunk_time = time.time()
        full_response += chunk + " "
    
    end_time = time.time()
    
    state["result"] = full_response.strip()
    state["current_step"] = "async_stream_completed"
    state["performance_data"] = {
        "method": "async_stream",
        "total_time": end_time - start_time,
        "first_chunk_time": first_chunk_time - start_time if first_chunk_time else 0,
        "streaming": True,
        "user_experience": "ğŸ† æœ€ä½³ - å¼‚æ­¥+æµå¼"
    }
    
    return state


def create_test_graphs():
    """åˆ›å»ºæµ‹è¯•å›¾"""
    graphs = {}
    
    # åœºæ™¯1: åŒæ­¥invoke
    workflow1 = StateGraph(PerformanceTestState)
    workflow1.add_node("process", sync_node_with_invoke)
    workflow1.set_entry_point("process")
    workflow1.add_edge("process", END)
    graphs["sync_invoke"] = workflow1.compile(checkpointer=MemorySaver())
    
    # åœºæ™¯2: åŒæ­¥stream
    workflow2 = StateGraph(PerformanceTestState)
    workflow2.add_node("process", sync_node_with_stream)
    workflow2.set_entry_point("process")
    workflow2.add_edge("process", END)
    graphs["sync_stream"] = workflow2.compile(checkpointer=MemorySaver())
    
    # åœºæ™¯3: å¼‚æ­¥invoke
    workflow3 = StateGraph(PerformanceTestState)
    workflow3.add_node("process", async_node_with_ainvoke)
    workflow3.set_entry_point("process")
    workflow3.add_edge("process", END)
    graphs["async_invoke"] = workflow3.compile(checkpointer=MemorySaver())
    
    # åœºæ™¯4: å¼‚æ­¥stream
    workflow4 = StateGraph(PerformanceTestState)
    workflow4.add_node("process", async_node_with_astream)
    workflow4.set_entry_point("process")
    workflow4.add_edge("process", END)
    graphs["async_stream"] = workflow4.compile(checkpointer=MemorySaver())
    
    return graphs


def test_sync_scenarios():
    """æµ‹è¯•åŒæ­¥åœºæ™¯"""
    print("âš¡ åŒæ­¥åœºæ™¯æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    graphs = create_test_graphs()
    initial_state = PerformanceTestState(
        input_text="è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ç”¨äºéªŒè¯åŒæ­¥å¼‚æ­¥è°ƒç”¨æ•ˆæœ",
        result="",
        current_step="initialized",
        performance_data={}
    )
    
    # æµ‹è¯•åœºæ™¯1: sync invoke
    print("ğŸ“ åœºæ™¯1: def + invoke + stream (å‡æµå¼)")
    result1 = graphs["sync_invoke"].invoke(initial_state, {"configurable": {"thread_id": "test1"}})
    perf1 = result1["performance_data"]
    print(f"   æ–¹æ³•: {perf1['method']}")
    print(f"   å“åº”æ—¶é—´: {perf1['response_time']:.2f}s")
    print(f"   æµå¼æ•ˆæœ: {'âœ…' if perf1['streaming'] else 'âŒ'}")
    print(f"   ç”¨æˆ·ä½“éªŒ: {perf1['user_experience']}")
    
    # æµ‹è¯•åœºæ™¯2: sync stream
    print("\nğŸ“ åœºæ™¯2: def + stream + stream (çœŸæµå¼)")
    result2 = graphs["sync_stream"].invoke(initial_state, {"configurable": {"thread_id": "test2"}})
    perf2 = result2["performance_data"]
    print(f"   æ–¹æ³•: {perf2['method']}")
    print(f"   æ€»æ—¶é—´: {perf2['total_time']:.2f}s")
    print(f"   é¦–chunkæ—¶é—´: {perf2['first_chunk_time']:.2f}s")
    print(f"   æµå¼æ•ˆæœ: {'âœ…' if perf2['streaming'] else 'âŒ'}")
    print(f"   ç”¨æˆ·ä½“éªŒ: {perf2['user_experience']}")


async def test_async_scenarios():
    """æµ‹è¯•å¼‚æ­¥åœºæ™¯"""
    print("\nğŸš€ å¼‚æ­¥åœºæ™¯æ€§èƒ½æµ‹è¯•")
    print("-" * 40)
    
    graphs = create_test_graphs()
    initial_state = PerformanceTestState(
        input_text="è¿™æ˜¯æµ‹è¯•æ–‡æœ¬ç”¨äºéªŒè¯åŒæ­¥å¼‚æ­¥è°ƒç”¨æ•ˆæœ",
        result="",
        current_step="initialized",
        performance_data={}
    )
    
    # æµ‹è¯•åœºæ™¯3: async invoke
    print("ğŸ“ åœºæ™¯3: async def + ainvoke + astream (å¼‚æ­¥éæµå¼)")
    result3 = await graphs["async_invoke"].ainvoke(initial_state, {"configurable": {"thread_id": "test3"}})
    perf3 = result3["performance_data"]
    print(f"   æ–¹æ³•: {perf3['method']}")
    print(f"   å“åº”æ—¶é—´: {perf3['response_time']:.2f}s")
    print(f"   æµå¼æ•ˆæœ: {'âœ…' if perf3['streaming'] else 'âŒ'}")
    print(f"   ç”¨æˆ·ä½“éªŒ: {perf3['user_experience']}")
    
    # æµ‹è¯•åœºæ™¯4: async stream
    print("\nğŸ“ åœºæ™¯4: async def + astream + astream (å¼‚æ­¥æµå¼)")
    result4 = await graphs["async_stream"].ainvoke(initial_state, {"configurable": {"thread_id": "test4"}})
    perf4 = result4["performance_data"]
    print(f"   æ–¹æ³•: {perf4['method']}")
    print(f"   æ€»æ—¶é—´: {perf4['total_time']:.2f}s")
    print(f"   é¦–chunkæ—¶é—´: {perf4['first_chunk_time']:.2f}s")
    print(f"   æµå¼æ•ˆæœ: {'âœ…' if perf4['streaming'] else 'âŒ'}")
    print(f"   ç”¨æˆ·ä½“éªŒ: {perf4['user_experience']}")


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ¯ LangGraphåŒæ­¥å¼‚æ­¥æ€§èƒ½å¯¹æ¯”æµ‹è¯•")
    print("=" * 70)
    print("ğŸ”¬ æ ¸å¿ƒå‘ç°: LLMè°ƒç”¨æ–¹å¼å†³å®šæµå¼æ•ˆæœï¼ŒGraphæµå¼æ¨¡å¼åªæ˜¯ä¼ è¾“ç®¡é“")
    print("=" * 70)
    
    # æµ‹è¯•åŒæ­¥åœºæ™¯
    test_sync_scenarios()
    
    # æµ‹è¯•å¼‚æ­¥åœºæ™¯
    asyncio.run(test_async_scenarios())
    
    print("\nğŸ’¡ æ€§èƒ½å¯¹æ¯”æ€»ç»“:")
    print("   âŒ def + invoke(): å‡æµå¼ï¼Œç”¨æˆ·ä½“éªŒå·®")
    print("   âœ… def + stream(): çœŸæµå¼ï¼Œç”¨æˆ·ä½“éªŒå¥½")
    print("   âš¡ async def + ainvoke(): å¼‚æ­¥æ€§èƒ½ï¼Œéæµå¼")
    print("   ğŸ† async def + astream(): å¼‚æ­¥æµå¼ï¼Œæœ€ä½³ä½“éªŒ")
    
    print("\nğŸ¯ æœ€ä½³å®è·µ:")
    print("   éœ€è¦æµå¼è¾“å‡º â†’ ä½¿ç”¨ stream() æˆ– astream()")
    print("   éœ€è¦é«˜å¹¶å‘ â†’ ä½¿ç”¨ async def + ainvoke/astream")
    print("   æœ€ä½³ç»„åˆ â†’ async def + astream() + graph.astream()")


if __name__ == "__main__":
    main()
