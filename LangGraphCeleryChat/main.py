"""
LangGraph Celery Chat - ä¼˜åŒ–ç®€åŒ–ç‰ˆ
å‚è€ƒ ReActAgentsTest çš„ç®€æ´å®ç°ï¼Œä¿æŒæ ¸å¿ƒ graph ä»£ç ä¸å˜
æ€»ä»£ç é‡ < 500 è¡Œ
"""

import json
import uuid
import time
import logging
import asyncio
from typing import Dict, Any, Optional, cast
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware  
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from celery import Celery
import redis
from redis import asyncio as aioredis
from langchain_core.runnables import RunnableConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# é…ç½®å’Œåˆå§‹åŒ–
# ============================================================================

# Redis é…ç½®
REDIS_URL = "redis://default:mfzstl2v@dbconn.sealoshzh.site:41277"
redis_client = redis.Redis.from_url(REDIS_URL, decode_responses=True)

# å¼‚æ­¥Rediså®¢æˆ·ç«¯ (ç”¨äºäº‹ä»¶æµ)
async_redis_client = None

async def get_async_redis():
    """è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯"""
    global async_redis_client
    if async_redis_client is None:
        async_redis_client = aioredis.from_url(REDIS_URL, decode_responses=True)
    return async_redis_client

# Celery é…ç½®
celery_app = Celery(
    "writing_tasks",
    broker=REDIS_URL,
    backend=REDIS_URL,
    include=["main"]  # åŒ…å«å½“å‰æ¨¡å—
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    result_expires=3600,
)

# ============================================================================
# è¯·æ±‚æ¨¡å‹
# ============================================================================

class TaskRequest(BaseModel):
    user_id: str
    topic: str
    max_words: int = 2000
    style: str = "professional"
    language: str = "zh"
    mode: str = "interactive"

class ResumeRequest(BaseModel):
    response: str = "yes"
    approved: bool = True

# ============================================================================
# Celery ä»»åŠ¡
# ============================================================================



async def _process_stream_chunk(chunk, task_id):
    """å¤„ç†æµå¼è¾“å‡ºçš„å•ä¸ª chunk - æå–çš„å…¬å…±å‡½æ•°"""
    try:
        event_data = None

        if isinstance(chunk, (list, tuple)) and len(chunk) == 2:
            stream_type, data = chunk

            if stream_type == "updates" and isinstance(data, dict):
                # å¤„ç†æ›´æ–°äº‹ä»¶
                step_name = list(data.keys())[0] if data else "unknown"
                step_data = data.get(step_name, {})

                content_info = {}
                if isinstance(step_data, dict):
                    # æå–æ¶ˆæ¯å†…å®¹
                    if 'messages' in step_data:
                        messages = step_data['messages']
                        if messages and len(messages) > 0:
                            last_msg = messages[-1]
                            if hasattr(last_msg, 'content'):
                                content = last_msg.content
                                content_info = {
                                    "content_preview": content[:500] + "..." if len(content) > 500 else content,
                                    "content_length": len(content),
                                    "message_type": type(last_msg).__name__
                                }

                    # æå–å…¶ä»–æœ‰ç”¨ä¿¡æ¯
                    for key, value in step_data.items():
                        if key != 'messages' and isinstance(value, (str, int, float, bool)):
                            content_info[key] = value

                event_data = {
                    "type": "progress_update",
                    "task_id": task_id,
                    "step": step_name,
                    "content_info": content_info,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }

            elif stream_type == "custom" and isinstance(data, dict):
                # å¤„ç†è‡ªå®šä¹‰äº‹ä»¶ - ç‰¹åˆ«å¤„ç†åŒ…å«å¤æ‚å†…å®¹çš„äº‹ä»¶
                event_data = {
                    "type": "custom_event",
                    "task_id": task_id,
                    "timestamp": datetime.now().isoformat(),
                    "step": data.get("step", "unknown"),
                    "status": data.get("status", ""),
                    "progress": data.get("progress", 0)
                }

                # å¦‚æœæœ‰ current_contentï¼Œç‰¹åˆ«å¤„ç†
                if "current_content" in data:
                    current_content = data["current_content"]
                    if isinstance(current_content, dict):
                        # æå–å…³é”®ä¿¡æ¯ç”¨äºå‰ç«¯æ˜¾ç¤º
                        content_summary = {
                            "title": current_content.get("title", ""),
                            "sections_count": len(current_content.get("sections", [])),
                            "has_content": bool(current_content)
                        }

                        # å¦‚æœæœ‰ç« èŠ‚ï¼Œæå–ç« èŠ‚æ ‡é¢˜
                        if "sections" in current_content:
                            sections = current_content["sections"]
                            if isinstance(sections, list) and sections:
                                content_summary["section_titles"] = [
                                    section.get("title", "") for section in sections[:5]  # åªå–å‰5ä¸ª
                                ]

                        event_data["content_summary"] = content_summary
                        event_data["full_content"] = current_content  # ä¿ç•™å®Œæ•´å†…å®¹
                    else:
                        event_data["current_content"] = current_content

                # æ·»åŠ å…¶ä»–å­—æ®µ
                for key, value in data.items():
                    if key not in ["step", "status", "progress", "current_content"]:
                        event_data[key] = value
            else:
                # å…¶ä»–ç±»å‹çš„è¾“å‡º
                event_data = {
                    "type": "raw_output",
                    "task_id": task_id,
                    "stream_type": stream_type,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }
        else:
            # éå…ƒç»„æ ¼å¼çš„è¾“å‡º
            event_data = {
                "type": "raw_output",
                "task_id": task_id,
                "data": chunk,
                "timestamp": datetime.now().isoformat()
            }

        # å†™å…¥äº‹ä»¶æµ
        if event_data:
            redis_client.xadd(
                f"events:{task_id}",
                {
                    "timestamp": str(time.time()),
                    "data": json.dumps(event_data, default=str, ensure_ascii=False)
                }
            )

        return event_data

    except Exception as e:
        logger.error(f"å¤„ç†æµå¼è¾“å‡ºå¤±è´¥: {e}")
        return None

def _check_for_interruption(chunk, task_id):
    """æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è¯·æ±‚ - æå–çš„å…¬å…±å‡½æ•°"""
    if isinstance(chunk, tuple) and len(chunk) == 2:
        stream_type, data = chunk
        if stream_type == "updates" and isinstance(data, dict) and "__interrupt__" in data:
            interrupt_info = data["__interrupt__"]

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæš‚åœ
            redis_client.hset(f"task:{task_id}", "status", "paused")

            # å‘é€ä¸­æ–­äº‹ä»¶ - æå–çœŸå®çš„ä¸­æ–­ä¿¡æ¯
            interrupt_event = {
                "type": "interrupt_request",
                "task_id": task_id,
                "timestamp": datetime.now().isoformat()
            }

            # æå–ä¸­æ–­ä¿¡æ¯ - å¤„ç† Interrupt å¯¹è±¡
            interrupt_data = None

            # å¤„ç† tuple æ ¼å¼çš„ä¸­æ–­ä¿¡æ¯
            if isinstance(interrupt_info, tuple) and len(interrupt_info) > 0:
                # interrupt_info æ˜¯ tupleï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ Interrupt å¯¹è±¡
                interrupt_obj = interrupt_info[0]
                if hasattr(interrupt_obj, 'value'):
                    interrupt_data = interrupt_obj.value
                else:
                    interrupt_data = {"message": str(interrupt_obj)}

            elif hasattr(interrupt_info, 'value'):
                # è¿™æ˜¯ä¸€ä¸ªç›´æ¥çš„ Interrupt å¯¹è±¡ï¼Œæå–å…¶ value
                interrupt_data = interrupt_info.value
            elif isinstance(interrupt_info, dict):
                # è¿™æ˜¯ä¸€ä¸ªæ™®é€šå­—å…¸
                interrupt_data = interrupt_info
            else:
                # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
                interrupt_data = {"message": str(interrupt_info)}

            # ä»ä¸­æ–­æ•°æ®ä¸­æå–å…·ä½“å†…å®¹
            if isinstance(interrupt_data, dict):
                interrupt_event.update({
                    "interrupt_type": interrupt_data.get("type", "confirmation"),
                    "title": interrupt_data.get("type", "éœ€è¦ç¡®è®¤"),
                    "message": interrupt_data.get("message", "è¯·ç¡®è®¤æ˜¯å¦ç»§ç»­"),
                    "instructions": interrupt_data.get("instructions", ""),
                    "interrupt_data": interrupt_data  # ä¿ç•™å®Œæ•´çš„ä¸­æ–­æ•°æ®ï¼ˆå·²ç»æ˜¯å¯åºåˆ—åŒ–çš„ï¼‰
                })
            else:
                # å›é€€åˆ°é»˜è®¤å€¼
                interrupt_event.update({
                    "interrupt_type": "confirmation",
                    "title": "éœ€è¦ç¡®è®¤",
                    "message": str(interrupt_data) if interrupt_data else "è¯·ç¡®è®¤æ˜¯å¦ç»§ç»­",
                    "instructions": "è¯·å›å¤ 'yes' æˆ– 'no'",
                    "interrupt_data": {"raw": str(interrupt_data)}
                })

            try:
                json_data = json.dumps(interrupt_event, ensure_ascii=False)


                redis_client.xadd(
                    f"events:{task_id}",
                    {
                        "timestamp": str(time.time()),
                        "data": json_data
                    }
                )
            except Exception as e:
                logger.error(f"å‘é€ä¸­æ–­äº‹ä»¶å¤±è´¥: {e}")

            return True
    return False

@celery_app.task(bind=True)
def execute_writing_task(self, user_id: str, session_id: str, task_id: str, config_data: Dict[str, Any]):
    """æ‰§è¡Œå†™ä½œä»»åŠ¡ - é‡æ„ç®€åŒ–ç‰ˆ"""

    async def run_workflow():
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            redis_client.hset(f"task:{task_id}", "status", "running")

            # æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹ä½¿ç”¨ AsyncRedisSaver
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from graph.graph import create_writing_assistant_graph

            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "topic": config_data.get("topic"),
                "user_id": user_id,
                "max_words": config_data.get("max_words", 2000),
                "style": config_data.get("style", "professional"),
                "language": config_data.get("language", "zh"),
                "mode": config_data.get("mode", "interactive"),
                "outline": None,
                "article": None,
                "search_results": [],
                "messages": [],
                "checkpointer": None
            }

            config = cast(RunnableConfig, {"configurable": {"thread_id": task_id}})
            logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_id}, ä¸»é¢˜: {config_data.get('topic')}")

            final_result = None
            interrupted = False
            # ä½¿ç”¨å®˜æ–¹æ¨èçš„ AsyncRedisSaver æ–¹å¼
            from langgraph.checkpoint.redis.aio import AsyncRedisSaver

            async with AsyncRedisSaver.from_conn_string(REDIS_URL) as checkpointer:
                await checkpointer.asetup()

                # åˆ›å»ºå¹¶ç¼–è¯‘å›¾ - å¼ºåˆ¶é‡æ–°åˆ›å»º
                workflow = create_writing_assistant_graph()
                graph = workflow.compile(checkpointer=checkpointer)
                logger.info(f"å›¾ç¼–è¯‘å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(workflow.nodes)}")

                # å¼‚æ­¥æµå¼æ‰§è¡Œ
                async for chunk in graph.astream(initial_state, config, stream_mode=["updates", "custom"]):
                    # å¤„ç†è¾“å‡º
                    await _process_stream_chunk(chunk, task_id)

                    # æ£€æŸ¥ä¸­æ–­
                    if _check_for_interruption(chunk, task_id):
                        interrupted = True
                        return {"interrupted": True, "task_id": task_id}

                    final_result = chunk

            # ä»»åŠ¡å®Œæˆå¤„ç†
            return await _handle_task_completion(task_id, final_result, interrupted)

        except Exception as e:
            return await _handle_task_failure(task_id, e)

    return asyncio.run(run_workflow())

async def _handle_task_completion(task_id: str, final_result, interrupted: bool):
    """å¤„ç†ä»»åŠ¡å®Œæˆ - æå–çš„å…¬å…±å‡½æ•°"""
    if not interrupted and final_result:
        # æå–ç»“æœ
        result_data = {}
        if isinstance(final_result, tuple) and len(final_result) == 2:
            _, data = final_result
            if isinstance(data, dict):
                # æŸ¥æ‰¾æ–‡ç« ç”ŸæˆèŠ‚ç‚¹çš„ç»“æœ
                for value in data.values():
                    if isinstance(value, dict):
                        result_data.update({
                            "outline": value.get("outline"),
                            "article": value.get("article"),
                            "search_results": value.get("search_results", [])
                        })
                        break

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        redis_client.hset(f"task:{task_id}", mapping={
            "status": "completed",
            "result": json.dumps(result_data, default=str, ensure_ascii=False),
            "completed_at": datetime.now().isoformat()
        })

        # å‘é€å®Œæˆäº‹ä»¶åˆ°äº‹ä»¶æµ
        completion_event = {
            "type": "task_complete",
            "task_id": task_id,
            "status": "completed",
            "result": result_data,
            "timestamp": datetime.now().isoformat()
        }

        try:
            redis_client.xadd(
                f"events:{task_id}",
                {
                    "timestamp": str(time.time()),
                    "data": json.dumps(completion_event, default=str, ensure_ascii=False)
                }
            )
        except Exception as e:
            logger.error(f"å‘é€å®Œæˆäº‹ä»¶å¤±è´¥: {e}")

        logger.info(f"ä»»åŠ¡å®Œæˆ: {task_id}")
        return {"completed": True, "result": result_data}

    return {"completed": False}

async def _handle_task_failure(task_id: str, error: Exception):
    """å¤„ç†ä»»åŠ¡å¤±è´¥ - æå–çš„å…¬å…±å‡½æ•°"""
    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task_id}, é”™è¯¯: {error}")
    redis_client.hset(f"task:{task_id}", mapping={
        "status": "failed",
        "error": str(error)
    })

    # å‘é€å¤±è´¥äº‹ä»¶åˆ°äº‹ä»¶æµ
    failure_event = {
        "type": "task_failed",
        "task_id": task_id,
        "status": "failed",
        "error": str(error),
        "timestamp": datetime.now().isoformat()
    }

    try:
        redis_client.xadd(
            f"events:{task_id}",
            {
                "timestamp": str(time.time()),
                "data": json.dumps(failure_event, default=str, ensure_ascii=False)
            }
        )
    except Exception as xe:
        logger.error(f"å‘é€å¤±è´¥äº‹ä»¶å¤±è´¥: {xe}")

    raise error


@celery_app.task(bind=True)
def resume_writing_task(self, user_id: str, session_id: str, task_id: str, user_response: str):
    """æ¢å¤å†™ä½œä»»åŠ¡ - å‚è€ƒ ReActAgentsTest çš„ç®€å•å®ç°"""
    
    async def resume_workflow():
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from graph.graph import create_writing_assistant_graph
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            redis_client.hset(f"task:{task_id}", "status", "running")     
            config = cast(RunnableConfig, {"configurable": {"thread_id": task_id}})
            # ä½¿ç”¨ä¸ execute_writing_task ç›¸åŒçš„ AsyncRedisSaver æ¨¡å¼
            from langgraph.types import Command
            interrupted = False
            # ä½¿ç”¨å®˜æ–¹æ¨èçš„ AsyncRedisSaver
            from langgraph.checkpoint.redis.aio import AsyncRedisSaver

            async with AsyncRedisSaver.from_conn_string(REDIS_URL) as checkpointer:
                await checkpointer.asetup()

                # åˆ›å»ºå¹¶ç¼–è¯‘å›¾ - å¼ºåˆ¶é‡æ–°åˆ›å»º
                workflow = create_writing_assistant_graph()
                graph = workflow.compile(checkpointer=checkpointer)

                async for chunk in graph.astream(Command(resume=user_response), config, stream_mode=["updates", "custom"]):
                    print("--------------------------------")
                    print("æ¢å¤ä»»åŠ¡æ”¶åˆ° chunk", chunk)
                    print("--------------------------------")
                    # ç®€å•è®°å½• chunk å†…å®¹
                    if isinstance(chunk, tuple) and len(chunk) == 2:
                        stream_type, data = chunk
                        logger.info(f"  æµç±»å‹: {stream_type}")
                        if isinstance(data, dict):
                            logger.info(f"  æ•°æ®é”®: {list(data.keys())}")
                            if stream_type == "updates":
                                # è®°å½•èŠ‚ç‚¹æ‰§è¡Œ
                                node_names = [k for k in data.keys() if k != "__interrupt__"]
                                if node_names:
                                    logger.info(f"  æ‰§è¡ŒèŠ‚ç‚¹: {node_names}")

                    # å†™å…¥äº‹ä»¶æµ
                    redis_client.xadd(
                        f"events:{task_id}",
                        {
                            "timestamp": str(time.time()),
                            "data": json.dumps(chunk, default=str, ensure_ascii=False)
                        }
                    )

                    # # æ£€æŸ¥ä¸­æ–­ - ä½¿ç”¨ç»Ÿä¸€çš„ä¸­æ–­å¤„ç†å‡½æ•°
                    is_interrupt = _check_for_interruption(chunk, task_id)
                    if is_interrupt:
                        interrupted = True
                        return {"interrupted": True, "task_id": task_id}

            # å¤„ç†å®Œæˆç»“æœ
                logger.info(f"ğŸ” æ¢å¤ä»»åŠ¡ç»“æŸæ£€æŸ¥: interrupted={interrupted}, chunk_count={chunk_count}")

                if not interrupted:
                    logger.info("âœ… ä»»åŠ¡æœªä¸­æ–­ï¼Œå¼€å§‹å¤„ç†å®Œæˆç»“æœ")
                    result_data = {}

                    # å¦‚æœæ²¡æœ‰å¤„ç†ä»»ä½• chunksï¼Œå°è¯•è·å–å½“å‰çŠ¶æ€
                    if chunk_count == 0:
                        logger.info("âš ï¸ æ²¡æœ‰å¤„ç†ä»»ä½• chunksï¼Œæ£€æŸ¥å½“å‰å›¾çŠ¶æ€...")
                        try:
                            # è·å–å½“å‰çŠ¶æ€
                            current_state = await checkpointer.aget_tuple(config)
                            if current_state and hasattr(current_state, 'checkpoint') and current_state.checkpoint:
                                state_data = current_state.checkpoint.get('channel_values', {})
                                logger.info(f"ğŸ“Š ä» checkpoint è·å–çŠ¶æ€é”®: {list(state_data.keys())}")

                                # æå–ç»“æœæ•°æ®
                                result_data = {
                                    "outline": state_data.get("outline"),
                                    "article": state_data.get("article"),
                                    "search_results": state_data.get("search_results", []),
                                    "topic": state_data.get("topic"),
                                    "enhancement_suggestions": state_data.get("enhancement_suggestions", [])
                                }

                                # æ£€æŸ¥æ˜¯å¦çœŸçš„å®Œæˆäº†
                                if result_data.get("article"):
                                    logger.info("âœ… æ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« ï¼Œä»»åŠ¡ç¡®å®å®Œæˆ")
                                else:
                                    logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« ï¼Œä»»åŠ¡å¯èƒ½æœªå®Œå…¨å®Œæˆ")
                            else:
                                logger.warning("âš ï¸ æ— æ³•è·å– checkpoint çŠ¶æ€")
                        except Exception as e:
                            logger.error(f"è·å– checkpoint çŠ¶æ€å¤±è´¥: {e}")

                    redis_client.hset(f"task:{task_id}", mapping={
                        "status": "completed",
                        "result": json.dumps(result_data, default=str, ensure_ascii=False)
                    })

                    logger.info(f"ğŸ“‹ ä»»åŠ¡å®Œæˆï¼Œç»“æœæ•°æ®é”®: {list(result_data.keys())}")
                    if result_data.get("article"):
                        article_length = len(result_data["article"]) if isinstance(result_data["article"], str) else 0
                        logger.info(f"âœ… æ–‡ç« ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {article_length} å­—ç¬¦")
                    else:
                        logger.warning("âš ï¸ ä»»åŠ¡å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆæ–‡ç« ")

                    logger.info(f"ğŸ¯ è¿”å› completed=Trueï¼Œresult é”®: {list(result_data.keys())}")
                    return {"completed": True, "result": result_data}
                else:
                    logger.info(f"ğŸ”„ ä»»åŠ¡è¢«ä¸­æ–­ï¼Œè¿”å› interrupted=True")

        except Exception as e:
            redis_client.hset(f"task:{task_id}", mapping={"status": "failed", "error": str(e)})
            raise
    
    return asyncio.run(resume_workflow())

# ============================================================================
# FastAPI åº”ç”¨
# ============================================================================

app = FastAPI(
    title="LangGraph-Celery-Redis-Stream",
    version="1.0.0",
    description="ç®€åŒ–ç‰ˆå®ç°ï¼Œä¿æŒæ ¸å¿ƒåŠŸèƒ½"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "LangGraph Celery Chat - ä¼˜åŒ–ç‰ˆ", "status": "running"}

@app.get("/health")
async def health():
    redis_status = "ok" if redis_client.ping() else "error"
    return {"status": "ok", "services": {"redis": redis_status, "celery": "ok"}}

# ============================================================================
# æ ¸å¿ƒ API
# ============================================================================

@app.post("/api/v1/tasks")
async def create_task(request: TaskRequest):
    """åˆ›å»ºä»»åŠ¡ - å‚è€ƒ ReActAgentsTest"""
    try:
        task_id = f"task_{uuid.uuid4().hex[:8]}"
        session_id = f"session_{request.user_id}_{int(time.time())}"
        
        # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
        task_data = {
            "task_id": task_id,
            "session_id": session_id,
            "user_id": request.user_id,
            "status": "pending",
            "created_at": datetime.now().isoformat(),
            "config": json.dumps(request.model_dump())
        }
        
        redis_client.hset(f"task:{task_id}", mapping=task_data)
        redis_client.expire(f"task:{task_id}", 3600)
        
        # å¯åŠ¨ Celery ä»»åŠ¡
        celery_task = execute_writing_task.delay(
            user_id=request.user_id,
            session_id=session_id,
            task_id=task_id,
            config_data=request.model_dump()
        )
        
        logger.info(f"åˆ›å»ºä»»åŠ¡: {task_id}")
        
        return {
            "task_id": task_id,
            "session_id": session_id,
            "status": "pending"
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        task_data = redis_client.hgetall(f"task:{task_id}")
        if not task_data:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # è§£æ JSON å­—æ®µ
        if "config" in task_data:
            task_data["config"] = json.loads(task_data["config"])
        if "result" in task_data:
            task_data["result"] = json.loads(task_data["result"])
            
        return task_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/tasks/{task_id}/resume")
async def resume_task(task_id: str, request: ResumeRequest):
    """æ¢å¤ä»»åŠ¡"""
    try:
        task_data = redis_client.hgetall(f"task:{task_id}")
        if not task_data:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        status = task_data.get("status")
        if status not in ["paused"]:
            raise HTTPException(status_code=400, detail=f"ä»»åŠ¡çŠ¶æ€ {status} ä¸æ”¯æŒæ¢å¤")
        
        # å¯åŠ¨æ¢å¤ä»»åŠ¡
        celery_task = resume_writing_task.delay(
            user_id=task_data.get("user_id"),
            session_id=task_data.get("session_id"),
            task_id=task_id,
            user_response=request.response
        )
        
        return {"message": "ä»»åŠ¡å·²æ¢å¤", "task_id": task_id}
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/events/{task_id}")
async def get_event_stream(task_id: str):
    """äº‹ä»¶æµ - çœŸæ­£çš„å¼‚æ­¥ç‰ˆæœ¬ (aioredis)"""

    async def event_generator():
        stream_name = f"events:{task_id}"
        last_id = "0"

        # è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯
        async_redis = await get_async_redis()

        # ç«‹å³å‘é€è¿æ¥ç¡®è®¤
        yield f"data: {json.dumps({'type': 'connected', 'task_id': task_id})}\n\n"

        try:
            # å¼‚æ­¥æ£€æŸ¥Redisè¿æ¥
            await async_redis.ping()
            yield f"data: {json.dumps({'type': 'debug', 'message': 'Redisè¿æ¥æ­£å¸¸'})}\n\n"
            # å¼‚æ­¥æ£€æŸ¥æµæ˜¯å¦å­˜åœ¨
            exists = await async_redis.exists(stream_name)
            yield f"data: {json.dumps({'type': 'debug', 'message': f'æµå­˜åœ¨: {exists}'})}\n\n"
            if exists:
                # å¼‚æ­¥è·å–æµé•¿åº¦
                length = await async_redis.xlen(stream_name)
                yield f"data: {json.dumps({'type': 'debug', 'message': f'æµé•¿åº¦: {length}'})}\n\n"

                # å¼‚æ­¥è¯»å–æ‰€æœ‰ç°æœ‰æ¶ˆæ¯
                all_messages = await async_redis.xrange(stream_name)
                for message_id, fields in all_messages:
                    try:
                        event_data = {
                            "id": message_id,
                            "timestamp": fields.get("timestamp"),
                            "data": json.loads(fields.get("data", "{}"))
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        last_id = message_id
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'error', 'message': f'è§£ææ¶ˆæ¯å¤±è´¥: {e}'})}\n\n"

            # å¼‚æ­¥ç›‘å¬æ–°æ¶ˆæ¯
            timeout_count = 0
            while timeout_count < 120:  # å¢åŠ åˆ°120æ¬¡ (2åˆ†é’Ÿ)
                # çœŸæ­£çš„å¼‚æ­¥xread - ä¸ä¼šé˜»å¡äº‹ä»¶å¾ªç¯ï¼
                try:
                    events = await async_redis.xread({stream_name: last_id}, count=10, block=1000)

                    if events:
                        timeout_count = 0  # é‡ç½®è¶…æ—¶è®¡æ•°
                        for stream, messages in events:
                            for message_id, fields in messages:
                                try:
                                    event_data = {
                                        "id": message_id,
                                        "timestamp": fields.get("timestamp"),
                                        "data": json.loads(fields.get("data", "{}"))
                                    }
                                    yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                                    last_id = message_id
                                except Exception as e:
                                    yield f"data: {json.dumps({'type': 'error', 'message': f'è§£ææ–°æ¶ˆæ¯å¤±è´¥: {e}'})}\n\n"
                    else:
                        timeout_count += 1
                        yield f"data: {json.dumps({'type': 'heartbeat', 'count': timeout_count})}\n\n"

                except asyncio.TimeoutError:
                    timeout_count += 1
                    yield f"data: {json.dumps({'type': 'heartbeat', 'count': timeout_count})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)