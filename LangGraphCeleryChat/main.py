"""
LangGraph Celery Chat - ä¼˜åŒ–ç®€åŒ–ç‰ˆ
å‚è€ƒ ReActAgentsTest çš„ç®€æ´å®ç°ï¼Œä¿æŒæ ¸å¿ƒ graph ä»£ç ä¸å˜
æ€»ä»£ç é‡ < 500 è¡Œ
"""

import json
import uuid
import time
import logging
import asyncio
from typing import Dict, Any, Optional, cast
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware  
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from celery import Celery
import redis
from redis import asyncio as aioredis
from langchain_core.runnables import RunnableConfig

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# é…ç½®å’Œåˆå§‹åŒ–
# ============================================================================

# Redis é…ç½®
REDIS_URL = "redis://default:mfzstl2v@dbconn.sealoshzh.site:41277"
redis_client = redis.Redis.from_url(REDIS_URL, decode_responses=True)

# å¼‚æ­¥Rediså®¢æˆ·ç«¯ (ç”¨äºäº‹ä»¶æµ)
async_redis_client = None

async def get_async_redis():
    """è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯"""
    global async_redis_client
    if async_redis_client is None:
        async_redis_client = aioredis.from_url(REDIS_URL, decode_responses=True)
    return async_redis_client

# Celery é…ç½®
celery_app = Celery(
    "writing_tasks",
    broker=REDIS_URL,
    backend=REDIS_URL,
    include=["main"]  # åŒ…å«å½“å‰æ¨¡å—
)

celery_app.conf.update(
    task_serializer="json",
    accept_content=["json"],
    result_serializer="json",
    timezone="UTC",
    enable_utc=True,
    result_expires=3600,
)

# ============================================================================
# è¯·æ±‚æ¨¡å‹
# ============================================================================

class TaskRequest(BaseModel):
    user_id: str
    topic: str
    max_words: int = 2000
    style: str = "professional"
    language: str = "zh"
    mode: str = "interactive"

class ResumeRequest(BaseModel):
    response: str = "yes"
    approved: bool = True

# ============================================================================
# Celery ä»»åŠ¡
# ============================================================================



async def _process_stream_chunk(chunk, task_id):
    """å¤„ç†æµå¼è¾“å‡ºçš„å•ä¸ª chunk - æå–çš„å…¬å…±å‡½æ•°"""
    try:
        event_data = None

        if isinstance(chunk, (list, tuple)) and len(chunk) == 2:
            stream_type, data = chunk

            if stream_type == "updates" and isinstance(data, dict):
                # å¤„ç†æ›´æ–°äº‹ä»¶
                step_name = list(data.keys())[0] if data else "unknown"
                step_data = data.get(step_name, {})

                content_info = {}
                if isinstance(step_data, dict):
                    # æå–æ¶ˆæ¯å†…å®¹
                    if 'messages' in step_data:
                        messages = step_data['messages']
                        if messages and len(messages) > 0:
                            last_msg = messages[-1]
                            if hasattr(last_msg, 'content'):
                                content = last_msg.content
                                content_info = {
                                    "content_preview": content[:500] + "..." if len(content) > 500 else content,
                                    "content_length": len(content),
                                    "message_type": type(last_msg).__name__
                                }

                    # æå–å…¶ä»–æœ‰ç”¨ä¿¡æ¯
                    for key, value in step_data.items():
                        if key != 'messages' and isinstance(value, (str, int, float, bool)):
                            content_info[key] = value

                event_data = {
                    "type": "progress_update",
                    "task_id": task_id,
                    "step": step_name,
                    "content_info": content_info,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }

            elif stream_type == "custom" and isinstance(data, dict):
                # å¤„ç†è‡ªå®šä¹‰äº‹ä»¶ - ç‰¹åˆ«å¤„ç†åŒ…å«å¤æ‚å†…å®¹çš„äº‹ä»¶
                event_data = {
                    "type": "custom_event",
                    "task_id": task_id,
                    "timestamp": datetime.now().isoformat(),
                    "step": data.get("step", "unknown"),
                    "status": data.get("status", ""),
                    "progress": data.get("progress", 0)
                }

                # å¦‚æœæœ‰ current_contentï¼Œç‰¹åˆ«å¤„ç†
                if "current_content" in data:
                    current_content = data["current_content"]
                    if isinstance(current_content, dict):
                        # æå–å…³é”®ä¿¡æ¯ç”¨äºå‰ç«¯æ˜¾ç¤º
                        content_summary = {
                            "title": current_content.get("title", ""),
                            "sections_count": len(current_content.get("sections", [])),
                            "has_content": bool(current_content)
                        }

                        # å¦‚æœæœ‰ç« èŠ‚ï¼Œæå–ç« èŠ‚æ ‡é¢˜
                        if "sections" in current_content:
                            sections = current_content["sections"]
                            if isinstance(sections, list) and sections:
                                content_summary["section_titles"] = [
                                    section.get("title", "") for section in sections[:5]  # åªå–å‰5ä¸ª
                                ]

                        event_data["content_summary"] = content_summary
                        event_data["full_content"] = current_content  # ä¿ç•™å®Œæ•´å†…å®¹
                    else:
                        event_data["current_content"] = current_content

                # æ·»åŠ å…¶ä»–å­—æ®µ
                for key, value in data.items():
                    if key not in ["step", "status", "progress", "current_content"]:
                        event_data[key] = value
            else:
                # å…¶ä»–ç±»å‹çš„è¾“å‡º
                event_data = {
                    "type": "raw_output",
                    "task_id": task_id,
                    "stream_type": stream_type,
                    "data": data,
                    "timestamp": datetime.now().isoformat()
                }
        else:
            # éå…ƒç»„æ ¼å¼çš„è¾“å‡º
            event_data = {
                "type": "raw_output",
                "task_id": task_id,
                "data": chunk,
                "timestamp": datetime.now().isoformat()
            }

        # å†™å…¥äº‹ä»¶æµ
        if event_data:
            redis_client.xadd(
                f"events:{task_id}",
                {
                    "timestamp": str(time.time()),
                    "data": json.dumps(event_data, default=str, ensure_ascii=False)
                }
            )

        return event_data

    except Exception as e:
        logger.error(f"å¤„ç†æµå¼è¾“å‡ºå¤±è´¥: {e}")
        return None

def _check_for_interruption(chunk, task_id):
    """æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­è¯·æ±‚ - æ”¹è¿›ç‰ˆæœ¬"""
    try:
        # è®°å½•åŸå§‹chunkç”¨äºè°ƒè¯•
        logger.debug(f"æ£€æŸ¥ä¸­æ–­ - chunkç±»å‹: {type(chunk)}, å†…å®¹: {chunk}")
        
        if isinstance(chunk, tuple) and len(chunk) == 2:
            stream_type, data = chunk
            logger.debug(f"æµç±»å‹: {stream_type}, æ•°æ®ç±»å‹: {type(data)}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ä¸­æ–­ä¿¡å·
            is_interrupt = False
            interrupt_info = None
            
            if stream_type == "updates" and isinstance(data, dict):
                # æ–¹å¼1ï¼šæ£€æŸ¥ __interrupt__ é”®
                if "__interrupt__" in data:
                    is_interrupt = True
                    interrupt_info = data["__interrupt__"]
                    logger.info(f"å‘ç°ä¸­æ–­ä¿¡å· (æ–¹å¼1): {interrupt_info}")
                
                # æ–¹å¼2ï¼šæ£€æŸ¥æ˜¯å¦æœ‰èŠ‚ç‚¹åŒ…å«ä¸­æ–­ä¿¡æ¯
                for node_name, node_data in data.items():
                    if isinstance(node_data, dict) and "interrupt" in str(node_data).lower():
                        is_interrupt = True
                        interrupt_info = node_data
                        logger.info(f"å‘ç°ä¸­æ–­ä¿¡å· (æ–¹å¼2) åœ¨èŠ‚ç‚¹ {node_name}: {interrupt_info}")
                        break
            
            elif stream_type == "custom" and isinstance(data, dict):
                # æ–¹å¼3ï¼šæ£€æŸ¥è‡ªå®šä¹‰äº‹ä»¶ä¸­çš„ä¸­æ–­
                if data.get("type") == "interrupt" or "interrupt" in data:
                    is_interrupt = True
                    interrupt_info = data
                    logger.info(f"å‘ç°ä¸­æ–­ä¿¡å· (æ–¹å¼3): {interrupt_info}")
            
            if not is_interrupt:
                # æ–¹å¼4ï¼šæ£€æŸ¥æ•´ä¸ªchunkæ˜¯å¦åŒ…å«ä¸­æ–­ç›¸å…³å†…å®¹
                chunk_str = str(chunk).lower()
                if "interrupt" in chunk_str and ("confirmation" in chunk_str or "permission" in chunk_str):
                    is_interrupt = True
                    interrupt_info = {"raw_chunk": chunk}
                    logger.info(f"å‘ç°ä¸­æ–­ä¿¡å· (æ–¹å¼4): åœ¨chunkå­—ç¬¦ä¸²ä¸­æ£€æµ‹åˆ°ä¸­æ–­")
            
            if is_interrupt:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºæš‚åœ
                redis_client.hset(f"task:{task_id}", "status", "paused")
                logger.info(f"ä»»åŠ¡ {task_id} çŠ¶æ€æ›´æ–°ä¸º paused")

                # æ„å»ºä¸­æ–­äº‹ä»¶
                interrupt_event = {
                    "type": "interrupt_request",
                    "task_id": task_id,
                    "timestamp": datetime.now().isoformat(),
                    "detected_by": "improved_detection"
                }

                # æå–ä¸­æ–­ä¿¡æ¯
                interrupt_data = _extract_interrupt_data(interrupt_info)
                interrupt_event.update(interrupt_data)

                # å‘é€ä¸­æ–­äº‹ä»¶
                try:
                    json_data = json.dumps(interrupt_event, ensure_ascii=False, default=str)
                    redis_client.xadd(
                        f"events:{task_id}",
                        {
                            "timestamp": str(time.time()),
                            "data": json_data
                        }
                    )
                    logger.info(f"ä¸­æ–­äº‹ä»¶å·²å‘é€: {interrupt_event.get('interrupt_type', 'unknown')}")
                except Exception as e:
                    logger.error(f"å‘é€ä¸­æ–­äº‹ä»¶å¤±è´¥: {e}")

                return True
        
        return False
        
    except Exception as e:
        logger.error(f"æ£€æŸ¥ä¸­æ–­æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return False

def _extract_interrupt_data(interrupt_info):
    """æå–ä¸­æ–­æ•°æ® - åˆ†ç¦»çš„è¾…åŠ©å‡½æ•°"""
    interrupt_data = {
        "interrupt_type": "confirmation",
        "title": "éœ€è¦ç¡®è®¤",
        "message": "è¯·ç¡®è®¤æ˜¯å¦ç»§ç»­",
        "instructions": "è¯·å›å¤ 'yes' æˆ– 'no'",
        "interrupt_data": {}
    }
    
    try:
        # å¤„ç†ä¸åŒç±»å‹çš„ä¸­æ–­ä¿¡æ¯
        if isinstance(interrupt_info, tuple) and len(interrupt_info) > 0:
            # interrupt_info æ˜¯ tupleï¼Œç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯ Interrupt å¯¹è±¡
            interrupt_obj = interrupt_info[0]
            if hasattr(interrupt_obj, 'value'):
                raw_data = interrupt_obj.value
            else:
                raw_data = {"message": str(interrupt_obj)}

        elif hasattr(interrupt_info, 'value'):
            # è¿™æ˜¯ä¸€ä¸ªç›´æ¥çš„ Interrupt å¯¹è±¡
            raw_data = interrupt_info.value
        elif isinstance(interrupt_info, dict):
            # è¿™æ˜¯ä¸€ä¸ªæ™®é€šå­—å…¸
            raw_data = interrupt_info
        else:
            # å…¶ä»–ç±»å‹ï¼Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
            raw_data = {"raw": str(interrupt_info)}

        # ä»åŸå§‹æ•°æ®ä¸­æå–ç»“æ„åŒ–ä¿¡æ¯
        if isinstance(raw_data, dict):
            interrupt_data.update({
                "interrupt_type": raw_data.get("type", "confirmation"),
                "title": raw_data.get("type", "éœ€è¦ç¡®è®¤"),
                "message": raw_data.get("message", "è¯·ç¡®è®¤æ˜¯å¦ç»§ç»­"),
                "instructions": raw_data.get("instructions", "è¯·å›å¤ 'yes' æˆ– 'no'"),
                "interrupt_data": raw_data
            })
        else:
            interrupt_data.update({
                "message": str(raw_data) if raw_data else "è¯·ç¡®è®¤æ˜¯å¦ç»§ç»­",
                "interrupt_data": {"raw": str(raw_data)}
            })
    
    except Exception as e:
        logger.error(f"æå–ä¸­æ–­æ•°æ®å¤±è´¥: {e}")
        interrupt_data["interrupt_data"] = {"error": str(e), "raw": str(interrupt_info)}
    
    return interrupt_data

@celery_app.task(bind=True)
def execute_writing_task(self, user_id: str, session_id: str, task_id: str, config_data: Dict[str, Any]):
    """æ‰§è¡Œå†™ä½œä»»åŠ¡ - é‡æ„ç®€åŒ–ç‰ˆ"""

    async def run_workflow():
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            redis_client.hset(f"task:{task_id}", "status", "running")

            # æŒ‰ç…§å®˜æ–¹ç¤ºä¾‹ä½¿ç”¨ AsyncRedisSaver
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from graph.graph import create_writing_assistant_graph

            # å‡†å¤‡åˆå§‹çŠ¶æ€
            initial_state = {
                "topic": config_data.get("topic"),
                "user_id": user_id,
                "max_words": config_data.get("max_words", 2000),
                "style": config_data.get("style", "professional"),
                "language": config_data.get("language", "zh"),
                "mode": config_data.get("mode", "interactive"),
                "outline": None,
                "article": None,
                "search_results": [],
                "messages": [],
                "checkpointer": None
            }

            config = cast(RunnableConfig, {"configurable": {"thread_id": task_id}})
            logger.info(f"å¼€å§‹æ‰§è¡Œä»»åŠ¡: {task_id}, ä¸»é¢˜: {config_data.get('topic')}")

            final_result = None
            interrupted = False
            # ä½¿ç”¨å®˜æ–¹æ¨èçš„ AsyncRedisSaver æ–¹å¼ - ä¿®å¤ç¯å¢ƒå˜é‡é—®é¢˜
            from langgraph.checkpoint.redis.aio import AsyncRedisSaver
            import os
            
            # ç¡®ä¿ç¯å¢ƒå˜é‡è®¾ç½®
            REDIS_URL = "redis://default:mfzstl2v@dbconn.sealoshzh.site:41277"
            
            async with AsyncRedisSaver.from_conn_string(REDIS_URL) as checkpointer:
                await checkpointer.asetup()

                # åˆ›å»ºå¹¶ç¼–è¯‘å›¾ - å¼ºåˆ¶é‡æ–°åˆ›å»º
                workflow = create_writing_assistant_graph()
                graph = workflow.compile(checkpointer=checkpointer)
                logger.info(f"å›¾ç¼–è¯‘å®Œæˆï¼ŒèŠ‚ç‚¹æ•°: {len(workflow.nodes)}")

                # å¼‚æ­¥æµå¼æ‰§è¡Œ
                async for chunk in graph.astream(initial_state, config, stream_mode=["updates", "custom"]):
                    # å¤„ç†è¾“å‡º
                    await _process_stream_chunk(chunk, task_id)

                    # æ£€æŸ¥ä¸­æ–­
                    if _check_for_interruption(chunk, task_id):
                        interrupted = True
                        return {"interrupted": True, "task_id": task_id}

                    final_result = chunk

            # ä»»åŠ¡å®Œæˆå¤„ç†
            return await _handle_task_completion(task_id, final_result, interrupted)

        except Exception as e:
            return await _handle_task_failure(task_id, e)

    return asyncio.run(run_workflow())

async def _handle_task_completion(task_id: str, final_result, interrupted: bool):
    """å¤„ç†ä»»åŠ¡å®Œæˆ - æå–çš„å…¬å…±å‡½æ•°"""
    if not interrupted and final_result:
        # æå–ç»“æœ
        result_data = {}
        if isinstance(final_result, tuple) and len(final_result) == 2:
            _, data = final_result
            if isinstance(data, dict):
                # æŸ¥æ‰¾æ–‡ç« ç”ŸæˆèŠ‚ç‚¹çš„ç»“æœ
                for value in data.values():
                    if isinstance(value, dict):
                        result_data.update({
                            "outline": value.get("outline"),
                            "article": value.get("article"),
                            "search_results": value.get("search_results", [])
                        })
                        break

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
        redis_client.hset(f"task:{task_id}", mapping={
            "status": "completed",
            "result": json.dumps(result_data, default=str, ensure_ascii=False),
            "completed_at": datetime.now().isoformat()
        })

        # å‘é€å®Œæˆäº‹ä»¶åˆ°äº‹ä»¶æµ
        completion_event = {
            "type": "task_complete",
            "task_id": task_id,
            "status": "completed",
            "result": result_data,
            "timestamp": datetime.now().isoformat()
        }

        try:
            redis_client.xadd(
                f"events:{task_id}",
                {
                    "timestamp": str(time.time()),
                    "data": json.dumps(completion_event, default=str, ensure_ascii=False)
                }
            )
        except Exception as e:
            logger.error(f"å‘é€å®Œæˆäº‹ä»¶å¤±è´¥: {e}")

        logger.info(f"ä»»åŠ¡å®Œæˆ: {task_id}")
        return {"completed": True, "result": result_data}

    return {"completed": False}

async def _handle_task_failure(task_id: str, error: Exception):
    """å¤„ç†ä»»åŠ¡å¤±è´¥ - æå–çš„å…¬å…±å‡½æ•°"""
    logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {task_id}, é”™è¯¯: {error}")
    redis_client.hset(f"task:{task_id}", mapping={
        "status": "failed",
        "error": str(error)
    })

    # å‘é€å¤±è´¥äº‹ä»¶åˆ°äº‹ä»¶æµ
    failure_event = {
        "type": "task_failed",
        "task_id": task_id,
        "status": "failed",
        "error": str(error),
        "timestamp": datetime.now().isoformat()
    }

    try:
        redis_client.xadd(
            f"events:{task_id}",
            {
                "timestamp": str(time.time()),
                "data": json.dumps(failure_event, default=str, ensure_ascii=False)
            }
        )
    except Exception as xe:
        logger.error(f"å‘é€å¤±è´¥äº‹ä»¶å¤±è´¥: {xe}")

    raise error


@celery_app.task(bind=True)
def resume_writing_task(self, user_id: str, session_id: str, task_id: str, user_response: str):
    """æ¢å¤å†™ä½œä»»åŠ¡ - å‚è€ƒ ReActAgentsTest çš„ç®€å•å®ç°"""
    
    async def resume_workflow():
        try:
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.abspath(__file__)))
            from graph.graph import create_writing_assistant_graph
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºè¿è¡Œä¸­
            redis_client.hset(f"task:{task_id}", "status", "running")     
            config = cast(RunnableConfig, {"configurable": {"thread_id": task_id}})
            # ä½¿ç”¨ä¸ execute_writing_task ç›¸åŒçš„ AsyncRedisSaver æ¨¡å¼
            from langgraph.types import Command
            interrupted = False
            # ä½¿ç”¨å®˜æ–¹æ¨èçš„ AsyncRedisSaver - ä¿®å¤ç¯å¢ƒå˜é‡é—®é¢˜
            from langgraph.checkpoint.redis.aio import AsyncRedisSaver
            import os
            
            # ç¡®ä¿ç¯å¢ƒå˜é‡è®¾ç½®
            os.environ["REDIS_URL"] = REDIS_URL
            
            async with AsyncRedisSaver.from_conn_string(REDIS_URL) as checkpointer:
                await checkpointer.asetup()

                # åˆ›å»ºå¹¶ç¼–è¯‘å›¾ - å¼ºåˆ¶é‡æ–°åˆ›å»º
                workflow = create_writing_assistant_graph()
                graph = workflow.compile(checkpointer=checkpointer)

                chunk_count = 0
                final_result = None
                
                # å…ˆæ£€æŸ¥å½“å‰å›¾çŠ¶æ€
                try:
                    current_state = await checkpointer.aget_tuple(config)
                    if current_state:
                        logger.info(f"æ¢å¤å‰çš„å›¾çŠ¶æ€: {current_state.metadata if hasattr(current_state, 'metadata') else 'unknown'}")
                        
                        # æ£€æŸ¥nextèŠ‚ç‚¹
                        if hasattr(current_state, 'next') and current_state.next:
                            logger.info(f"ğŸ¯ æ¢å¤å‰çš„nextèŠ‚ç‚¹: {current_state.next}")
                        else:
                            logger.warning("âš ï¸ æ¢å¤å‰æ²¡æœ‰nextèŠ‚ç‚¹ï¼Œå›¾å¯èƒ½å·²å®Œæˆæˆ–å‡ºé”™")
                            
                        if hasattr(current_state, 'checkpoint') and current_state.checkpoint:
                            state_data = current_state.checkpoint.get('channel_values', {})
                            logger.info(f"æ¢å¤å‰çš„çŠ¶æ€é”®: {list(state_data.keys())}")
                            
                            # æ‰“å°çŠ¶æ€å€¼æ¦‚è§ˆ
                            state_overview = {}
                            for key, value in state_data.items():
                                if value is not None:
                                    if isinstance(value, str):
                                        state_overview[key] = f"str({len(value)} chars)"
                                    elif isinstance(value, list):
                                        state_overview[key] = f"list({len(value)} items)"
                                    elif isinstance(value, dict):
                                        state_overview[key] = f"dict({len(value)} keys)"
                                    else:
                                        state_overview[key] = f"{type(value).__name__}"
                            logger.info(f"çŠ¶æ€å€¼æ¦‚è§ˆ: {state_overview}")
                    else:
                        logger.warning("æ¢å¤å‰æ— æ³•è·å–å›¾çŠ¶æ€")
                except Exception as state_error:
                    logger.error(f"æ£€æŸ¥æ¢å¤å‰çŠ¶æ€å¤±è´¥: {state_error}")

                async for chunk in graph.astream(Command(resume=user_response), config, stream_mode=["updates", "custom"]):
                    chunk_count += 1
                    logger.info(f"æ¢å¤ä»»åŠ¡æ”¶åˆ° chunk #{chunk_count}: {type(chunk)}")
                    
                    # å¤„ç†æµå¼è¾“å‡º
                    await _process_stream_chunk(chunk, task_id)
                    
                    # è®°å½• chunk å†…å®¹
                    if isinstance(chunk, tuple) and len(chunk) == 2:
                        stream_type, data = chunk
                        logger.info(f"  æµç±»å‹: {stream_type}")
                        if isinstance(data, dict):
                            logger.info(f"  æ•°æ®é”®: {list(data.keys())}")
                            if stream_type == "updates":
                                # è®°å½•èŠ‚ç‚¹æ‰§è¡Œ
                                node_names = [k for k in data.keys() if k != "__interrupt__"]
                                if node_names:
                                    logger.info(f"  æ‰§è¡ŒèŠ‚ç‚¹: {node_names}")
                                    
                                # ä¿å­˜æœ€åçš„ç»“æœ
                                for node_name in node_names:
                                    if node_name in data and isinstance(data[node_name], dict):
                                        final_result = (stream_type, data)

                    # æ£€æŸ¥ä¸­æ–­ - ä½¿ç”¨ç»Ÿä¸€çš„ä¸­æ–­å¤„ç†å‡½æ•°
                    is_interrupt = _check_for_interruption(chunk, task_id)
                    if is_interrupt:
                        interrupted = True
                        logger.info(f"æ£€æµ‹åˆ°æ–°çš„ä¸­æ–­ï¼Œchunk #{chunk_count}")
                        return {"interrupted": True, "task_id": task_id}

                logger.info(f"æ¢å¤ä»»åŠ¡å¤„ç†å®Œæˆï¼Œæ€»å…±å¤„ç†äº† {chunk_count} ä¸ªchunks")
                
                # å¦‚æœæ²¡æœ‰å¤„ç†ä»»ä½•chunksï¼Œè¯´æ˜å¯èƒ½å·²ç»å®Œæˆæˆ–å‡ºé”™
                if chunk_count == 0:
                    logger.warning("âš ï¸ æ²¡æœ‰å¤„ç†ä»»ä½•chunksï¼Œå¯èƒ½å›¾å·²ç»å®Œæˆæˆ–å‘ç”Ÿé”™è¯¯")
                    # å°è¯•è·å–å½“å‰å®Œæ•´çŠ¶æ€
                    try:
                        current_state = await checkpointer.aget_tuple(config)
                        if current_state and hasattr(current_state, 'checkpoint') and current_state.checkpoint:
                            state_data = current_state.checkpoint.get('channel_values', {})
                            logger.info(f"å®ŒæˆåçŠ¶æ€é”®è¯¦æƒ…: {[(k, type(v)) for k, v in state_data.items()]}")
                            
                            # æ£€æŸ¥æ˜¯å¦çœŸçš„å®Œæˆäº†
                            if state_data.get('article'):
                                logger.info("âœ… å‘ç°æ–‡ç« å†…å®¹ï¼Œä»»åŠ¡ç¡®å®å·²å®Œæˆ")
                                final_result = ('completed', state_data)
                            elif state_data.get('outline'):
                                logger.warning("âš ï¸ åªæœ‰å¤§çº²ï¼Œå¯èƒ½ä»»åŠ¡æœªå®Œå…¨å®Œæˆ")
                            else:
                                logger.error("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆå†…å®¹")
                    except Exception as e:
                        logger.error(f"è·å–å®ŒæˆçŠ¶æ€å¤±è´¥: {e}")

            # å¤„ç†å®Œæˆç»“æœ - æ”¹è¿›ç‰ˆ
            logger.info(f"ğŸ” æ¢å¤ä»»åŠ¡ç»“æŸæ£€æŸ¥: interrupted={interrupted}, final_result={bool(final_result)}")

            if not interrupted:
                logger.info("âœ… ä»»åŠ¡æœªä¸­æ–­ï¼Œå¼€å§‹å¤„ç†å®Œæˆç»“æœ")
                result_data = {}

                # æ–¹å¼1ï¼šä»final_resultè·å–ç»“æœ
                if final_result:
                    logger.info("ä»final_resultæå–æ•°æ®...")
                    try:
                        if isinstance(final_result, tuple) and len(final_result) == 2:
                            stream_type, data = final_result
                            if stream_type == 'completed' and isinstance(data, dict):
                                # ç›´æ¥ä½¿ç”¨å®Œæˆçš„çŠ¶æ€æ•°æ®
                                result_data = {
                                    "outline": data.get("outline"),
                                    "article": data.get("article"),
                                    "search_results": data.get("search_results", []),
                                    "topic": data.get("topic"),
                                    "enhancement_suggestions": data.get("enhancement_suggestions", [])
                                }
                                logger.info(f"ä»completedçŠ¶æ€æå–ç»“æœé”®: {[k for k, v in result_data.items() if v]}")
                            elif stream_type == 'updates' and isinstance(data, dict):
                                # ä»æ›´æ–°ä¸­æå–ç»“æœ
                                for node_name, node_data in data.items():
                                    if isinstance(node_data, dict):
                                        result_data.update({
                                            "outline": node_data.get("outline") or result_data.get("outline"),
                                            "article": node_data.get("article") or result_data.get("article"),
                                            "search_results": node_data.get("search_results", result_data.get("search_results", [])),
                                            "topic": node_data.get("topic") or result_data.get("topic"),
                                            "enhancement_suggestions": node_data.get("enhancement_suggestions", result_data.get("enhancement_suggestions", []))
                                        })
                                logger.info(f"ä»updatesæå–ç»“æœé”®: {[k for k, v in result_data.items() if v]}")
                    except Exception as final_error:
                        logger.error(f"ä»final_resultæå–å¤±è´¥: {final_error}")

                # æ–¹å¼2ï¼šä»checkpointerè·å–çŠ¶æ€
                if not any(result_data.values()):
                    logger.info("ä»final_resultæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•checkpoint...")
                    try:
                        current_state = await checkpointer.aget_tuple(config)
                        if current_state and hasattr(current_state, 'checkpoint') and current_state.checkpoint:
                            state_data = current_state.checkpoint.get('channel_values', {})
                            logger.info(f"ğŸ“Š ä» checkpoint è·å–çŠ¶æ€é”®: {list(state_data.keys())}")

                            # æå–ç»“æœæ•°æ®
                            result_data = {
                                "outline": state_data.get("outline"),
                                "article": state_data.get("article"),
                                "search_results": state_data.get("search_results", []),
                                "topic": state_data.get("topic"),
                                "enhancement_suggestions": state_data.get("enhancement_suggestions", [])
                            }
                            logger.info(f"ä»checkpointæå–çš„ç»“æœé”®: {[k for k, v in result_data.items() if v]}")
                    except Exception as checkpoint_error:
                        logger.error(f"ä»checkpointè·å–å¤±è´¥: {checkpoint_error}")

                # æ–¹å¼3ï¼šä»Redisè·å–å†å²ç»“æœ
                if not any(result_data.values()):
                    logger.info("ä»checkpointæœªè·å–åˆ°æ•°æ®ï¼Œå°è¯•Redis...")
                    try:
                        task_result = redis_client.hget(f"task:{task_id}", "result")
                        if task_result:
                            existing_result = json.loads(task_result)
                            if existing_result and any(existing_result.values()):
                                result_data = existing_result
                                logger.info(f"ä»Redisè·å–çš„ç»“æœé”®: {[k for k, v in result_data.items() if v]}")
                    except Exception as redis_e:
                        logger.error(f"ä»Redisè·å–ç»“æœå¤±è´¥: {redis_e}")

                # æ£€æŸ¥ç»“æœçŠ¶æ€
                if result_data.get("article"):
                    logger.info("âœ… æ‰¾åˆ°ç”Ÿæˆçš„æ–‡ç« ï¼Œä»»åŠ¡ç¡®å®å®Œæˆ")
                elif result_data.get("outline"):
                    logger.info("âœ… æ‰¾åˆ°å¤§çº²ï¼Œä½†æ²¡æœ‰æ–‡ç«  - ä»»åŠ¡å¯èƒ½æœªå®Œå…¨å®Œæˆ")
                else:
                    logger.warning("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å†…å®¹ï¼Œä»»åŠ¡å¯èƒ½å¤±è´¥æˆ–æœªå®Œæˆ")
                    # åˆ›å»ºé»˜è®¤ç»“æœ
                    if not result_data:
                        result_data = {
                            "outline": None,
                            "article": None,
                            "search_results": [],
                            "topic": None,
                            "enhancement_suggestions": []
                        }

                redis_client.hset(f"task:{task_id}", mapping={
                    "status": "completed",
                    "result": json.dumps(result_data, default=str, ensure_ascii=False)
                })

                logger.info(f"ğŸ“‹ ä»»åŠ¡å®Œæˆï¼Œç»“æœæ•°æ®é”®: {list(result_data.keys())}")
                if result_data.get("article"):
                    article_length = len(result_data["article"]) if isinstance(result_data["article"], str) else 0
                    logger.info(f"âœ… æ–‡ç« ç”ŸæˆæˆåŠŸï¼Œé•¿åº¦: {article_length} å­—ç¬¦")
                else:
                    logger.warning("âš ï¸ ä»»åŠ¡å®Œæˆä½†æ²¡æœ‰ç”Ÿæˆæ–‡ç« ")

                logger.info(f"ğŸ¯ è¿”å› completed=Trueï¼Œresult é”®: {list(result_data.keys())}")
                return {"completed": True, "result": result_data}
            else:
                logger.info(f"ğŸ”„ ä»»åŠ¡è¢«ä¸­æ–­ï¼Œè¿”å› interrupted=True")

        except Exception as e:
            redis_client.hset(f"task:{task_id}", mapping={"status": "failed", "error": str(e)})
            raise
    
    return asyncio.run(resume_workflow())

# ============================================================================
# FastAPI åº”ç”¨
# ============================================================================

app = FastAPI(
    title="LangGraph-Celery-Redis-Stream",
    version="1.0.0",
    description="ç®€åŒ–ç‰ˆå®ç°ï¼Œä¿æŒæ ¸å¿ƒåŠŸèƒ½"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

@app.get("/")
async def root():
    return {"message": "LangGraph Celery Chat - ä¼˜åŒ–ç‰ˆ", "status": "running"}

@app.get("/health")
async def health():
    redis_status = "ok" if redis_client.ping() else "error"
    return {"status": "ok", "services": {"redis": redis_status, "celery": "ok"}}

# ============================================================================
# æ ¸å¿ƒ API
# ============================================================================

@app.post("/api/v1/tasks")
async def create_task(request: TaskRequest):
    """åˆ›å»ºä»»åŠ¡ - å‚è€ƒ ReActAgentsTest"""
    try:
        task_id = f"task_{uuid.uuid4().hex[:8]}"
        session_id = f"session_{request.user_id}_{int(time.time())}"
        
        # å­˜å‚¨ä»»åŠ¡ä¿¡æ¯
        task_data = {
            "task_id": task_id,
            "session_id": session_id,
            "user_id": request.user_id,
            "status": "pending",
            "created_at": datetime.now().isoformat(),
            "config": json.dumps(request.model_dump())
        }
        
        redis_client.hset(f"task:{task_id}", mapping=task_data)
        redis_client.expire(f"task:{task_id}", 3600)
        
        # å¯åŠ¨ Celery ä»»åŠ¡
        celery_task = execute_writing_task.delay(
            user_id=request.user_id,
            session_id=session_id,
            task_id=task_id,
            config_data=request.model_dump()
        )
        
        logger.info(f"åˆ›å»ºä»»åŠ¡: {task_id}")
        
        return {
            "task_id": task_id,
            "session_id": session_id,
            "status": "pending"
        }
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/tasks/{task_id}")
async def get_task_status(task_id: str):
    """è·å–ä»»åŠ¡çŠ¶æ€"""
    try:
        task_data = redis_client.hgetall(f"task:{task_id}")
        if not task_data:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        # è§£æ JSON å­—æ®µ
        if "config" in task_data:
            task_data["config"] = json.loads(task_data["config"])
        if "result" in task_data:
            task_data["result"] = json.loads(task_data["result"])
            
        return task_data
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/api/v1/tasks/{task_id}/resume")
async def resume_task(task_id: str, request: ResumeRequest):
    """æ¢å¤ä»»åŠ¡"""
    try:
        task_data = redis_client.hgetall(f"task:{task_id}")
        if not task_data:
            raise HTTPException(status_code=404, detail="ä»»åŠ¡ä¸å­˜åœ¨")
        
        status = task_data.get("status")
        if status not in ["paused"]:
            raise HTTPException(status_code=400, detail=f"ä»»åŠ¡çŠ¶æ€ {status} ä¸æ”¯æŒæ¢å¤")
        
        # å¯åŠ¨æ¢å¤ä»»åŠ¡
        celery_task = resume_writing_task.delay(
            user_id=task_data.get("user_id"),
            session_id=task_data.get("session_id"),
            task_id=task_id,
            user_response=request.response
        )
        
        return {"message": "ä»»åŠ¡å·²æ¢å¤", "task_id": task_id}
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/v1/events/{task_id}")
async def get_event_stream(task_id: str):
    """äº‹ä»¶æµ - çœŸæ­£çš„å¼‚æ­¥ç‰ˆæœ¬ (aioredis)"""

    async def event_generator():
        stream_name = f"events:{task_id}"
        last_id = "0"

        # è·å–å¼‚æ­¥Rediså®¢æˆ·ç«¯
        async_redis = await get_async_redis()

        # ç«‹å³å‘é€è¿æ¥ç¡®è®¤
        yield f"data: {json.dumps({'type': 'connected', 'task_id': task_id})}\n\n"

        try:
            # å¼‚æ­¥æ£€æŸ¥Redisè¿æ¥
            await async_redis.ping()
            yield f"data: {json.dumps({'type': 'debug', 'message': 'Redisè¿æ¥æ­£å¸¸'})}\n\n"
            # å¼‚æ­¥æ£€æŸ¥æµæ˜¯å¦å­˜åœ¨
            exists = await async_redis.exists(stream_name)
            yield f"data: {json.dumps({'type': 'debug', 'message': f'æµå­˜åœ¨: {exists}'})}\n\n"
            if exists:
                # å¼‚æ­¥è·å–æµé•¿åº¦
                length = await async_redis.xlen(stream_name)
                yield f"data: {json.dumps({'type': 'debug', 'message': f'æµé•¿åº¦: {length}'})}\n\n"

                # å¼‚æ­¥è¯»å–æ‰€æœ‰ç°æœ‰æ¶ˆæ¯
                all_messages = await async_redis.xrange(stream_name)
                for message_id, fields in all_messages:
                    try:
                        event_data = {
                            "id": message_id,
                            "timestamp": fields.get("timestamp"),
                            "data": json.loads(fields.get("data", "{}"))
                        }
                        yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                        last_id = message_id
                    except Exception as e:
                        yield f"data: {json.dumps({'type': 'error', 'message': f'è§£ææ¶ˆæ¯å¤±è´¥: {e}'})}\n\n"

            # å¼‚æ­¥ç›‘å¬æ–°æ¶ˆæ¯
            timeout_count = 0
            while timeout_count < 120:  # å¢åŠ åˆ°120æ¬¡ (2åˆ†é’Ÿ)
                # çœŸæ­£çš„å¼‚æ­¥xread - ä¸ä¼šé˜»å¡äº‹ä»¶å¾ªç¯ï¼
                try:
                    events = await async_redis.xread({stream_name: last_id}, count=10, block=1000)

                    if events:
                        timeout_count = 0  # é‡ç½®è¶…æ—¶è®¡æ•°
                        for stream, messages in events:
                            for message_id, fields in messages:
                                try:
                                    event_data = {
                                        "id": message_id,
                                        "timestamp": fields.get("timestamp"),
                                        "data": json.loads(fields.get("data", "{}"))
                                    }
                                    yield f"data: {json.dumps(event_data, ensure_ascii=False)}\n\n"
                                    last_id = message_id
                                except Exception as e:
                                    yield f"data: {json.dumps({'type': 'error', 'message': f'è§£ææ–°æ¶ˆæ¯å¤±è´¥: {e}'})}\n\n"
                    else:
                        timeout_count += 1
                        yield f"data: {json.dumps({'type': 'heartbeat', 'count': timeout_count})}\n\n"

                except asyncio.TimeoutError:
                    timeout_count += 1
                    yield f"data: {json.dumps({'type': 'heartbeat', 'count': timeout_count})}\n\n"

        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*"
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=False)