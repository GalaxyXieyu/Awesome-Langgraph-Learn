# 快速开始：多 LLM 流程提示词调试

## 问题
在你的流程中，`generate_report_node` 的输入来自上游 LLM 的输出。想在 Playground 调试时切换不同提示词，但每次都要重新输入参数。

## 解决方案（3分钟上手）

### 第一步：保存中间结果（已完成✅）

```bash
python tools/middle_result_dataset.py
```

**结果**：已创建 3 个测试场景，保存在 `.middle_results_cache/`

查看场景：
```bash
ls .middle_results_cache/
# ai_formal_deep.json
# fintech_concise_shallow.json
# new_energy_detailed_medium.json
```

### 第二步：创建提示词的多个版本

```bash
# 复制现有提示词
cp prompts/report_generator.yaml prompts/report_generator_v2.yaml

# 修改 v2 版本（比如改进 system message）
```

### 第三步：批量对比测试

创建测试脚本 `test_prompts.py`：

```python
from tools.prompt_comparison_test import PromptComparisonTest

tester = PromptComparisonTest()

# 用相同的输入测试两个版本
results = tester.compare_prompts(
    prompt_files=[
        "report_generator.yaml",      # 原始版本
        "report_generator_v2.yaml"    # 修改后的版本
    ],
    scenario_name="ai_formal_deep",   # 使用保存的场景
    save_results=True
)
```

运行：
```bash
python test_prompts.py
```

### 第四步：查看对比结果

```bash
# 查看统计摘要
cat comparison_results/comparison_*.json

# 对比完整输出
# Windows PowerShell:
diff (cat comparison_results/ai_formal_deep_report_generator_*.md)

# 或者用文本编辑器打开对比
code comparison_results/
```

---

## 方案对比

| 方案 | 操作方式 | 优势 | 适用场景 |
|-----|---------|------|---------|
| **方案1：本地批量测试** | 运行 Python 脚本 | • 零手动输入<br>• 批量对比<br>• 速度快 | **日常开发（推荐）** |
| 方案2：LangSmith Dataset | Playground 选择 Dataset | • 可视化<br>• 团队共享 | 团队协作 |
| 方案3：从 Trace 导出 | 从 Trace 打开 | • 快速启动<br>⚠️ 切换提示词会丢失输入 | 临时调试 |

---

## 实际使用示例

### 场景：优化报告的语言风格

```bash
# 1. 创建新版本
cp prompts/report_generator.yaml prompts/report_generator_casual.yaml

# 2. 修改 system message，让语言更通俗易懂
# 编辑 report_generator_casual.yaml

# 3. 批量测试 3 个场景
python -c "
from tools.prompt_comparison_test import PromptComparisonTest

tester = PromptComparisonTest()

for scenario in ['ai_formal_deep', 'fintech_concise_shallow', 'new_energy_detailed_medium']:
    print(f'\n测试场景: {scenario}')
    tester.compare_prompts(
        ['report_generator.yaml', 'report_generator_casual.yaml'],
        scenario,
        save_results=True
    )
"

# 4. 查看结果
ls comparison_results/
```

---

## 高级功能

### 1. 从实际运行中保存中间结果

修改 `graph/nodes.py`，在 `web_search_node` 末尾添加：

```python
# 调试模式：保存中间结果
if os.getenv("SAVE_MIDDLE_RESULT") == "true":
    from tools.middle_result_dataset import MiddleResultDataset
    debug = MiddleResultDataset()
    debug.save_middle_result_manually(
        topic=state.get("topic"),
        year_range=state.get("year_range"),
        style=state.get("style"),
        depth=state.get("depth"),
        focus_areas=state.get("focus_areas"),
        search_results=state.get("search_results_formatted"),
        scenario_name=state.get("topic", "default").replace(" ", "_")
    )
```

使用：
```bash
SAVE_MIDDLE_RESULT=true python main.py --query "区块链技术发展"
```

### 2. 创建 LangSmith Dataset（可选）

如果配置了 `LANGSMITH_API_KEY`：

```python
from tools.middle_result_dataset import MiddleResultDataset

manager = MiddleResultDataset()
manager.create_langsmith_dataset("report_generator_middle_results")
```

然后在 LangSmith Playground 中：
1. 选择 `report_generator` 提示词
2. 点击 "Select Dataset" → 选择 `report_generator_middle_results`
3. 切换提示词版本，输入会自动填充 ✅

---

## 工作流建议

### 日常开发迭代

```
1. 修改提示词
   ↓
2. 运行本地批量测试（3个场景，2分钟）
   ↓
3. 查看对比结果
   ↓
4. 继续迭代或推送到 Hub
```

### 版本管理

```
prompts/
  ├── report_generator.yaml          # 当前版本（生产）
  ├── report_generator_v2.yaml       # 实验版本1
  └── report_generator_casual.yaml   # 实验版本2（通俗风格）

# 测试后选择最优版本
mv report_generator_v2.yaml report_generator.yaml
```

---

## 常见问题

### Q1: 如何添加新的测试场景？

**方法1**：编辑 `tools/middle_result_dataset.py`，添加新的 `save_middle_result_manually` 调用

**方法2**：从实际运行中保存（见高级功能）

### Q2: 对比结果保存在哪里？

- **统计摘要**：`comparison_results/comparison_*.json`
- **完整输出**：`comparison_results/<scenario>_<prompt>_*.md`

### Q3: 如何只测试特定场景？

```python
# 只测试一个场景
tester.compare_prompts(
    ['report_generator.yaml', 'report_generator_v2.yaml'],
    scenario_name="ai_formal_deep"  # 指定场景
)
```

---

## 文件清单

- ✅ `tools/middle_result_dataset.py` - 中间结果管理工具
- ✅ `tools/prompt_comparison_test.py` - 提示词对比测试工具
- ✅ `.middle_results_cache/` - 中间结果缓存（已创建3个场景）
- ✅ `comparison_results/` - 对比测试结果（运行测试后生成）
- ✅ `docs/多LLM流程调试方案.md` - 完整方案文档

---

## 总结

**核心价值**：
- 🎯 **固化输入**：中间结果保存为场景，无需重复输入
- ⚡ **快速对比**：一次运行测试多个提示词版本
- 📊 **结果追溯**：自动保存每次测试的完整输出
- 🔄 **快速迭代**：修改 → 测试 → 对比，循环往复

**下一步**：
1. ✅ 已创建 3 个测试场景
2. 创建 `report_generator_v2.yaml`（修改提示词）
3. 运行 `compare_prompts` 对比效果
4. 选择最优版本，继续迭代

有问题查看：`docs/多LLM流程调试方案.md`

