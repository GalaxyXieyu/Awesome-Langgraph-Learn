"""
æ‰¹é‡ä¸Šä¼ æ‰€æœ‰æœ¬åœ°è¯„ä¼°å™¨åˆ° LangSmith å¹³å°
"""
import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from evaluation.evaluator_manager import EvaluatorManager


def upload_all_evaluators():
    """ä¸Šä¼ æ‰€æœ‰é…ç½®ä¸­çš„è¯„ä¼°å™¨åˆ° LangSmith å¹³å°"""
    print("=" * 60)
    print("æ‰¹é‡ä¸Šä¼ æœ¬åœ°è¯„ä¼°å™¨åˆ° LangSmith å¹³å°")
    print("=" * 60)
    
    manager = EvaluatorManager()
    
    # è·å–æ‰€æœ‰è¯„ä¼°å™¨
    evaluators = manager.list_evaluators()
    
    if not evaluators:
        print("\n[WARN] æœªæ‰¾åˆ°ä»»ä½•è¯„ä¼°å™¨é…ç½®")
        return
    
    print(f"\næ‰¾åˆ° {len(evaluators)} ä¸ªè¯„ä¼°å™¨:")
    for name in evaluators.keys():
        print(f"  - {name}")
    
    print("\nå¼€å§‹ä¸Šä¼ ...\n")
    print("-" * 60)
    
    results = {
        'success': [],
        'failed': [],
        'skipped': []
    }
    
    for evaluator_name, description in evaluators.items():
        print(f"\n[{evaluator_name}]")
        print("-" * 60)
        
        try:
            # è·å–é…ç½®
            config = manager.config.get('evaluators', {}).get(evaluator_name, {})
            dataset_name = config.get('dataset', 'default')
            
            # å°è¯•æ¨é€
            success = manager.push(
                evaluator_name=evaluator_name,
                dataset_name=dataset_name,
                description=description
            )
            
            if success:
                results['success'].append(evaluator_name)
                print(f"âœ… {evaluator_name} ä¸Šä¼ æˆåŠŸ")
            else:
                results['failed'].append(evaluator_name)
                print(f"âš ï¸  {evaluator_name} ä¸Šä¼ å¤±è´¥ï¼ˆå¯èƒ½éœ€è¦æ‰‹åŠ¨ä¸Šä¼ ï¼‰")
                
        except Exception as e:
            results['failed'].append(evaluator_name)
            print(f"âŒ {evaluator_name} ä¸Šä¼ å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    # æ±‡æ€»ç»“æœ
    print("\n" + "=" * 60)
    print("ä¸Šä¼ ç»“æœæ±‡æ€»")
    print("=" * 60)
    print(f"\nâœ… æˆåŠŸ: {len(results['success'])} ä¸ª")
    for name in results['success']:
        print(f"   - {name}")
    
    print(f"\nâš ï¸  å¤±è´¥: {len(results['failed'])} ä¸ª")
    for name in results['failed']:
        print(f"   - {name}")
        print(f"     ğŸ’¡ å¯ä»¥æŸ¥çœ‹: evaluation/{name}_source.py")
    
    if results['failed']:
        print(f"\nğŸ’¡ å¯¹äºä¸Šä¼ å¤±è´¥çš„è¯„ä¼°å™¨ï¼š")
        print(f"  1. æŸ¥çœ‹æå–çš„æºä»£ç æ–‡ä»¶: evaluation/{{evaluator_name}}_source.py")
        print(f"  2. è®¿é—® LangSmith Web UI:")
        for name in results['failed']:
            config = manager.config.get('evaluators', {}).get(name, {})
            dataset = config.get('dataset', 'default')
            print(f"     - {name}: https://smith.langchain.com/datasets/{dataset}/evaluators")
        print(f"  3. ç‚¹å‡» 'Create Custom Code Evaluator'")
        print(f"  4. ç²˜è´´å¯¹åº”è¯„ä¼°å™¨çš„æºä»£ç ")
    
    print("\n" + "=" * 60)


if __name__ == "__main__":
    upload_all_evaluators()

