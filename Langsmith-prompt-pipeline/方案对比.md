# 多 LLM 流程调试方案对比

## 两种方案

### 方案1：手动保存中间结果（原方案）
**文件**：`tools/middle_result_dataset.py`  
**文档**：`快速开始-提示词调试.md`

**特点**：
- 手动编写代码保存中间结果
- 需要为每个 Graph 适配

### 方案2：自动提取中间结果（推荐 ⭐）
**文件**：`tools/auto_extract_from_trace.py`  
**文档**：`优雅方案-自动提取.md`

**特点**：
- 从 LangSmith Trace 自动提取
- 通用，适用任何 Graph

---

## 详细对比

| 维度 | 方案1（手动） | 方案2（自动） |
|-----|------------|------------|
| **代码侵入** | ❌ 需要修改 nodes.py | ✅ 零侵入 |
| **适配成本** | ❌ 每个 Graph 都要适配 | ✅ 通用，无需适配 |
| **提取方式** | ❌ 手动编写保存代码 | ✅ 自动从 Trace 提取 |
| **新增场景** | ❌ 修改代码重新保存 | ✅ 重新运行即可 |
| **节点覆盖** | ❌ 手动指定节点 | ✅ 自动提取所有 LLM 节点 |
| **离线使用** | ✅ 支持 | ⚠️ 需要联网 |
| **团队协作** | ⚠️ 需要共享文件 | ✅ 自动共享到 LangSmith |
| **上手难度** | ⚠️ 需要理解代码结构 | ✅ 开箱即用 |

---

## 使用场景推荐

### 推荐方案2的场景（90%的情况）

✅ **多个 LangGraph 项目**
- 不需要为每个项目写适配代码
- 一个工具通用所有项目

✅ **团队协作开发**
- Dataset 自动共享到 LangSmith
- 团队成员直接使用

✅ **快速原型开发**
- 零配置，运行即用
- 专注业务逻辑

✅ **标准 LangGraph 项目**
- 已配置 LangSmith 追踪
- 使用标准的 LLM 调用

### 推荐方案1的场景（特殊情况）

✅ **完全离线开发**
- 没有网络连接
- 不能使用 LangSmith

✅ **高度定制的数据格式**
- 需要特殊的数据预处理
- 需要完全控制数据结构

✅ **学习和理解流程**
- 想深入理解数据流
- 需要完全透明的过程

---

## 使用流程对比

### 方案1：手动保存

```bash
# 1. 编写保存代码（一次性）
# 修改 tools/middle_result_dataset.py
# 添加你的场景

# 2. 运行保存脚本
python tools/middle_result_dataset.py

# 3. 使用保存的数据
python tools/prompt_comparison_test.py
```

**痛点**：
- 每个新 Graph 都要重写保存逻辑
- 新增场景需要修改代码

### 方案2：自动提取（推荐）

```bash
# 1. 运行你的 Graph（日常工作）
python main.py --query "测试查询"

# 2. 自动提取（2分钟）
python tools/auto_extract_from_trace.py
# → 交互式选择运行记录
# → 自动创建 Dataset

# 3. Playground 使用
# → 访问 LangSmith
# → 选择 Dataset
# → 输入自动填充 ✅
```

**优势**：
- 零代码修改
- 适用所有 Graph
- 更新场景只需重新运行

---

## 实际案例

### 案例1：单个项目快速开发

**场景**：开发一个报告生成项目，需要调试提示词

**方案2（推荐）**：
1. 正常开发，运行几次测试
2. `python tools/auto_extract_from_trace.py`
3. 在 Playground 中调试

**时间**：2分钟  
**维护成本**：0

### 案例2：多个项目的团队

**场景**：团队有 5 个不同的 LangGraph 项目

**方案1（手动）**：
- 每个项目写一套保存逻辑
- 5 × 30分钟 = 150分钟
- 后续维护：每个项目单独维护

**方案2（自动）**：
- 一个工具适用所有项目
- 5 × 2分钟 = 10分钟
- 后续维护：0（通用工具）

**节省时间**：140分钟 + 后续维护成本

### 案例3：新增测试场景

**场景**：需要添加 10 个新的测试场景

**方案1（手动）**：
```python
# 需要修改代码，添加 10 次 save_middle_result_manually 调用
manager.save_middle_result_manually(
    topic="场景1",
    # ... 很多参数
)
# 重复 10 次
```
**时间**：30分钟

**方案2（自动）**：
```bash
# 运行 10 次你的 Graph
for query in scenarios:
    python main.py --query "$query"

# 一次性提取
python tools/auto_extract_from_trace.py
# → 选择这 10 次运行
```
**时间**：5分钟

---

## 迁移指南

### 从方案1迁移到方案2

如果你已经使用了方案1，想切换到方案2：

```bash
# 1. 确保配置了 LangSmith
export LANGSMITH_API_KEY='your-key'

# 2. 重新运行几个典型场景
python main.py --query "场景1"
python main.py --query "场景2"
python main.py --query "场景3"

# 3. 使用新工具提取
python tools/auto_extract_from_trace.py
# → 创建 Dataset

# 4. 在 Playground 中使用
# → 选择新创建的 Dataset

# 5. 删除旧的手动代码（可选）
# 保留 .middle_results_cache/ 作为备份
```

### 两种方案共存

两种方案可以共存，互不冲突：

- **方案1**：本地快速测试（离线场景）
- **方案2**：团队协作、Playground 使用（在线场景）

---

## 常见问题

### Q1: 方案2需要什么前置条件？

**A**: 只需要：
1. 配置 `LANGSMITH_API_KEY` 环境变量
2. 你的项目正常运行（会自动记录到 LangSmith）

### Q2: 方案2适用于所有 LangGraph 项目吗？

**A**: 是的！只要项目中有 LLM 调用（使用 LangChain 的 `ChatOpenAI`、`AzureChatOpenAI` 等），就会被自动检测和提取。

### Q3: 方案2会影响项目性能吗？

**A**: 不会！提取过程是事后分析，不影响运行时性能。

### Q4: 我应该使用哪个方案？

**A**: 
- **默认推荐**：方案2（自动提取）
- **特殊情况**：离线开发用方案1

### Q5: 两个方案的数据格式兼容吗？

**A**: 数据格式略有不同：
- 方案1：自定义格式，保存在 `.middle_results_cache/`
- 方案2：LangSmith 标准格式，保存在 `.trace_cache/` + LangSmith Dataset

但都可以在 Playground 中使用（通过 Dataset）。

---

## 总结

### 核心建议

**优先使用方案2（自动提取）**

理由：
1. ✅ **零代码侵入**：不需要修改现有项目
2. ✅ **通用性强**：适用任何 LangGraph 项目
3. ✅ **维护成本低**：一次配置，永久使用
4. ✅ **团队友好**：自动共享到 LangSmith

**特殊情况使用方案1**：
- 完全离线开发
- 需要高度定制的数据格式

### 快速决策树

```
需要离线使用？
├─ 是 → 方案1（手动）
└─ 否 → 有多个 Graph 项目？
         ├─ 是 → 方案2（自动）⭐
         └─ 否 → 方案2（自动）⭐
```

结论：**绝大多数情况推荐方案2**

---

## 文件导航

### 方案1（手动）
- 工具：`tools/middle_result_dataset.py`
- 测试：`tools/prompt_comparison_test.py`
- 文档：`快速开始-提示词调试.md`

### 方案2（自动）⭐
- 工具：`tools/auto_extract_from_trace.py`
- 文档：`优雅方案-自动提取.md`

### 其他
- 完整方案：`docs/多LLM流程调试方案.md`
- 本文档：`方案对比.md`

