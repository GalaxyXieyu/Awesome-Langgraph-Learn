# LangSmith Prompt å·¥ç¨‹åŒ–å¹³å°

> ğŸš€ ä¼ä¸šçº§æç¤ºè¯å¼€å‘ã€æµ‹è¯•ã€ä¼˜åŒ–çš„å®Œæ•´è§£å†³æ–¹æ¡ˆ

[![LangChain](https://img.shields.io/badge/LangChain-Latest-blue)](https://python.langchain.com/)
[![LangSmith](https://img.shields.io/badge/LangSmith-Integrated-green)](https://smith.langchain.com/)
[![LangGraph](https://img.shields.io/badge/LangGraph-Workflow-orange)](https://langchain-ai.github.io/langgraph/)

---

## ğŸ“– ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒ SOPï¼šæç¤ºè¯å¼€å‘å·¥ä½œæµ](#-æ ¸å¿ƒ-sopæç¤ºè¯å¼€å‘å·¥ä½œæµ)
- [ç³»ç»Ÿæ¶æ„](#-ç³»ç»Ÿæ¶æ„)
- [æ ¸å¿ƒåŠŸèƒ½](#-æ ¸å¿ƒåŠŸèƒ½)
- [ä½¿ç”¨æŒ‡å—](#-ä½¿ç”¨æŒ‡å—)
- [é«˜çº§ç‰¹æ€§](#-é«˜çº§ç‰¹æ€§)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. ç¯å¢ƒé…ç½®

```bash
# å…‹éš†é¡¹ç›®
git clone <your-repo>
cd Langsmith-prompt-pipeline

# å®‰è£…ä¾èµ–
pip install -r requirements.txt

# é…ç½®ç¯å¢ƒå˜é‡
cp .env.example .env
# ç¼–è¾‘ .env å¡«å…¥ä½ çš„ API Keys
```

**å¿…éœ€çš„ç¯å¢ƒå˜é‡**ï¼š
```bash
# LangSmithï¼ˆå¿…éœ€ï¼‰
LANGSMITH_API_KEY="lsv2_pt_xxxxxxxxxxxx"
LANGSMITH_PROJECT="prompt-pipeline"

# LLM é…ç½®ï¼ˆæ”¯æŒ Azure OpenAI æˆ– OpenAIï¼‰
LLM_PROVIDER="azure"  # æˆ– "openai"

# Azure OpenAI
AZURE_ENDPOINT="https://your-resource.openai.azure.com/"
AZURE_DEPLOYMENT="gpt-4"
AZURE_API_KEY="your-key"
AZURE_API_VERSION="2024-02-15-preview"

# æˆ–æ ‡å‡† OpenAI
OPENAI_API_KEY="sk-xxxxx"
OPENAI_MODEL="gpt-4o"
```

### 2. éªŒè¯å®‰è£…

```bash
# æµ‹è¯• LLM è¿æ¥
python config/azure_config.py

# æµ‹è¯• LangSmith è¿æ¥
python config/langsmith_config.py

# æµ‹è¯•æç¤ºè¯ç®¡ç†å™¨
python prompts/prompt_manager.py
```

### 3. è¿è¡Œç¬¬ä¸€ä¸ªç¤ºä¾‹

```bash
# ç”ŸæˆæŠ¥å‘Šï¼ˆè‡ªåŠ¨è¿½è¸ªåˆ° LangSmithï¼‰
python main.py --query "åˆ†æ2024å¹´AIè¡Œä¸šå‘å±•è¶‹åŠ¿"
```

è®¿é—® [LangSmith æ§åˆ¶å°](https://smith.langchain.com/) æŸ¥çœ‹è¿è¡Œè¿½è¸ª ğŸ‰

---

## ğŸ¯ æ ¸å¿ƒ SOPï¼šæç¤ºè¯å¼€å‘å·¥ä½œæµ

### æ ‡å‡†æ“ä½œæµç¨‹ï¼ˆ5æ­¥å¾ªç¯ï¼‰

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. å¼€å‘   2. æµ‹è¯•   3. è¯„ä¼°   4. ä¼˜åŒ–   5. éƒ¨ç½²           â”‚
â”‚  â†“         â†“         â†“         â†“         â†“                  â”‚
â”‚  YAML  â†’  Dataset â†’  LangSmithâ†’ è°ƒä¼˜  â†’  Hub æ¨é€           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: å¼€å‘æç¤ºè¯

**åˆ›å»ºæˆ–ç¼–è¾‘ YAML æ–‡ä»¶**ï¼ˆæ ‡å‡†åŒ–æ ¼å¼ï¼‰ï¼š

```yaml
# prompts/parameter_parser.yaml
version: v1.0.0
description: "ä»ç”¨æˆ·è¾“å…¥ä¸­æå–ç»“æ„åŒ–å‚æ•°"

messages:
  - role: system
    content: |
      ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å‚æ•°æå–åŠ©æ‰‹ã€‚
      ä»ç”¨æˆ·æŸ¥è¯¢ä¸­æå–ä»¥ä¸‹å‚æ•°ï¼š
      - topic: ä¸»é¢˜
      - year_range: å¹´ä»½èŒƒå›´
      - style: æŠ¥å‘Šé£æ ¼
      - depth: æ·±åº¦çº§åˆ«
      
      ä»¥ JSON æ ¼å¼è¾“å‡ºã€‚

  - role: user
    content: "{user_query}"

input_variables:
  - user_query

metadata:
  author: "Your Name"
  tags: ["parameter_extraction", "json_output"]
```

**é…ç½®æç¤ºè¯ä¿¡æ¯**ï¼ˆ`prompts/prompts_config.yaml`ï¼‰ï¼š

```yaml
prompts:
  parameter_parser:
    file: "parameter_parser.yaml"
    hub_name: "parameter_parser"
    test_dataset: "parameter_parser"  # å…³è”çš„æµ‹è¯•æ•°æ®é›†
    
    # ä¸“å±è¯„ä¼°å™¨
    evaluators:
      - "structure_evaluator"
      - "parameter_extraction_evaluator"
    
    # è¯„ä¼°å™¨æƒé‡
    evaluator_weights:
      structure_evaluator: 0.3
      parameter_extraction_evaluator: 0.7
```

### Step 2: åˆ›å»ºæµ‹è¯•æ•°æ®é›†

**æ–¹å¼ 1ï¼šæ‰‹åŠ¨åˆ›å»º**

```python
from evaluation.datasets import DatasetManager

manager = DatasetManager()

test_cases = [
    {
        "input": {"user_query": "åˆ†æ2023-2024å¹´AIè¡Œä¸šå‘å±•è¶‹åŠ¿ï¼Œè¦è¯¦ç»†çš„"},
        "expected_params": {
            "topic": "AIè¡Œä¸šå‘å±•è¶‹åŠ¿",
            "year_range": "2023-2024",
            "depth": "è¯¦ç»†"
        }
    },
    # æ›´å¤šæµ‹è¯•ç”¨ä¾‹...
]

# åˆ›å»ºæ•°æ®é›†
manager.create_dataset(
    dataset_name="parameter_parser",
    test_cases=test_cases
)
```

**æ–¹å¼ 2ï¼šè‡ªåŠ¨æ•è·**ï¼ˆæ¨èï¼‰

```python
# åœ¨ä»£ç ä¸­ä½¿ç”¨ @capture_middle_result è£…é¥°å™¨
from tools.capture import capture_middle_result

@capture_middle_result(
    dataset_name="parameter_parser",
    step_name="parse_params"
)
def parse_parameters(user_query: str):
    # ä½ çš„é€»è¾‘
    return result
```

è¿è¡Œç¨‹åºåï¼Œä¸­é—´ç»“æœè‡ªåŠ¨ä¿å­˜åˆ° LangSmith Datasetï¼

### Step 3: è¿è¡Œè¯„ä¼°

**æ–¹å¼ 1ï¼šä½¿ç”¨ PromptManagerï¼ˆæ¨èï¼‰**

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# è¯„ä¼°æç¤ºè¯è´¨é‡
result = manager.evaluate_prompt('parameter_parser')

print(f"è´¨é‡è¯„åˆ†: {result['quality_score']:.2%}")
print(f"æµ‹è¯•ç”¨ä¾‹æ•°: {result['total']}")
```

**æ–¹å¼ 2ï¼šä½¿ç”¨ EvaluationRunnerï¼ˆé«˜çº§ï¼‰**

```python
from evaluation.evaluation import EvaluationRunner

runner = EvaluationRunner()

# è¯„ä¼°å•ä¸ªæç¤ºè¯
result = runner.evaluate_prompt(
    prompt_name='parameter_parser',
    dataset_name='parameter_parser',
    experiment_name='v1_test'
)

# å¯¹æ¯”å¤šä¸ªé…ç½®
comparison = runner.compare_prompts(
    prompt_name='parameter_parser',
    dataset_name='parameter_parser',
    llm_configs={
        "gpt4": {"temperature": 0.3},
        "gpt35": {"temperature": 0.7}
    }
)
```

**æŸ¥çœ‹è¯„ä¼°æŠ¥å‘Š**ï¼š

```
============================================================
LangSmith Evaluator - æç¤ºè¯è´¨é‡è¯„ä¼°
============================================================

æç¤ºè¯: parameter_parser
æ•°æ®é›†: parameter_parser
å®éªŒåç§°: eval_parameter_parser_20241027_143022
è¯„ä¼°å™¨æ•°é‡: 2

å¼€å§‹è¯„ä¼°...

è¯„ä¼°ç»“æœæ±‡æ€»:
------------------------------------------------------------
  structure_valid: 95.00%
  parameter_extraction: 88.00%
------------------------------------------------------------
  æ€»åˆ†: 91.50%
  æµ‹è¯•æ•°: 10

æŸ¥çœ‹è¯¦ç»†ç»“æœ: https://smith.langchain.com/
```

### Step 4: ä¼˜åŒ–è¿­ä»£

**æ ¹æ®è¯„ä¼°ç»“æœä¼˜åŒ–æç¤ºè¯**ï¼š

1. **åœ¨ LangSmith Playground ä¸­æµ‹è¯•**
   - æ‰“å¼€ [LangSmith Playground](https://smith.langchain.com/)
   - é€‰æ‹©ä½ çš„ Dataset
   - å®æ—¶è°ƒæ•´æç¤ºè¯
   - æŸ¥çœ‹è¾“å‡ºå˜åŒ–

2. **æœ¬åœ°ä¿®æ”¹ YAML**
   ```yaml
   # ä¼˜åŒ–åçš„ parameter_parser.yaml
   messages:
     - role: system
       content: |
         ä½ æ˜¯ä¸“ä¸šçš„å‚æ•°æå–åŠ©æ‰‹ã€‚
         
         æå–è§„åˆ™ï¼š
         1. topic: å¿…éœ€ï¼Œæå–ä¸»é¢˜å…³é”®è¯
         2. year_range: è¯†åˆ«å¹´ä»½èŒƒå›´ï¼ˆå¦‚ "2023-2024"ï¼‰
         3. style: å¯é€‰ï¼ŒæŠ¥å‘Šé£æ ¼ï¼ˆä¸“ä¸š/é€šä¿—ï¼‰
         4. depth: å¯é€‰ï¼Œæ·±åº¦ï¼ˆç®€è¦/è¯¦ç»†/æ·±å…¥ï¼‰
         
         **è¾“å‡ºæ ¼å¼**ï¼ˆå¿…é¡»æ˜¯æœ‰æ•ˆ JSONï¼‰ï¼š
         {
           "topic": "string",
           "year_range": "string",
           "style": "string",
           "depth": "string"
         }
   ```

3. **é‡æ–°è¯„ä¼°**
   ```bash
   python -c "from prompts.prompt_manager import PromptManager; \
              PromptManager().evaluate_prompt('parameter_parser')"
   ```

4. **ç‰ˆæœ¬å¯¹æ¯”**
   ```python
   # å¯¹æ¯”ä¼˜åŒ–å‰å
   runner.compare_prompts(
       prompt_name='parameter_parser',
       dataset_name='parameter_parser',
       prompt_versions=['v1.0.0', 'v1.1.0']
   )
   ```

### Step 5: éƒ¨ç½²åˆ°ç”Ÿäº§

**æ¨é€åˆ° LangSmith Hub**ï¼š

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# æ¨é€åˆ° Hubï¼ˆå¸¦ç‰ˆæœ¬å·ï¼‰
manager.push(
    prompt_name='parameter_parser',
    commit_message='ä¼˜åŒ–å‚æ•°æå–å‡†ç¡®æ€§',
    change_type='minor'  # 'major' | 'minor' | 'patch'
)
```

**å›¢é˜Ÿåä½œ - è‡ªåŠ¨åŒæ­¥**ï¼š

```python
# å…¶ä»–å¼€å‘è€…æ‹‰å–æœ€æ–°ç‰ˆæœ¬
manager.pull('parameter_parser')  # è‡ªåŠ¨æ‹‰å–æœ€æ–°ç‰ˆæœ¬

# æˆ–å¯ç”¨è‡ªåŠ¨æ‹‰å–ï¼ˆé»˜è®¤å¼€å¯ï¼‰
manager = PromptManager(auto_pull=True)
prompt = manager.get('parameter_parser')  # è‡ªåŠ¨è·å–æœ€æ–°ç‰ˆ
```

**å›æ»šç‰ˆæœ¬**ï¼ˆå¦‚æœ‰é—®é¢˜ï¼‰ï¼š

```python
# æŸ¥çœ‹å†å²ç‰ˆæœ¬
manager.list_versions('parameter_parser')

# å›æ»šåˆ°æŒ‡å®šç‰ˆæœ¬
manager.rollback('parameter_parser', 'v1.0.0')
```

---

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        ç”¨æˆ·å±‚                                â”‚
â”‚  main.py  â”‚  prompts/  â”‚  evaluation/  â”‚  LangSmith UI      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      æ ¸å¿ƒæ¨¡å—å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Prompt ç®¡ç†    â”‚   è¯„ä¼°ç³»ç»Ÿ       â”‚   LangGraph æµç¨‹       â”‚
â”‚  - YAMLå­˜å‚¨     â”‚   - è¯„ä¼°å™¨æ³¨å†Œè¡¨  â”‚   - StateGraph       â”‚
â”‚  - ç‰ˆæœ¬ç®¡ç†     â”‚   - åŠ¨æ€åŠ è½½      â”‚   - èŠ‚ç‚¹ç¼–æ’          â”‚
â”‚  - HubåŒæ­¥      â”‚   - æ•°æ®é›†ç®¡ç†    â”‚   - æµå¼è¾“å‡º          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      åŸºç¡€æœåŠ¡å±‚                              â”‚
â”‚  LangSmith API  â”‚  LLM (Azure/OpenAI)  â”‚  å¤–éƒ¨å·¥å…·          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

#### 1. **é…ç½®é©±åŠ¨** - ä¸€åˆ‡çš†é…ç½®
```yaml
# prompts_config.yaml
prompts:
  parameter_parser:
    evaluators: ["structure_evaluator", "parameter_extraction_evaluator"]
    evaluator_weights: {structure_evaluator: 0.3}
```

#### 2. **èŒè´£åˆ†ç¦»** - å•ä¸€èŒè´£åŸåˆ™
```
PromptManager   â†’ æç¤ºè¯ç®¡ç†ï¼ˆåŠ è½½ã€ä¿å­˜ã€åŒæ­¥ï¼‰
EvaluationRunner â†’ è¯„ä¼°æµç¨‹ç¼–æ’ï¼ˆä¸åŒ…å«ä¸šåŠ¡é€»è¾‘ï¼‰
Evaluators      â†’ æŒ‰æç¤ºè¯ç±»å‹ç»„ç»‡ï¼ˆreport, parameter, summary...ï¼‰
```

#### 3. **å¯æ‰©å±•æ€§** - æ’ä»¶åŒ–æ¶æ„
```python
# æ–°å¢æç¤ºè¯è¯„ä¼°å™¨
# 1. åˆ›å»º evaluation/evaluators/your_type.py
# 2. æ³¨å†Œåˆ° EVALUATOR_REGISTRY
# 3. é…ç½®åˆ° prompts_config.yaml
# å®Œæˆï¼æ— éœ€ä¿®æ”¹æ ¸å¿ƒä»£ç 
```

---

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. Prompt ç®¡ç†ç³»ç»Ÿ

**ç‰¹æ€§**ï¼š
- âœ… YAML æ ‡å‡†åŒ–æ ¼å¼ï¼ˆChatPromptTemplateï¼‰
- âœ… ç‰ˆæœ¬ç®¡ç†ï¼ˆè¯­ä¹‰åŒ–ç‰ˆæœ¬å·ï¼‰
- âœ… LangSmith Hub åŒæ­¥
- âœ… è‡ªåŠ¨æ‹‰å–æœ€æ–°ç‰ˆæœ¬
- âœ… ä¸€é”®å›æ»š

**æ–‡ä»¶ç»“æ„**ï¼š
```
prompts/
â”œâ”€â”€ prompts_config.yaml      # æç¤ºè¯é…ç½®æ¸…å•
â”œâ”€â”€ parameter_parser.yaml    # å‚æ•°è§£ææç¤ºè¯
â”œâ”€â”€ report_generator.yaml    # æŠ¥å‘Šç”Ÿæˆæç¤ºè¯
â””â”€â”€ prompt_manager.py        # ç®¡ç†å™¨æ ¸å¿ƒä»£ç 
```

**æ ¸å¿ƒ API**ï¼š
```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# è·å–æç¤ºè¯ï¼ˆè‡ªåŠ¨æ‹‰å–æœ€æ–°ç‰ˆï¼‰
prompt_template = manager.build_prompt_from_name('parameter_parser')

# æ¨é€åˆ° Hub
manager.push('parameter_parser', commit_message='ä¼˜åŒ–æå–å‡†ç¡®æ€§')

# è¯„ä¼°è´¨é‡
result = manager.evaluate_prompt('parameter_parser')
```

### 2. è¯„ä¼°ç³»ç»Ÿ

**æ¶æ„è®¾è®¡**ï¼š
```
evaluation/
â”œâ”€â”€ evaluation.py           # è¯„ä¼°è¿è¡Œå™¨ï¼ˆæµç¨‹ç¼–æ’ï¼‰
â”œâ”€â”€ datasets.py            # æ•°æ®é›†ç®¡ç†
â””â”€â”€ evaluators/            # è¯„ä¼°å™¨ç›®å½•ï¼ˆæŒ‰æç¤ºè¯ç±»å‹ç»„ç»‡ï¼‰
    â”œâ”€â”€ __init__.py        # è¯„ä¼°å™¨æ³¨å†Œè¡¨
    â”œâ”€â”€ report.py          # æŠ¥å‘Šè¯„ä¼°å™¨
    â””â”€â”€ parameter.py       # å‚æ•°è¯„ä¼°å™¨
```

**è¯„ä¼°å™¨æ³¨å†Œè¡¨**ï¼ˆåŠ¨æ€åŠ è½½ï¼‰ï¼š
```python
# evaluation/evaluators/__init__.py
EVALUATOR_REGISTRY = {
    # é€šç”¨è¯„ä¼°å™¨
    "structure_evaluator": ReportEvaluators.structure_evaluator,
    "content_completeness_evaluator": ReportEvaluators.content_completeness_evaluator,
    
    # å‚æ•°è§£æä¸“ç”¨
    "parameter_extraction_evaluator": ParameterEvaluators.parameter_extraction_evaluator,
    "field_type_evaluator": ParameterEvaluators.field_type_evaluator,
}

# æ ¹æ®é…ç½®è‡ªåŠ¨åŠ è½½è¯„ä¼°å™¨
evaluators = get_evaluators_for_prompt(prompt_config)
```

**è¯„ä¼°å™¨ç±»å‹**ï¼š

| è¯„ä¼°å™¨ | ç±»å‹ | ç”¨é€” | é€‚ç”¨æç¤ºè¯ |
|--------|------|------|-----------|
| `structure_evaluator` | è§„åˆ™ | æ£€æŸ¥è¾“å‡ºç»“æ„ | æ‰€æœ‰ |
| `content_completeness_evaluator` | è§„åˆ™ | æ£€æŸ¥å†…å®¹å®Œæ•´æ€§ | report_generator |
| `relevance_evaluator` | LLM | æ£€æŸ¥å†…å®¹ç›¸å…³æ€§ | report_generator |
| `parameter_usage_evaluator` | è§„åˆ™ | æ£€æŸ¥å‚æ•°ä½¿ç”¨ | report_generator |
| `parameter_extraction_evaluator` | è§„åˆ™ | æ£€æŸ¥å‚æ•°æå–å‡†ç¡®æ€§ | parameter_parser |
| `field_type_evaluator` | è§„åˆ™ | æ£€æŸ¥å­—æ®µç±»å‹ | parameter_parser |

**è‡ªå®šä¹‰è¯„ä¼°å™¨**ï¼š
```python
# evaluation/evaluators/your_type.py
from langsmith.evaluation import EvaluationResult, run_evaluator

class YourEvaluators:
    @staticmethod
    @run_evaluator
    def your_custom_evaluator(run, example) -> EvaluationResult:
        """è‡ªå®šä¹‰è¯„ä¼°é€»è¾‘"""
        output = run.outputs.get("report", "")
        
        # ä½ çš„è¯„ä¼°é€»è¾‘
        score = calculate_score(output)
        
        return EvaluationResult(
            key="your_metric",
            score=score,
            comment="è¯„ä¼°è¯´æ˜"
        )
```

### 3. LangGraph å·¥ä½œæµ

**å¤šæ­¥éª¤æµç¨‹ç¼–æ’**ï¼š
```python
# graph/graph.py
workflow = StateGraph(ReportState)

# æ·»åŠ èŠ‚ç‚¹
workflow.add_node("parse_parameters", parse_parameters_node)
workflow.add_node("web_search", web_search_node)
workflow.add_node("generate_report", generate_report_node)
workflow.add_node("quality_check", quality_check_node)

# å®šä¹‰æµç¨‹
workflow.set_entry_point("parse_parameters")
workflow.add_edge("parse_parameters", "web_search")
workflow.add_edge("web_search", "generate_report")
workflow.add_edge("generate_report", "quality_check")
```

**çŠ¶æ€ç®¡ç†**ï¼š
```python
# graph/state.py
class ReportState(TypedDict):
    user_query: str
    topic: str
    year_range: str
    style: str
    depth: str
    search_results: List[Dict]
    report: str
    metadata: Dict[str, Any]
```

### 4. ä¸­é—´ç»“æœæ•è·

**è‡ªåŠ¨æ•è·è£…é¥°å™¨**ï¼š
```python
from tools.capture import capture_middle_result

@capture_middle_result(
    dataset_name="parameter_parser",
    step_name="parse_params"
)
def parse_parameters(user_query: str):
    # è§£æé€»è¾‘
    result = extract_params(user_query)
    return result

# è¿è¡Œåï¼Œç»“æœè‡ªåŠ¨ä¿å­˜åˆ° LangSmith Datasetï¼
```

**æ‰‹åŠ¨æ•è·**ï¼š
```python
from tools.capture import MiddleResultCapture

capture = MiddleResultCapture("my_dataset")

# æ•è·è¾“å…¥è¾“å‡º
capture.add_example(
    inputs={"query": "ç”¨æˆ·è¾“å…¥"},
    outputs={"result": "è¾“å‡ºç»“æœ"},
    metadata={"step": "parse_params"}
)

# æ‰¹é‡ä¸Šä¼ 
capture.upload_to_dataset()
```

### 5. LangSmith å®Œæ•´é›†æˆ

**è¿½è¸ªï¼ˆTracingï¼‰**ï¼š
```python
from config.langsmith_config import LangSmithConfig

# å¯ç”¨è¿½è¸ª
LangSmithConfig.enable_tracing(project_name="my_project")

# æ‰€æœ‰ LLM è°ƒç”¨è‡ªåŠ¨è¿½è¸ªåˆ° LangSmithï¼
```

**æ•°æ®é›†ç®¡ç†**ï¼š
```python
from evaluation.datasets import DatasetManager

manager = DatasetManager()

# åˆ›å»ºæ•°æ®é›†
manager.create_dataset_from_file(
    dataset_name="test_cases",
    filepath="examples/test_cases.json"
)

# åˆ—å‡ºæ‰€æœ‰æ•°æ®é›†
manager.list_datasets()

# åˆ é™¤æ•°æ®é›†
manager.delete_dataset("old_dataset")
```

---

## ğŸ“˜ ä½¿ç”¨æŒ‡å—

### åœºæ™¯ 1ï¼šå¼€å‘æ–°æç¤ºè¯

```bash
# 1. åˆ›å»º YAML æ–‡ä»¶
cat > prompts/summary.yaml << 'EOF'
version: v1.0.0
description: "ç”Ÿæˆå†…å®¹æ‘˜è¦"
messages:
  - role: system
    content: "ä½ æ˜¯æ‘˜è¦ç”ŸæˆåŠ©æ‰‹ï¼Œæå–å…³é”®ä¿¡æ¯ã€‚"
  - role: user
    content: "{content}"
input_variables: ["content"]
EOF

# 2. é…ç½®æç¤ºè¯
# ç¼–è¾‘ prompts/prompts_config.yaml æ·»åŠ é…ç½®

# 3. åˆ›å»ºæµ‹è¯•æ•°æ®é›†
python -c "
from evaluation.datasets import DatasetManager
manager = DatasetManager()
manager.create_dataset('summary_test', test_cases=[...])
"

# 4. æµ‹è¯•æç¤ºè¯
python -c "
from prompts.prompt_manager import PromptManager
result = PromptManager().evaluate_prompt('summary')
print(f'è´¨é‡è¯„åˆ†: {result[\"quality_score\"]:.2%}')
"

# 5. æ¨é€åˆ° Hub
python -c "
from prompts.prompt_manager import PromptManager
PromptManager().push('summary', commit_message='åˆå§‹ç‰ˆæœ¬')
"
```

### åœºæ™¯ 2ï¼šä¼˜åŒ–ç°æœ‰æç¤ºè¯

```python
from prompts.prompt_manager import PromptManager
from evaluation.evaluation import EvaluationRunner

manager = PromptManager()
runner = EvaluationRunner()

# 1. è¯„ä¼°å½“å‰ç‰ˆæœ¬
baseline = runner.evaluate_prompt(
    prompt_name='parameter_parser',
    dataset_name='parameter_parser',
    experiment_name='baseline_v1'
)

# 2. ä¿®æ”¹ prompts/parameter_parser.yaml

# 3. è¯„ä¼°ä¼˜åŒ–ç‰ˆæœ¬
optimized = runner.evaluate_prompt(
    prompt_name='parameter_parser',
    dataset_name='parameter_parser',
    experiment_name='optimized_v2'
)

# 4. å¯¹æ¯”ç»“æœ
print(f"Baseline åˆ†æ•°: {baseline['overall_score']:.2%}")
print(f"ä¼˜åŒ–ååˆ†æ•°: {optimized['overall_score']:.2%}")
print(f"æå‡: {(optimized['overall_score'] - baseline['overall_score']):.2%}")

# 5. å¦‚æœæå‡æ˜æ˜¾ï¼Œæ¨é€æ–°ç‰ˆæœ¬
if optimized['overall_score'] > baseline['overall_score']:
    manager.push('parameter_parser', commit_message='æå‡å‡†ç¡®æ€§', change_type='minor')
```

### åœºæ™¯ 3ï¼šA/B æµ‹è¯•ä¸åŒ LLM é…ç½®

```python
from evaluation.evaluation import EvaluationRunner

runner = EvaluationRunner()

# å¯¹æ¯”ä¸åŒæ¸©åº¦å‚æ•°
comparison = runner.compare_prompts(
    prompt_name='report_generator',
    dataset_name='report_test',
    llm_configs={
        "ä¿å®ˆæ¨¡å¼": {"temperature": 0.3},
        "å‡è¡¡æ¨¡å¼": {"temperature": 0.7},
        "åˆ›é€ æ¨¡å¼": {"temperature": 0.9}
    }
)

# æŸ¥çœ‹å¯¹æ¯”ç»“æœ
print("\nå„é…ç½®å¾—åˆ†:")
for config_name, result in comparison.items():
    print(f"  {config_name}: {result['overall_score']:.2%}")

print(f"\næ¨èé…ç½®: {comparison['recommended_version']}")
```

### åœºæ™¯ 4ï¼šå›¢é˜Ÿåä½œ

**å¼€å‘è€… A - ä¼˜åŒ–æç¤ºè¯**ï¼š
```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# æ‹‰å–æœ€æ–°ç‰ˆæœ¬
manager.pull('parameter_parser')

# æœ¬åœ°ä¿®æ”¹å¹¶æµ‹è¯•
result = manager.evaluate_prompt('parameter_parser')

# æ¨é€åˆ° Hub
if result['quality_score'] > 0.9:
    manager.push('parameter_parser', commit_message='ä¼˜åŒ–å‚æ•°æå–')
```

**å¼€å‘è€… B - ä½¿ç”¨æœ€æ–°ç‰ˆæœ¬**ï¼š
```python
from prompts.prompt_manager import PromptManager

# è‡ªåŠ¨æ‹‰å–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
manager = PromptManager(auto_pull=True)

# ä½¿ç”¨æç¤ºè¯ï¼ˆè‡ªåŠ¨è·å–æœ€æ–°ç‰ˆï¼‰
prompt = manager.build_prompt_from_name('parameter_parser')

# æ— éœ€æ‰‹åŠ¨åŒæ­¥ï¼Œè‡ªåŠ¨è·å–å›¢é˜Ÿæœ€æ–°ç‰ˆæœ¬ï¼
```

---

## ğŸ“ é«˜çº§ç‰¹æ€§

### 1. æµå¼è¾“å‡ºï¼ˆStreamingï¼‰

```python
from graph.graph import ReportGraphBuilder
import asyncio

async def stream_report():
    builder = ReportGraphBuilder()
    
    # å¼‚æ­¥æµå¼è¿è¡Œ
    final_state = await builder.arun(
        user_query="åˆ†æAIè¡Œä¸š2024å¹´å‘å±•"
    )
    
    # é€æ­¥è¾“å‡ºæ¯ä¸ªèŠ‚ç‚¹çš„ç»“æœ
    # parse_parameters -> web_search -> generate_report -> quality_check

asyncio.run(stream_report())
```

### 2. è‡ªå®šä¹‰è¯„ä¼°å™¨

```python
# 1. åˆ›å»ºè¯„ä¼°å™¨
# evaluation/evaluators/summary.py
class SummaryEvaluators:
    @staticmethod
    @run_evaluator
    def conciseness_evaluator(run, example):
        """ç®€æ´æ€§è¯„ä¼°"""
        output = run.outputs.get("summary", "")
        word_count = len(output)
        
        # è¯„åˆ†é€»è¾‘ï¼š100-200å­—æœ€ä½³
        if 100 <= word_count <= 200:
            score = 1.0
        else:
            score = max(0, 1 - abs(word_count - 150) / 150)
        
        return EvaluationResult(
            key="conciseness",
            score=score,
            comment=f"å­—æ•°: {word_count}"
        )

# 2. æ³¨å†Œè¯„ä¼°å™¨
# evaluation/evaluators/__init__.py
EVALUATOR_REGISTRY['conciseness_evaluator'] = SummaryEvaluators.conciseness_evaluator

# 3. é…ç½®ä½¿ç”¨
# prompts_config.yaml
prompts:
  summary:
    evaluators:
      - "structure_evaluator"
      - "conciseness_evaluator"  # ä½ çš„è‡ªå®šä¹‰è¯„ä¼°å™¨
```

### 3. æ‰¹é‡è¯„ä¼°

```python
from evaluation.evaluation import EvaluationRunner

runner = EvaluationRunner()

# æ‰¹é‡è¯„ä¼°æ‰€æœ‰æç¤ºè¯
prompts = ['parameter_parser', 'report_generator', 'summary']

results = {}
for prompt_name in prompts:
    results[prompt_name] = runner.evaluate_prompt(
        prompt_name=prompt_name,
        dataset_name=f"{prompt_name}_test"
    )

# ç”Ÿæˆè´¨é‡æŠ¥å‘Š
for name, result in results.items():
    print(f"{name}: {result['overall_score']:.2%}")
```

### 4. ç‰ˆæœ¬å›æ»š

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# æŸ¥çœ‹å†å²ç‰ˆæœ¬
versions = manager.list_versions('parameter_parser')
# v1.0.0, v1.1.0, v1.2.0

# å›æ»šåˆ°ç¨³å®šç‰ˆæœ¬
manager.rollback('parameter_parser', 'v1.1.0')

# éªŒè¯å›æ»šæ•ˆæœ
result = manager.evaluate_prompt('parameter_parser')
```

### 5. è‡ªåŠ¨åŒ– CI/CD

```bash
# .github/workflows/prompt_quality.yml
name: Prompt Quality Check

on: [push, pull_request]

jobs:
  evaluate:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Setup Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.10'
      
      - name: Install dependencies
        run: pip install -r requirements.txt
      
      - name: Run Evaluation
        env:
          LANGSMITH_API_KEY: ${{ secrets.LANGSMITH_API_KEY }}
        run: |
          python -c "
          from prompts.prompt_manager import PromptManager
          manager = PromptManager()
          
          for prompt in ['parameter_parser', 'report_generator']:
              result = manager.evaluate_prompt(prompt)
              score = result['quality_score']
              
              if score < 0.8:
                  print(f'âŒ {prompt} è´¨é‡ä¸è¾¾æ ‡: {score:.2%}')
                  exit(1)
              else:
                  print(f'âœ… {prompt} è´¨é‡åˆæ ¼: {score:.2%}')
          "
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
Langsmith-prompt-pipeline/
â”œâ”€â”€ config/                      # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ azure_config.py          # LLM é…ç½®ï¼ˆAzure/OpenAIï¼‰
â”‚   â””â”€â”€ langsmith_config.py      # LangSmith é…ç½®
â”‚
â”œâ”€â”€ prompts/                     # æç¤ºè¯ç®¡ç†
â”‚   â”œâ”€â”€ prompts_config.yaml      # æç¤ºè¯é…ç½®æ¸…å•
â”‚   â”œâ”€â”€ parameter_parser.yaml    # å‚æ•°è§£ææç¤ºè¯
â”‚   â”œâ”€â”€ report_generator.yaml    # æŠ¥å‘Šç”Ÿæˆæç¤ºè¯
â”‚   â””â”€â”€ prompt_manager.py        # æç¤ºè¯ç®¡ç†å™¨
â”‚
â”œâ”€â”€ evaluation/                  # è¯„ä¼°ç³»ç»Ÿ
â”‚   â”œâ”€â”€ evaluation.py            # è¯„ä¼°è¿è¡Œå™¨
â”‚   â”œâ”€â”€ datasets.py              # æ•°æ®é›†ç®¡ç†
â”‚   â””â”€â”€ evaluators/              # è¯„ä¼°å™¨ï¼ˆæŒ‰ç±»å‹ç»„ç»‡ï¼‰
â”‚       â”œâ”€â”€ __init__.py          # è¯„ä¼°å™¨æ³¨å†Œè¡¨
â”‚       â”œâ”€â”€ report.py            # æŠ¥å‘Šè¯„ä¼°å™¨
â”‚       â””â”€â”€ parameter.py         # å‚æ•°è¯„ä¼°å™¨
â”‚
â”œâ”€â”€ graph/                       # LangGraph å·¥ä½œæµ
â”‚   â”œâ”€â”€ graph.py                 # Graph æ„å»ºå™¨
â”‚   â”œâ”€â”€ state.py                 # çŠ¶æ€å®šä¹‰
â”‚   â””â”€â”€ nodes.py                 # èŠ‚ç‚¹å®ç°
â”‚
â”œâ”€â”€ tools/                       # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ capture.py               # ä¸­é—´ç»“æœæ•è·
â”‚   â””â”€â”€ search_tool.py           # æœç´¢å·¥å…·
â”‚
â”œâ”€â”€ main.py                      # ä¸»ç¨‹åºå…¥å£
â”œâ”€â”€ requirements.txt             # ä¾èµ–æ¸…å•
â””â”€â”€ README.md                    # æœ¬æ–‡æ¡£
```

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•åˆ‡æ¢ LLM æä¾›å•†ï¼Ÿ

**A**: ä¿®æ”¹ `.env` æ–‡ä»¶ï¼š

```bash
# ä½¿ç”¨ Azure OpenAI
LLM_PROVIDER="azure"
AZURE_ENDPOINT="..."
AZURE_DEPLOYMENT="..."

# æˆ–ä½¿ç”¨æ ‡å‡† OpenAI
LLM_PROVIDER="openai"
OPENAI_API_KEY="sk-..."
OPENAI_MODEL="gpt-4o"
```

### Q2: è¯„ä¼°å™¨å¦‚ä½•é€‰æ‹©ï¼Ÿ

**A**: æ ¹æ®æç¤ºè¯ç±»å‹é€‰æ‹©ï¼š

```yaml
# å‚æ•°æå–ç±»æç¤ºè¯
parameter_parser:
  evaluators:
    - "structure_evaluator"           # æ£€æŸ¥ JSON æ ¼å¼
    - "parameter_extraction_evaluator" # æ£€æŸ¥æå–å‡†ç¡®æ€§
    - "field_type_evaluator"          # æ£€æŸ¥å­—æ®µç±»å‹

# å†…å®¹ç”Ÿæˆç±»æç¤ºè¯
report_generator:
  evaluators:
    - "structure_evaluator"            # æ£€æŸ¥ç»“æ„
    - "content_completeness_evaluator" # æ£€æŸ¥å®Œæ•´æ€§
    - "relevance_evaluator"            # æ£€æŸ¥ç›¸å…³æ€§ï¼ˆLLMï¼‰
```

### Q3: å¦‚ä½•å¤„ç†å¤§è§„æ¨¡æµ‹è¯•æ•°æ®é›†ï¼Ÿ

**A**: ä½¿ç”¨åˆ†æ‰¹è¯„ä¼°ï¼š

```python
from evaluation.evaluation import EvaluationRunner

runner = EvaluationRunner()

# è®¾ç½®å¹¶å‘æ•°
result = runner.evaluate_prompt(
    prompt_name='parameter_parser',
    dataset_name='large_dataset',
    max_concurrency=10  # å¹¶å‘æ‰§è¡Œ
)
```

### Q4: å¦‚ä½•é›†æˆåˆ°ç°æœ‰é¡¹ç›®ï¼Ÿ

**A**: ä¸‰æ­¥é›†æˆï¼š

```python
# 1. å®‰è£…ä¾èµ–
pip install langsmith langchain-openai langgraph

# 2. å¤åˆ¶æ¨¡å—åˆ°ä½ çš„é¡¹ç›®
cp -r prompts/ evaluation/ config/ your_project/

# 3. ä½¿ç”¨
from prompts.prompt_manager import PromptManager
manager = PromptManager()
prompt = manager.build_prompt_from_name('your_prompt')
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [LangSmith å®˜æ–¹æ–‡æ¡£](https://docs.smith.langchain.com/)
- [LangChain å®˜æ–¹æ–‡æ¡£](https://python.langchain.com/)
- [LangGraph å®˜æ–¹æ–‡æ¡£](https://langchain-ai.github.io/langgraph/)

**é¡¹ç›®å†…æ–‡æ¡£**ï¼š
- `docs/evaluator-guide.md` - è¯„ä¼°å™¨å¼€å‘æŒ‡å—
- `docs/evaluation-configuration-guide.md` - è¯„ä¼°é…ç½®è¯¦è§£
- `docs/capture-decorator-guide.md` - ä¸­é—´ç»“æœæ•è·æŒ‡å—
- `EVALUATION_QA.md` - è¯„ä¼°ç³»ç»Ÿ Q&A
- `FEATURES_SUMMARY.md` - åŠŸèƒ½ç‰¹æ€§æ€»ç»“

---

## ğŸ¤ è´¡çŒ®æŒ‡å—

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

**å¼€å‘æµç¨‹**ï¼š
1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
4. æ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
5. å¼€å¯ Pull Request

---

## ğŸ“„ è®¸å¯è¯

MIT License - è¯¦è§ [LICENSE](LICENSE) æ–‡ä»¶

---

## ğŸ’¬ è”ç³»æ–¹å¼

- ä½œè€…: Your Name
- Email: your.email@example.com
- é¡¹ç›®ä¸»é¡µ: https://github.com/your-username/langsmith-prompt-pipeline

---

<p align="center">
  <b>ğŸŒŸ å¦‚æœè¿™ä¸ªé¡¹ç›®å¯¹ä½ æœ‰å¸®åŠ©ï¼Œè¯·ç»™ä¸€ä¸ª Starï¼ğŸŒŸ</b>
</p>
