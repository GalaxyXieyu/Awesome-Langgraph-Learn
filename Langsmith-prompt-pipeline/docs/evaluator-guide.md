# LangSmith Evaluator å®Œæ•´ä½¿ç”¨æŒ‡å—

> ğŸ“Š å¦‚ä½•ä½¿ç”¨ LangSmith Evaluator è¯„ä¼°å’Œä¼˜åŒ–æç¤ºè¯è´¨é‡

---

## ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [å†…ç½®è¯„ä¼°å™¨](#å†…ç½®è¯„ä¼°å™¨)
- [ä½¿ç”¨æ–¹å¼](#ä½¿ç”¨æ–¹å¼)
- [å®Œæ•´å·¥ä½œæµ](#å®Œæ•´å·¥ä½œæµ)
- [è‡ªå®šä¹‰è¯„ä¼°å™¨](#è‡ªå®šä¹‰è¯„ä¼°å™¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## å¿«é€Ÿå¼€å§‹

### 5 åˆ†é’Ÿä¸Šæ‰‹

```bash
# 1. ç¡®ä¿å·²é…ç½® LangSmith
export LANGSMITH_API_KEY="your-key"

# 2. è¿è¡Œç®€å•è¯„ä¼°
cd Langsmith-prompt-pipeline
python examples/run_evaluation_example.py

# 3. æŸ¥çœ‹ LangSmith æ§åˆ¶å°
# è®¿é—® https://smith.langchain.com/
```

### æœ€ç®€ç¤ºä¾‹

```python
from evaluation.run_evaluation import EvaluationRunner

# åˆ›å»ºè¯„ä¼°è¿è¡Œå™¨
runner = EvaluationRunner()

# è¯„ä¼°æç¤ºè¯
results = runner.evaluate_prompt(
    dataset_name="test_cases",
    test_cases_file="examples/test_cases.json"
)

# æŸ¥çœ‹ç»“æœ
print(f"æ€»åˆ†: {results['overall_score']:.2%}")
```

---

## æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Evaluatorï¼Ÿ

**Evaluatorï¼ˆè¯„ä¼°å™¨ï¼‰** æ˜¯ LangSmith æä¾›çš„è‡ªåŠ¨åŒ–æµ‹è¯•å·¥å…·ï¼Œç”¨äºï¼š
- è¯„ä¼° LLM è¾“å‡ºè´¨é‡
- å¯¹æ¯”ä¸åŒç‰ˆæœ¬çš„æç¤ºè¯
- è‡ªåŠ¨åŒ–è´¨é‡æŠŠå…³

### Evaluator vs äººå·¥æµ‹è¯•

| ç»´åº¦ | äººå·¥æµ‹è¯• | Evaluatorï¼ˆè‡ªåŠ¨åŒ–ï¼‰ |
|-----|---------|------------------|
| **é€Ÿåº¦** | æ…¢ï¼ˆ1ä¸ªæµ‹è¯• â‰ˆ 5åˆ†é’Ÿï¼‰ | å¿«ï¼ˆ100ä¸ªæµ‹è¯• â‰ˆ 2åˆ†é’Ÿï¼‰ |
| **ä¸€è‡´æ€§** | ä¸»è§‚ï¼Œä¸åŒäººè¯„åˆ†ä¸åŒ | å®¢è§‚ï¼Œæ ‡å‡†ç»Ÿä¸€ |
| **è¦†ç›–ç‡** | ä½ï¼ˆé€šå¸¸ 3-5 ä¸ªç”¨ä¾‹ï¼‰ | é«˜ï¼ˆå¯æµ‹è¯• 100+ ç”¨ä¾‹ï¼‰ |
| **æˆæœ¬** | é«˜ï¼ˆéœ€è¦äººåŠ›ï¼‰ | ä½ï¼ˆå…¨è‡ªåŠ¨ï¼‰ |
| **é€‚ç”¨åœºæ™¯** | ç»†å¾®è´¨é‡åˆ¤æ–­ | æ‰¹é‡è´¨é‡æ£€æµ‹ã€ç‰ˆæœ¬å¯¹æ¯” |

**æœ€ä½³å®è·µ**ï¼šç»“åˆä½¿ç”¨
- Evaluatorï¼šå¿«é€Ÿç­›é€‰ã€æ‰¹é‡æµ‹è¯•ã€æŒç»­é›†æˆ
- äººå·¥æµ‹è¯•ï¼šæœ€ç»ˆéªŒæ”¶ã€è¾¹ç•Œæƒ…å†µã€åˆ›æ„è¯„ä¼°

---

## å†…ç½®è¯„ä¼°å™¨

æœ¬é¡¹ç›®æä¾› 4 ä¸ªä¸“ä¸šè¯„ä¼°å™¨ï¼ˆ`evaluation/evaluators.py`ï¼‰ï¼š

### 1. structure_evaluatorï¼ˆç»“æ„è¯„ä¼°å™¨ï¼‰

**ä½œç”¨**ï¼šæ£€æŸ¥æŠ¥å‘Šçš„åŸºæœ¬ç»“æ„

**è¯„ä¼°é¡¹**ï¼š
- âœ… æ ‡é¢˜å±‚çº§ï¼ˆè‡³å°‘ 3 ä¸ª `#` æ ‡é¢˜ï¼‰
- âœ… æ®µè½æ•°é‡ï¼ˆè‡³å°‘ 5 ä¸ªæ®µè½ï¼‰
- âœ… æœ€å°é•¿åº¦ï¼ˆâ‰¥ 800 å­—ç¬¦ï¼‰
- âœ… æœ€å¤§é•¿åº¦ï¼ˆâ‰¤ 20000 å­—ç¬¦ï¼‰

**è¯„åˆ†é€»è¾‘**ï¼š
```python
score = é€šè¿‡æ£€æŸ¥é¡¹æ•° / æ€»æ£€æŸ¥é¡¹æ•°
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
structure_valid: 0.75
comment: é€šè¿‡ 3/4 é¡¹æ£€æŸ¥ã€‚æœªé€šè¿‡: min_length
```

### 2. content_completeness_evaluatorï¼ˆå†…å®¹å®Œæ•´æ€§è¯„ä¼°å™¨ï¼‰

**ä½œç”¨**ï¼šæ£€æŸ¥æŠ¥å‘Šæ˜¯å¦åŒ…å«å¿…è¦ç« èŠ‚

**å¿…è¦ç« èŠ‚**ï¼š
- æ‘˜è¦/æ¦‚è¿°ï¼ˆExecutive Summaryï¼‰
- èƒŒæ™¯ï¼ˆBackgroundï¼‰
- å‘ç°/åˆ†æï¼ˆFindings/Analysisï¼‰
- ç»“è®º/å»ºè®®ï¼ˆConclusion/Recommendationï¼‰

**è¯„åˆ†é€»è¾‘**ï¼š
```python
score = åŒ…å«ç« èŠ‚æ•° / å¿…è¦ç« èŠ‚æ•°
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
content_complete: 1.0
comment: åŒ…å« 4/4 ä¸ªå¿…è¦ç« èŠ‚
```

### 3. relevance_evaluatorï¼ˆç›¸å…³æ€§è¯„ä¼°å™¨ï¼‰â­

**ä½œç”¨**ï¼šä½¿ç”¨ LLM è¯„ä¼°å†…å®¹è´¨é‡

**è¯„ä¼°ç»´åº¦**ï¼š
1. ä¸»é¢˜ç›¸å…³æ€§ï¼ˆå†…å®¹æ˜¯å¦ç´§æ‰£ä¸»é¢˜ï¼‰
2. ä¿¡æ¯ä¸°å¯Œåº¦ï¼ˆæ˜¯å¦æä¾›æœ‰ä»·å€¼çš„ä¿¡æ¯ï¼‰
3. é€»è¾‘è¿è´¯æ€§ï¼ˆå†…å®¹æ˜¯å¦æœ‰é€»è¾‘æ€§ï¼‰
4. ä¸“ä¸šæ€§ï¼ˆè¯­è¨€å’Œåˆ†ææ˜¯å¦ä¸“ä¸šï¼‰

**è¯„åˆ†é€»è¾‘**ï¼š
```python
# ä½¿ç”¨ GPT-4 è¿›è¡Œè¯„åˆ†
overall_score = (relevance + information + coherence + professionalism) / 4
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
relevance: 0.88
comment: ä¸»é¢˜ç›¸å…³ï¼Œå†…å®¹ä¸°å¯Œï¼Œé€»è¾‘æ¸…æ™°ï¼Œä¸“ä¸šæ€§å¼º
```

### 4. parameter_usage_evaluatorï¼ˆå‚æ•°ä½¿ç”¨è¯„ä¼°å™¨ï¼‰

**ä½œç”¨**ï¼šæ£€æŸ¥å¤šå‚æ•°æç¤ºè¯æ˜¯å¦æ­£ç¡®ä½¿ç”¨å‚æ•°

**æ£€æŸ¥é¡¹**ï¼š
- å¹´ä»½èŒƒå›´æ˜¯å¦åœ¨æŠ¥å‘Šä¸­æåŠ
- å…³æ³¨é¢†åŸŸæ˜¯å¦è¢«è¦†ç›–
- é£æ ¼æ˜¯å¦ç¬¦åˆè¦æ±‚ï¼ˆformal/casual/concise/detailedï¼‰

**è¯„åˆ†é€»è¾‘**ï¼š
```python
score = æ­£ç¡®ä½¿ç”¨å‚æ•°æ•° / æ€»å‚æ•°æ•°
```

**ç¤ºä¾‹è¾“å‡º**ï¼š
```
parameter_usage: 1.0
comment: å‚æ•°ä½¿ç”¨æ£€æŸ¥: 3/3 é€šè¿‡
```

---

## ä½¿ç”¨æ–¹å¼

### æ–¹å¼ 1ï¼šå‘½ä»¤è¡Œè¿è¡Œï¼ˆæ¨èï¼‰

```bash
# åŸºç¡€è¯„ä¼°
python evaluation/run_evaluation.py \
  --dataset test_cases \
  --test-file examples/test_cases.json

# ç‰ˆæœ¬å¯¹æ¯”
python evaluation/run_evaluation.py \
  --dataset test_cases \
  --compare v1.0 v1.1 v1.2

# ä¿å­˜æŠ¥å‘Š
python evaluation/run_evaluation.py \
  --dataset test_cases \
  --output evaluation_reports/latest.json
```

### æ–¹å¼ 2ï¼šä¸ PromptManager é›†æˆ

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# ç®€å•æµ‹è¯•ï¼ˆåªæµ‹è¯•æ ¼å¼ï¼‰
result = manager.evaluate_prompt(
    'report_generator',
    use_full_pipeline=False
)

# å®Œæ•´æµ‹è¯•ï¼ˆä½¿ç”¨æ‰€æœ‰ä¸“ä¸šè¯„ä¼°å™¨ï¼‰
result = manager.evaluate_prompt(
    'report_generator',
    use_full_pipeline=True  # â­ æ¨è
)

print(f"è´¨é‡åˆ†æ•°: {result['quality_score']:.2%}")
for key, score in result['scores'].items():
    print(f"  {key}: {score:.2%}")
```

### æ–¹å¼ 3ï¼šåœ¨æ¨é€å‰è‡ªåŠ¨è¯„ä¼°

```python
manager = PromptManager()

# æ¨é€æ—¶è‡ªåŠ¨è¿è¡Œè¯„ä¼°
manager.push(
    'report_generator',
    with_test=True,  # è‡ªåŠ¨è¯„ä¼°
    create_backup=True
)

# è¾“å‡ºç¤ºä¾‹ï¼š
# [2/4] æ­¥éª¤ 2/4: è¿è¡Œ LangSmith æµ‹è¯•...
# [TEST] LangSmith è‡ªåŠ¨æµ‹è¯•: report_generator
#   è´¨é‡åˆ†æ•°: 92%
# [OK] æµ‹è¯•é€šè¿‡
```

### æ–¹å¼ 4ï¼šPython ä»£ç è°ƒç”¨

```python
from evaluation.run_evaluation import EvaluationRunner
from evaluation.evaluators import ReportEvaluators

runner = EvaluationRunner()

# è‡ªå®šä¹‰è¯„ä¼°å™¨
results = runner.evaluate_prompt(
    dataset_name="test_cases",
    experiment_name="my_experiment",
    evaluators=[
        ReportEvaluators.structure_evaluator,
        ReportEvaluators.relevance_evaluator,
    ]
)
```

---

## å®Œæ•´å·¥ä½œæµ

### åœºæ™¯ï¼šä¼˜åŒ–æç¤ºè¯è´¨é‡

#### ç¬¬ 1 æ­¥ï¼šåˆ›å»ºæµ‹è¯•æ•°æ®é›†

**é€‰é¡¹ Aï¼šä»æ–‡ä»¶åˆ›å»º**

```json
// examples/test_cases.json
[
  {
    "id": "test_001",
    "name": "AIè¡Œä¸šæ­£å¼æŠ¥å‘Š",
    "input": {
      "user_query": "å†™ä¸€ä»½äººå·¥æ™ºèƒ½è¡Œä¸š2023-2024å¹´çš„åˆ†ææŠ¥å‘Š"
    },
    "quality_criteria": {
      "min_length": 2000,
      "must_include": ["æŠ€æœ¯åˆ›æ–°", "å¸‚åœºè§„æ¨¡"]
    }
  }
]
```

**é€‰é¡¹ Bï¼šè‡ªåŠ¨æ•è·ï¼ˆæ¨èï¼‰**

```bash
# è¿è¡Œç¨‹åºè‡ªåŠ¨æ•è·çœŸå®æµ‹è¯•æ•°æ®
python main.py --query "äººå·¥æ™ºèƒ½è¡Œä¸šåˆ†æ" --style formal
python main.py --query "é‡‘èç§‘æŠ€æŠ¥å‘Š" --depth deep

# è‡ªåŠ¨æ¨é€åˆ° LangSmith Dataset âœ…
```

#### ç¬¬ 2 æ­¥ï¼šè¿è¡ŒåŸºå‡†è¯„ä¼°

```python
from evaluation.run_evaluation import EvaluationRunner

runner = EvaluationRunner()

# è¯„ä¼°å½“å‰ç‰ˆæœ¬ï¼ˆv1.0ï¼‰
baseline = runner.evaluate_prompt(
    dataset_name="report_generator",
    experiment_name="baseline_v1.0"
)

print(f"åŸºå‡†åˆ†æ•°: {baseline['overall_score']:.2%}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
è¯„ä¼°ç»“æœæ±‡æ€»:
------------------------------------------------------------
  structure_valid: 85%
  content_complete: 90%
  relevance: 78%
  parameter_usage: 92%
------------------------------------------------------------
  æ€»åˆ†: 86%
  æµ‹è¯•æ•°: 5
```

#### ç¬¬ 3 æ­¥ï¼šä¼˜åŒ–æç¤ºè¯

åœ¨ LangSmith Playground æˆ–æœ¬åœ°ä¿®æ”¹æç¤ºè¯ï¼š

```yaml
# prompts/report_generator.yaml

messages:
  - role: system
    content: |
      ä½ æ˜¯ä¸€ä½èµ„æ·±çš„è¡Œä¸šåˆ†æä¸“å®¶ã€‚
      
      # ä¼˜åŒ–ç‚¹ 1ï¼šæ›´æ˜ç¡®çš„è§’è‰²å®šä¹‰
      ä½ æ‹¥æœ‰ 10 å¹´ä»¥ä¸Šçš„è¡Œä¸šç ”ç©¶ç»éªŒï¼Œæ“…é•¿ï¼š
      - æ•°æ®é©±åŠ¨çš„æ·±åº¦åˆ†æ
      - æ¸…æ™°çš„é€»è¾‘æ¡†æ¶
      - ä¸“ä¸šçš„å•†ä¸šæ´å¯Ÿ
      
      # ä¼˜åŒ–ç‚¹ 2ï¼šæ›´è¯¦ç»†çš„è¾“å‡ºè¦æ±‚
      æŠ¥å‘Šç»“æ„å¿…é¡»åŒ…å«ï¼š
      1. æ‰§è¡Œæ‘˜è¦ï¼ˆ200å­—ï¼‰
      2. è¡Œä¸šèƒŒæ™¯ï¼ˆ500å­—ï¼‰
      3. æ ¸å¿ƒå‘ç°ï¼ˆ1000å­—ï¼‰
      4. è¶‹åŠ¿é¢„æµ‹ï¼ˆ500å­—ï¼‰
      5. æˆ˜ç•¥å»ºè®®ï¼ˆ300å­—ï¼‰
```

#### ç¬¬ 4 æ­¥ï¼šè¯„ä¼°ä¼˜åŒ–æ•ˆæœ

```python
# è¯„ä¼°æ–°ç‰ˆæœ¬ï¼ˆv1.1ï¼‰
improved = runner.evaluate_prompt(
    dataset_name="report_generator",
    experiment_name="improved_v1.1"
)

print(f"ä¼˜åŒ–ååˆ†æ•°: {improved['overall_score']:.2%}")
print(f"æå‡: {(improved['overall_score'] - baseline['overall_score']):.2%}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
è¯„ä¼°ç»“æœæ±‡æ€»:
------------------------------------------------------------
  structure_valid: 95%  (+10%)
  content_complete: 100% (+10%)
  relevance: 92%  (+14%)
  parameter_usage: 95%  (+3%)
------------------------------------------------------------
  æ€»åˆ†: 95%
  æå‡: +9%
```

#### ç¬¬ 5 æ­¥ï¼šç‰ˆæœ¬å¯¹æ¯”ï¼ˆå¯é€‰ï¼‰

```python
# å¯¹æ¯”å¤šä¸ªç‰ˆæœ¬
comparison = runner.compare_prompts(
    dataset_name="report_generator",
    prompt_versions=["v1.0", "v1.1", "v1.2"]
)

print(f"æ¨èç‰ˆæœ¬: {comparison['recommended_version']}")
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
ç‰ˆæœ¬å¯¹æ¯”æŠ¥å‘Š
============================================================

å„ç»´åº¦å¯¹æ¯”:
------------------------------------------------------------
ç»´åº¦                       v1.0        v1.1        v1.2
------------------------------------------------------------
content_complete          90.00%      100.00%     100.00%
parameter_usage           92.00%      95.00%      97.00%
relevance                 78.00%      92.00%      88.00%
structure_valid           85.00%      95.00%      98.00%
------------------------------------------------------------

æ€»åˆ†å¯¹æ¯”                  86.25%      95.50%      95.75%

âœ“ æ¨èç‰ˆæœ¬: v1.2
```

#### ç¬¬ 6 æ­¥ï¼šæ¨é€æœ€ä¼˜ç‰ˆæœ¬

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# æ¨é€æœ€ä¼˜ç‰ˆæœ¬
manager.push(
    'report_generator',
    with_test=True,      # å†æ¬¡éªŒè¯
    create_backup=True   # å¤‡ä»½æ—§ç‰ˆæœ¬
)
```

---

## è‡ªå®šä¹‰è¯„ä¼°å™¨

### ä¸ºä»€ä¹ˆéœ€è¦è‡ªå®šä¹‰è¯„ä¼°å™¨ï¼Ÿ

å†…ç½®è¯„ä¼°å™¨è¦†ç›–é€šç”¨åœºæ™¯ï¼Œä½†ä½ å¯èƒ½éœ€è¦ï¼š
- è¯„ä¼°ä¸šåŠ¡ç‰¹å®šæŒ‡æ ‡ï¼ˆå¦‚ä¸“ä¸šæœ¯è¯­ä½¿ç”¨ç‡ï¼‰
- æ£€æŸ¥åˆè§„æ€§è¦æ±‚ï¼ˆå¦‚ç¦ç”¨è¯æ±‡ï¼‰
- è¯„ä¼°æ ¼å¼è¦æ±‚ï¼ˆå¦‚ç‰¹å®šç« èŠ‚ï¼‰

### ç¤ºä¾‹ï¼šåˆ›å»ºè‡ªå®šä¹‰è¯„ä¼°å™¨

```python
from langsmith.evaluation import run_evaluator, EvaluationResult

@run_evaluator
def terminology_evaluator(run, example) -> EvaluationResult:
    """
    æ£€æŸ¥ä¸“ä¸šæœ¯è¯­ä½¿ç”¨æƒ…å†µ
    """
    output = run.outputs.get("report", "") if run.outputs else ""
    
    # å®šä¹‰å¿…éœ€çš„ä¸“ä¸šæœ¯è¯­
    required_terms = [
        "CAGR",           # å¤åˆå¹´å¢é•¿ç‡
        "å¸‚åœºä»½é¢",
        "ç«äº‰æ ¼å±€",
        "SWOTåˆ†æ"
    ]
    
    # æ£€æŸ¥æ¯ä¸ªæœ¯è¯­æ˜¯å¦å‡ºç°
    found_terms = [term for term in required_terms if term in output]
    score = len(found_terms) / len(required_terms)
    
    return EvaluationResult(
        key="terminology_usage",
        score=score,
        comment=f"ä½¿ç”¨äº† {len(found_terms)}/{len(required_terms)} ä¸ªä¸“ä¸šæœ¯è¯­"
    )
```

### ä½¿ç”¨è‡ªå®šä¹‰è¯„ä¼°å™¨

```python
from evaluation.run_evaluation import EvaluationRunner
from evaluation.evaluators import ReportEvaluators

runner = EvaluationRunner()

results = runner.evaluate_prompt(
    dataset_name="test_cases",
    evaluators=[
        ReportEvaluators.structure_evaluator,
        ReportEvaluators.relevance_evaluator,
        terminology_evaluator,  # è‡ªå®šä¹‰è¯„ä¼°å™¨
    ]
)
```

### æ›´å¤šè‡ªå®šä¹‰ç¤ºä¾‹

#### 1. ç¦ç”¨è¯æ£€æŸ¥

```python
@run_evaluator
def forbidden_words_evaluator(run, example) -> EvaluationResult:
    """æ£€æŸ¥æ˜¯å¦åŒ…å«ç¦ç”¨è¯"""
    output = run.outputs.get("report", "")
    
    forbidden = ["å¯èƒ½", "æˆ–è®¸", "å¤§æ¦‚", "ä¹Ÿè®¸"]  # é¿å…ä¸ç¡®å®šæ€§è¯æ±‡
    found_forbidden = [word for word in forbidden if word in output]
    
    score = 1.0 if not found_forbidden else 0.5
    
    return EvaluationResult(
        key="no_forbidden_words",
        score=score,
        comment=f"å‘ç° {len(found_forbidden)} ä¸ªç¦ç”¨è¯" if found_forbidden else "æœªå‘ç°ç¦ç”¨è¯"
    )
```

#### 2. æ•°æ®å¼•ç”¨æ£€æŸ¥

```python
@run_evaluator
def data_citation_evaluator(run, example) -> EvaluationResult:
    """æ£€æŸ¥æ˜¯å¦å¼•ç”¨å…·ä½“æ•°æ®"""
    output = run.outputs.get("report", "")
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ•°å­—å’Œç™¾åˆ†æ¯”
    import re
    has_numbers = bool(re.search(r'\d+', output))
    has_percentage = bool(re.search(r'\d+%', output))
    has_year = bool(re.search(r'20\d{2}', output))
    
    checks = [has_numbers, has_percentage, has_year]
    score = sum(checks) / len(checks)
    
    return EvaluationResult(
        key="data_citation",
        score=score,
        comment=f"åŒ…å«æ•°æ®å¼•ç”¨: {sum(checks)}/3"
    )
```

---

## æœ€ä½³å®è·µ

### 1. è¯„ä¼°å™¨é€‰æ‹©

**åœºæ™¯ Aï¼šå¿«é€ŸéªŒè¯æç¤ºè¯æ ¼å¼**
```python
# ä½¿ç”¨ç®€å•è¯„ä¼°å™¨
evaluators = [
    ReportEvaluators.structure_evaluator,
]
```

**åœºæ™¯ Bï¼šå…¨é¢è´¨é‡è¯„ä¼°ï¼ˆæ¨èï¼‰**
```python
# ä½¿ç”¨æ‰€æœ‰è¯„ä¼°å™¨
evaluators = [
    ReportEvaluators.structure_evaluator,
    ReportEvaluators.content_completeness_evaluator,
    ReportEvaluators.relevance_evaluator,
    ReportEvaluators.parameter_usage_evaluator,
]
```

**åœºæ™¯ Cï¼šç”Ÿäº§ç¯å¢ƒæŠŠå…³**
```python
# ä½¿ç”¨æ‰€æœ‰è¯„ä¼°å™¨ + è‡ªå®šä¹‰è¯„ä¼°å™¨
evaluators = [
    *ReportEvaluators.__dict__.values(),  # æ‰€æœ‰å†…ç½®
    terminology_evaluator,                 # è‡ªå®šä¹‰
    forbidden_words_evaluator,
]
```

### 2. Dataset ç®¡ç†

**è§„åˆ™**ï¼š
- 1 ä¸ªæç¤ºè¯ = 1 ä¸ªç‹¬ç«‹ Dataset
- Dataset åç§° = æç¤ºè¯åç§°ï¼ˆå¦‚ `report_generator`ï¼‰
- è‡³å°‘ 5 ä¸ªæµ‹è¯•ç”¨ä¾‹
- è¦†ç›–ä¸åŒåœºæ™¯ï¼ˆformal/casual, shallow/deepï¼‰

**ç¤ºä¾‹**ï¼š
```
Datasets:
  - parameter_parser (å‚æ•°è§£ææç¤ºè¯ä¸“ç”¨)
  - report_generator (æŠ¥å‘Šç”Ÿæˆæç¤ºè¯ä¸“ç”¨)
  - summary_generator (æ‘˜è¦ç”Ÿæˆæç¤ºè¯ä¸“ç”¨)
```

### 3. è¯„ä¼°é¢‘ç‡

| é˜¶æ®µ | é¢‘ç‡ | ç›®çš„ |
|-----|------|------|
| **å¼€å‘é˜¶æ®µ** | æ¯æ¬¡ä¿®æ”¹å | åŠæ—¶å‘ç°é—®é¢˜ |
| **ä¼˜åŒ–é˜¶æ®µ** | æ¯ä¸ªç‰ˆæœ¬ | å¯¹æ¯”æ•ˆæœ |
| **æ¨é€å‰** | å¿…é¡» | è´¨é‡æŠŠå…³ |
| **ç”Ÿäº§ç¯å¢ƒ** | æ¯å‘¨ | ç›‘æ§è´¨é‡ä¸‹é™ |

### 4. åˆ†æ•°é˜ˆå€¼è®¾ç½®

```python
# prompts/prompts_config.yaml

prompts:
  report_generator:
    min_quality_score: 0.85  # æœ€ä½è´¨é‡åˆ†æ•°
    
    # å„ç»´åº¦æœ€ä½åˆ†æ•°ï¼ˆå¯é€‰ï¼‰
    min_scores:
      structure_valid: 0.80
      content_complete: 0.90
      relevance: 0.85
      parameter_usage: 0.80
```

### 5. é”™è¯¯å¤„ç†

```python
try:
    result = runner.evaluate_prompt(dataset_name="test_cases")
except Exception as e:
    print(f"è¯„ä¼°å¤±è´¥: {e}")
    
    # é™çº§æ–¹æ¡ˆï¼šä½¿ç”¨ç®€å•è¯„ä¼°å™¨
    result = manager.evaluate_prompt(
        'report_generator',
        use_full_pipeline=False
    )
```

---

## å¸¸è§é—®é¢˜

### Q1: è¯„ä¼°å¾ˆæ…¢æ€ä¹ˆåŠï¼Ÿ

**åŸå› **ï¼š
- LLM è¯„ä¼°å™¨ï¼ˆrelevance_evaluatorï¼‰éœ€è¦è°ƒç”¨ GPT-4
- æµ‹è¯•ç”¨ä¾‹å¤ªå¤š

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ¡ˆ 1ï¼šå‡å°‘æµ‹è¯•ç”¨ä¾‹
# ä» 100 ä¸ªå‡å°‘åˆ° 20 ä¸ªä»£è¡¨æ€§ç”¨ä¾‹

# æ–¹æ¡ˆ 2ï¼šåªç”¨è§„åˆ™è¯„ä¼°å™¨ï¼ˆä¸ç”¨ LLMï¼‰
evaluators = [
    ReportEvaluators.structure_evaluator,
    ReportEvaluators.content_completeness_evaluator,
    # ReportEvaluators.relevance_evaluator,  # æ³¨é‡Šæ‰
    ReportEvaluators.parameter_usage_evaluator,
]

# æ–¹æ¡ˆ 3ï¼šæé«˜å¹¶å‘æ•°
runner.evaluate_prompt(
    dataset_name="test_cases",
    max_concurrency=5  # é»˜è®¤ 2
)
```

### Q2: å¦‚ä½•æŸ¥çœ‹è¯¦ç»†çš„è¯„ä¼°ç»“æœï¼Ÿ

è®¿é—® LangSmith æ§åˆ¶å°ï¼š
1. https://smith.langchain.com/
2. ç‚¹å‡» `Projects` â†’ é€‰æ‹©ä½ çš„é¡¹ç›®
3. æ‰¾åˆ°å¯¹åº”çš„ experiment
4. æŸ¥çœ‹æ¯ä¸ªæµ‹è¯•ç”¨ä¾‹çš„è¯¦ç»†åˆ†æ•°

### Q3: Dataset å¦‚ä½•æ›´æ–°ï¼Ÿ

```python
from evaluation.datasets import DatasetManager

dm = DatasetManager()

# åˆ é™¤æ—§æ•°æ®é›†
dm.delete_dataset("report_generator")

# é‡æ–°åˆ›å»º
dm.create_dataset_from_file(
    dataset_name="report_generator",
    filepath="examples/test_cases_updated.json"
)
```

æˆ–è€…ä½¿ç”¨è‡ªåŠ¨æ•è·ï¼ˆæ¨èï¼‰ï¼š
```bash
# è¿è¡Œç¨‹åºï¼Œè‡ªåŠ¨æ•è·æ–°æ•°æ®
python main.py --query "æ–°çš„æµ‹è¯•åœºæ™¯"

# è‡ªåŠ¨æ¨é€åˆ° Dataset âœ…
```

### Q4: å¦‚ä½•å¯¹æ¯”å†å²ç‰ˆæœ¬ï¼Ÿ

```python
# åœ¨ LangSmith æ§åˆ¶å°
# Projects â†’ Experiments â†’ é€‰æ‹©ä¸¤ä¸ªå®éªŒ â†’ Compare
```

æˆ–ä½¿ç”¨ä»£ç ï¼š
```python
runner.compare_prompts(
    dataset_name="test_cases",
    prompt_versions=["v1.0", "v1.1"]  # æŒ‡å®šç‰ˆæœ¬
)
```

---

## æ€»ç»“

### æ ¸å¿ƒä»·å€¼

| åŠŸèƒ½ | ä»·å€¼ |
|-----|------|
| **è‡ªåŠ¨åŒ–è¯„ä¼°** | èŠ‚çœ 90% æµ‹è¯•æ—¶é—´ |
| **ç‰ˆæœ¬å¯¹æ¯”** | å®¢è§‚é€‰æ‹©æœ€ä¼˜ç‰ˆæœ¬ |
| **è´¨é‡æŠŠå…³** | é¿å…ä½è´¨é‡æç¤ºè¯ä¸Šçº¿ |
| **æŒç»­ç›‘æ§** | åŠæ—¶å‘ç°è´¨é‡ä¸‹é™ |

### å¿«é€Ÿå‚è€ƒ

```bash
# è¿è¡Œè¯„ä¼°
python evaluation/run_evaluation.py --dataset test_cases

# ç‰ˆæœ¬å¯¹æ¯”
python evaluation/run_evaluation.py --dataset test_cases --compare v1.0 v1.1

# é›†æˆåˆ° PromptManager
python -c "
from prompts.prompt_manager import PromptManager
manager = PromptManager()
result = manager.evaluate_prompt('report_generator', use_full_pipeline=True)
print(f'è´¨é‡åˆ†æ•°: {result[\"quality_score\"]:.2%}')
"

# è¿è¡Œç¤ºä¾‹
python examples/run_evaluation_example.py
```

### æ¨èå·¥ä½œæµ

```
1. å¼€å‘æç¤ºè¯
   â†“
2. è¿è¡Œç¨‹åº â†’ è‡ªåŠ¨æ•è·æµ‹è¯•æ•°æ®
   â†“
3. è¿è¡Œè¯„ä¼° â†’ è·å–åŸºå‡†åˆ†æ•°
   â†“
4. ä¼˜åŒ–æç¤ºè¯
   â†“
5. é‡æ–°è¯„ä¼° â†’ å¯¹æ¯”æ•ˆæœ
   â†“
6. æ¨é€æœ€ä¼˜ç‰ˆæœ¬ï¼ˆè‡ªåŠ¨è¯„ä¼°ï¼‰
```

---

## ç›¸å…³é“¾æ¥

- [LangSmith å®˜æ–¹æ–‡æ¡£](https://docs.langchain.com/langsmith/)
- [Evaluator API æ–‡æ¡£](https://docs.langchain.com/langsmith/evaluation)
- [é¡¹ç›® README](../README.md)
- [Dataset æ•è·æŒ‡å—](capture-decorator-guide.md)

