# è¯„ä¼°å™¨é…ç½®æŒ‡å—

> å¦‚ä½•ä¸ºæ¯ä¸ªæç¤ºè¯é…ç½®ä¸“å±çš„è¯„ä¼°å™¨å’Œæµ‹è¯•æ•°æ®é›†

---

## ğŸ“– ç›®å½•

- [æ ¸å¿ƒæ¦‚å¿µ](#æ ¸å¿ƒæ¦‚å¿µ)
- [é…ç½®æ–‡ä»¶è¯¦è§£](#é…ç½®æ–‡ä»¶è¯¦è§£)
- [æ–¹æ³•åæ›´æ–°](#æ–¹æ³•åæ›´æ–°)
- [use_full_pipeline å‚æ•°è¯¦è§£](#use_full_pipeline-å‚æ•°è¯¦è§£)
- [Dataset é€‰æ‹©ç­–ç•¥](#dataset-é€‰æ‹©ç­–ç•¥)
- [è‡ªåŠ¨ç‰ˆæœ¬å·](#è‡ªåŠ¨ç‰ˆæœ¬å·)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

---

## æ ¸å¿ƒæ¦‚å¿µ

### è¯„ä¼°å™¨ä¸æç¤ºè¯çš„å…³ç³»

```
æ¯ä¸ªæç¤ºè¯ â†’ ä¸“å± Dataset + ä¸“å±è¯„ä¼°å™¨ç»„åˆ

parameter_parserï¼ˆå‚æ•°è§£æï¼‰
  â”œâ”€ Dataset: parameter_parser
  â”œâ”€ è¯„ä¼°å™¨:
  â”‚   â”œâ”€ structure_evaluatorï¼ˆé€šç”¨ï¼‰
  â”‚   â””â”€ parameter_extraction_evaluatorï¼ˆä¸“å±ï¼‰
  â””â”€ æƒé‡: [0.3, 0.7]

report_generatorï¼ˆæŠ¥å‘Šç”Ÿæˆï¼‰
  â”œâ”€ Dataset: report_generator
  â”œâ”€ è¯„ä¼°å™¨:
  â”‚   â”œâ”€ structure_evaluatorï¼ˆé€šç”¨ï¼‰
  â”‚   â”œâ”€ content_completeness_evaluatorï¼ˆé€šç”¨ï¼‰
  â”‚   â”œâ”€ relevance_evaluatorï¼ˆé€šç”¨ï¼‰
  â”‚   â””â”€ report_quality_evaluatorï¼ˆä¸“å±ï¼‰
  â””â”€ æƒé‡: [0.15, 0.20, 0.35, 0.30]
```

---

## é…ç½®æ–‡ä»¶è¯¦è§£

### prompts_config.yaml çš„å®Œæ•´é…ç½®

```yaml
# Prompts é…ç½®æ–‡ä»¶

# ç‰ˆæœ¬ç®¡ç†é…ç½®
versioning:
  auto_increment: true         # â­ è‡ªåŠ¨é€’å¢ç‰ˆæœ¬å·
  version_format: "semantic"   # semantic: v1.2.3 | timestamp: v20241027-153000
  create_backup: true          # â­ æ¨é€æ—¶è‡ªåŠ¨åˆ›å»ºå¤‡ä»½

# Prompt é…ç½®
prompts:
  # ç¤ºä¾‹ï¼šå‚æ•°è§£æ Prompt
  parameter_parser:
    file: "parameter_parser.yaml"
    hub_name: "parameter_parser"
    test_dataset: "parameter_parser"  # â­ ç‹¬ç«‹ Dataset
    min_quality_score: 0.8
    description: "ä»ç”¨æˆ·è¾“å…¥ä¸­æå–ç»“æ„åŒ–å‚æ•°"
    
    # â­ ä¸“å±è¯„ä¼°å™¨é…ç½®
    evaluators:
      - "structure_evaluator"              # é€šç”¨è¯„ä¼°å™¨
      - "parameter_extraction_evaluator"   # ä¸“å±è¯„ä¼°å™¨ï¼ˆéœ€è‡ªå®šä¹‰ï¼‰
    
    # â­ è¯„ä¼°å™¨æƒé‡ï¼ˆå¯é€‰ï¼Œä¸è®¾ç½®åˆ™å¹³å‡åˆ†é…ï¼‰
    evaluator_weights:
      structure_evaluator: 0.3
      parameter_extraction_evaluator: 0.7
    
  # ç¤ºä¾‹ï¼šæŠ¥å‘Šç”Ÿæˆ Prompt
  report_generator:
    file: "report_generator.yaml"
    hub_name: "report_generator"
    test_dataset: "report_generator"  # â­ ç‹¬ç«‹ Dataset
    min_quality_score: 0.85
    description: "ç”Ÿæˆç»“æ„åŒ–çš„åˆ†ææŠ¥å‘Š"
    
    # â­ ä¸“å±è¯„ä¼°å™¨é…ç½®
    evaluators:
      - "structure_evaluator"
      - "content_completeness_evaluator"
      - "relevance_evaluator"             # LLM è¯„ä¼°å™¨ï¼ˆæœ€é‡è¦ï¼‰
      - "parameter_usage_evaluator"
    
    # â­ è¯„ä¼°å™¨æƒé‡
    evaluator_weights:
      structure_evaluator: 0.15
      content_completeness_evaluator: 0.20
      relevance_evaluator: 0.35          # æƒé‡æœ€é«˜
      parameter_usage_evaluator: 0.30
```

### é…ç½®é¡¹è¯´æ˜

| é…ç½®é¡¹ | è¯´æ˜ | ç¤ºä¾‹ | æ˜¯å¦å¿…éœ€ |
|-------|------|------|---------|
| `file` | YAML æ–‡ä»¶å | `report_generator.yaml` | âœ… |
| `hub_name` | Hub ä¸Šçš„åç§° | `report_generator` | âœ… |
| `test_dataset` | æµ‹è¯•æ•°æ®é›†åç§° | `report_generator` | âœ… |
| `min_quality_score` | æœ€ä½è´¨é‡åˆ†æ•° | `0.85` | âœ… |
| `evaluators` | è¯„ä¼°å™¨åˆ—è¡¨ | `["structure_evaluator", ...]` | ğŸ”¶ å¯é€‰ |
| `evaluator_weights` | è¯„ä¼°å™¨æƒé‡ | `{structure: 0.15, ...}` | ğŸ”¶ å¯é€‰ |

---

## æ–¹æ³•åæ›´æ–°

### æ¨èä½¿ç”¨ `evaluate_prompt()`

```python
from prompts.prompt_manager import PromptManager

manager = PromptManager()

# âœ… æ¨èï¼šä½¿ç”¨æ–°æ–¹æ³•å
result = manager.evaluate_prompt('report_generator', use_full_pipeline=True)

# âš ï¸ å…¼å®¹ï¼šæ—§æ–¹æ³•åä»ç„¶å¯ç”¨
result = manager.evaluate_prompt('report_generator', use_full_pipeline=True)
```

**ä¸ºä»€ä¹ˆæ”¹åï¼Ÿ**
- `evaluate_prompt` æ›´ç›´è§‚ï¼Œæ˜ç¡®è¡¨ç¤º"è¯„ä¼°æç¤ºè¯"
- ~~`test_with_langsmith` å¤ªé•¿ä¸”ä¸å¤Ÿè¯­ä¹‰åŒ–~~ **å·²æ”¹ä¸º `evaluate_prompt`**
- ä¿ç•™æ—§æ–¹æ³•åæ˜¯ä¸ºäº†å‘åå…¼å®¹

---

## use_full_pipeline å‚æ•°è¯¦è§£

### å‚æ•°ä½œç”¨

```python
use_full_pipeline: bool = False
```

| å€¼ | æµ‹è¯•å†…å®¹ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|----|---------|------|---------|
| `False`ï¼ˆé»˜è®¤ï¼‰| åªæµ‹è¯• Prompt æ ¼å¼ | âš¡ å¿«ï¼ˆ5-10ç§’ï¼‰ | å¼€å‘é˜¶æ®µï¼Œå¿«é€ŸéªŒè¯æ ¼å¼ |
| `True` | å®Œæ•´æµç¨‹ + æ‰€æœ‰è¯„ä¼°å™¨ | ğŸ¢ æ…¢ï¼ˆ30-60ç§’ï¼‰ | æ¨é€å‰ï¼Œå…¨é¢è´¨é‡è¯„ä¼° |

### è¯¦ç»†å¯¹æ¯”

#### use_full_pipeline=Falseï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰

```python
result = manager.evaluate_prompt('report_generator', use_full_pipeline=False)

# æµ‹è¯•å†…å®¹ï¼š
# âœ… Prompt æ ¼å¼æ˜¯å¦æ­£ç¡®
# âœ… å‚æ•°æ˜¯å¦èƒ½æ­£ç¡®æ ¼å¼åŒ–
# âœ… åŸºæœ¬çš„æ ¼å¼æ£€æŸ¥
# âŒ ä¸è¿è¡Œå®Œæ•´æµç¨‹
# âŒ ä¸è°ƒç”¨ LLM
# âŒ ä¸ä½¿ç”¨ä¸“ä¸šè¯„ä¼°å™¨

# é€‚ç”¨åœºæ™¯ï¼š
# - å¼€å‘é˜¶æ®µé¢‘ç¹æµ‹è¯•
# - åªä¿®æ”¹äº† Prompt æ ¼å¼
# - å¿«é€ŸéªŒè¯è¯­æ³•é”™è¯¯
```

#### use_full_pipeline=Trueï¼ˆå®Œæ•´æ¨¡å¼ï¼‰

```python
result = manager.evaluate_prompt('report_generator', use_full_pipeline=True)

# æµ‹è¯•å†…å®¹ï¼š
# âœ… è¿è¡Œå®Œæ•´çš„æŠ¥å‘Šç”Ÿæˆæµç¨‹
# âœ… è°ƒç”¨ LLM ç”ŸæˆçœŸå®è¾“å‡º
# âœ… è¿è¡Œæ‰€æœ‰é…ç½®çš„è¯„ä¼°å™¨
# âœ… ä½¿ç”¨çœŸå® Dataset
# âœ… è®¡ç®—åŠ æƒå¹³å‡åˆ†æ•°

# é€‚ç”¨åœºæ™¯ï¼š
# - æ¨é€å‰çš„æœ€ç»ˆéªŒè¯
# - ä¼˜åŒ–åå¯¹æ¯”æ•ˆæœ
# - ç‰ˆæœ¬å¯¹æ¯”è¯„ä¼°
```

### å®é™…è¿è¡Œå¯¹æ¯”

**å¿«é€Ÿæ¨¡å¼è¾“å‡º**ï¼š
```
[EVAL] è¯„ä¼°æç¤ºè¯: report_generator
  [OK] æ ¼å¼éªŒè¯é€šè¿‡
     - æµ‹è¯•ç”¨ä¾‹: 5
     - è´¨é‡åˆ†æ•°: 90%
å®Œæˆæ—¶é—´: 8 ç§’
```

**å®Œæ•´æ¨¡å¼è¾“å‡º**ï¼š
```
[EVAL] è¯„ä¼°æç¤ºè¯: report_generator
  ä½¿ç”¨å®Œæ•´æµç¨‹æµ‹è¯•ï¼ˆåŒ…å«ä¸“ä¸šè¯„ä¼°å™¨ï¼‰

è¯„ä¼°ç»“æœæ±‡æ€»:
------------------------------------------------------------
  structure_evaluator: 95%
  content_completeness_evaluator: 100%
  relevance_evaluator: 92%
  parameter_usage_evaluator: 97%
------------------------------------------------------------
  æ€»åˆ†: 96%ï¼ˆåŠ æƒå¹³å‡ï¼‰
  æµ‹è¯•æ•°: 5

æŸ¥çœ‹è¯¦ç»†ç»“æœ: https://smith.langchain.com/

å®Œæˆæ—¶é—´: 45 ç§’
```

---

## Dataset é€‰æ‹©ç­–ç•¥

### åŸåˆ™ï¼šä¸€ä¸ªæç¤ºè¯ = ä¸€ä¸ªç‹¬ç«‹ Dataset

```
âŒ é”™è¯¯ï¼šæ‰€æœ‰æç¤ºè¯å…±ç”¨ä¸€ä¸ª Dataset
prompts:
  parameter_parser:
    test_dataset: "test_cases"  # âŒ
  report_generator:
    test_dataset: "test_cases"  # âŒ
  summary_generator:
    test_dataset: "test_cases"  # âŒ

é—®é¢˜ï¼š
- Dataset å†…å®¹æ··æ‚
- ä¸åŒæç¤ºè¯çš„è¾“å…¥æ ¼å¼ä¸åŒ
- è¯„ä¼°ç»“æœä¸å‡†ç¡®

âœ… æ­£ç¡®ï¼šæ¯ä¸ªæç¤ºè¯æœ‰ç‹¬ç«‹ Dataset
prompts:
  parameter_parser:
    test_dataset: "parameter_parser"  # âœ…
  report_generator:
    test_dataset: "report_generator"  # âœ…
  summary_generator:
    test_dataset: "summary_generator"  # âœ…

ä¼˜ç‚¹ï¼š
- ä¸“å±æµ‹è¯•ç”¨ä¾‹
- è¾“å…¥æ ¼å¼ä¸€è‡´
- è¯„ä¼°æ›´å‡†ç¡®
```

### Dataset å†…å®¹è®¾è®¡

**parameter_parser Dataset ç¤ºä¾‹**ï¼š
```json
// datasets/parameter_parser.json
[
  {
    "inputs": {
      "user_query": "å†™ä¸€ä»½ AI è¡Œä¸š 2024 å¹´æŠ¥å‘Š"
    },
    "outputs": {
      "expected_params": {
        "topic": "AI è¡Œä¸š",
        "year_range": "2024"
      }
    }
  },
  {
    "inputs": {
      "user_query": "é‡‘èç§‘æŠ€è¯¦ç»†åˆ†æï¼Œå…³æ³¨æ”¯ä»˜å’ŒåŒºå—é“¾"
    },
    "outputs": {
      "expected_params": {
        "topic": "é‡‘èç§‘æŠ€",
        "focus_areas": "æ”¯ä»˜,åŒºå—é“¾",
        "depth": "detailed"
      }
    }
  }
]
```

**report_generator Dataset ç¤ºä¾‹**ï¼š
```json
// datasets/report_generator.json
[
  {
    "inputs": {
      "user_query": "ç”Ÿæˆ AI è¡Œä¸šæŠ¥å‘Š"
    },
    "quality_criteria": {
      "min_length": 2000,
      "must_include": ["æ‘˜è¦", "èƒŒæ™¯", "åˆ†æ", "ç»“è®º"],
      "style": "formal"
    }
  }
]
```

---

## è‡ªåŠ¨ç‰ˆæœ¬å·

### é…ç½®è‡ªåŠ¨ç‰ˆæœ¬å·

```yaml
# prompts_config.yaml

versioning:
  auto_increment: true         # â­ å¯ç”¨è‡ªåŠ¨é€’å¢
  version_format: "semantic"   # ç‰ˆæœ¬å·æ ¼å¼
  create_backup: true          # è‡ªåŠ¨åˆ›å»ºå¤‡ä»½
```

### ç‰ˆæœ¬å·æ ¼å¼

**1. è¯­ä¹‰åŒ–ç‰ˆæœ¬ï¼ˆæ¨èï¼‰**
```yaml
version_format: "semantic"

# ç”Ÿæˆæ ¼å¼ï¼šv1.2.3
# - major.minor.patch
# - 1.0.0 â†’ 1.0.1 â†’ 1.0.2 â†’ 1.1.0 â†’ 2.0.0
```

**2. æ—¶é—´æˆ³ç‰ˆæœ¬**
```yaml
version_format: "timestamp"

# ç”Ÿæˆæ ¼å¼ï¼šv20241027-153000
# - vYYYYMMDD-HHMMSS
# - é€‚åˆé¢‘ç¹æ›´æ–°çš„åœºæ™¯
```

### ä½¿ç”¨æ–¹å¼

#### æ–¹å¼ 1ï¼šè‡ªåŠ¨ç‰ˆæœ¬å·ï¼ˆæ¨èï¼‰

```python
manager = PromptManager()

# æ¨é€æ—¶è‡ªåŠ¨ç”Ÿæˆç‰ˆæœ¬å·
manager.push('report_generator')

# è‡ªåŠ¨å‘ç”Ÿï¼š
# 1. è¯»å–å½“å‰ç‰ˆæœ¬ï¼ˆä» .versions/report_generator.jsonï¼‰
# 2. é€’å¢ç‰ˆæœ¬å·ï¼šv1.2.3 â†’ v1.2.4
# 3. æ¨é€ä¸»ç‰ˆæœ¬åˆ° Hub: "report_generator"
# 4. åˆ›å»ºå¤‡ä»½ç‰ˆæœ¬: "report_generator-v1.2.4"
# 5. æ›´æ–°æœ¬åœ° YAML: version: v1.2.4
# 6. ä¿å­˜ç‰ˆæœ¬å†å²
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
[4/4] æ­¥éª¤ 4/4: åˆ›å»ºç‰ˆæœ¬å¤‡ä»½...
  å½“å‰ç‰ˆæœ¬: v1.2.3
  æ–°ç‰ˆæœ¬: v1.2.4
[OK] å·²å¤‡ä»½åˆ°: report_generator-v1.2.4
  ç‰ˆæœ¬å·: v1.2.4
[OK] å·²æ›´æ–° YAML ç‰ˆæœ¬å·: v1.2.4
```

#### æ–¹å¼ 2ï¼šæ‰‹åŠ¨æŒ‡å®šå˜æ›´ç±»å‹

```python
# å°ä¼˜åŒ–ï¼ˆé»˜è®¤ï¼‰
manager.push('report_generator')  # v1.2.3 â†’ v1.2.4

# æ–°åŠŸèƒ½
manager.push('report_generator', change_type='minor')  # v1.2.3 â†’ v1.3.0

# å¤§æ”¹åŠ¨
manager.push('report_generator', change_type='major')  # v1.2.3 â†’ v2.0.0
```

### ç‰ˆæœ¬å†å²è®°å½•

```json
// prompts/.versions/report_generator.json
{
  "current_version": "v1.2.4",
  "updated_at": "2024-10-27T15:30:00",
  "history": [
    {
      "version": "v1.0.0",
      "timestamp": "2024-10-01T10:00:00",
      "change_type": "initial"
    },
    {
      "version": "v1.1.0",
      "timestamp": "2024-10-15T14:20:00",
      "change_type": "minor"
    },
    {
      "version": "v1.2.0",
      "timestamp": "2024-10-20T09:15:00",
      "change_type": "minor"
    },
    {
      "version": "v1.2.4",
      "timestamp": "2024-10-27T15:30:00",
      "change_type": "patch"
    }
  ]
}
```

---

## å®Œæ•´ç¤ºä¾‹

### åœºæ™¯ï¼šæ·»åŠ æ–°çš„æç¤ºè¯ "summary_generator"

#### æ­¥éª¤ 1ï¼šé…ç½®æç¤ºè¯

```yaml
# prompts_config.yaml

prompts:
  summary_generator:
    file: "summary_generator.yaml"
    hub_name: "summary_generator"
    test_dataset: "summary_generator"  # â­ ç‹¬ç«‹ Dataset
    min_quality_score: 0.80
    description: "ç”Ÿæˆæ–‡ç« æ‘˜è¦"
    
    # â­ ä¸“å±è¯„ä¼°å™¨
    evaluators:
      - "structure_evaluator"
      - "summary_quality_evaluator"  # è‡ªå®šä¹‰è¯„ä¼°å™¨
    
    evaluator_weights:
      structure_evaluator: 0.3
      summary_quality_evaluator: 0.7
```

#### æ­¥éª¤ 2ï¼šåˆ›å»ºä¸“å±è¯„ä¼°å™¨

```python
# evaluation/evaluators.py

@run_evaluator
def summary_quality_evaluator(run, example) -> EvaluationResult:
    """æ‘˜è¦è´¨é‡è¯„ä¼°å™¨"""
    output = run.outputs.get("summary", "")
    
    checks = {
        "length": 100 <= len(output) <= 500,  # é•¿åº¦åˆç†
        "concise": "æ€»ä¹‹" in output or "ç»¼ä¸Š" in output,  # æœ‰æ€»ç»“
        "key_points": output.count("â€¢") >= 3  # è‡³å°‘3ä¸ªè¦ç‚¹
    }
    
    score = sum(checks.values()) / len(checks)
    
    return EvaluationResult(
        key="summary_quality",
        score=score,
        comment=f"é€šè¿‡ {sum(checks.values())}/3 é¡¹æ£€æŸ¥"
    )
```

#### æ­¥éª¤ 3ï¼šæ·»åŠ è¯„ä¼°å™¨æ˜ å°„

```python
# prompts/prompt_manager.py

def _get_evaluators_by_names(self, evaluator_names):
    evaluator_map = {
        'structure_evaluator': ReportEvaluators.structure_evaluator,
        'content_completeness_evaluator': ReportEvaluators.content_completeness_evaluator,
        'relevance_evaluator': ReportEvaluators.relevance_evaluator,
        'parameter_usage_evaluator': ReportEvaluators.parameter_usage_evaluator,
        'summary_quality_evaluator': summary_quality_evaluator,  # â­ æ·»åŠ 
    }
    # ...
```

#### æ­¥éª¤ 4ï¼šåˆ›å»ºæµ‹è¯• Dataset

```bash
# è¿è¡Œç¨‹åºè‡ªåŠ¨æ•è·
python main.py --query "æ€»ç»“è¿™ç¯‡æ–‡ç« "
python main.py --query "ç”Ÿæˆæ‘˜è¦ï¼Œé‡ç‚¹å…³æ³¨æ ¸å¿ƒè§‚ç‚¹"

# è‡ªåŠ¨æ¨é€åˆ° "summary_generator" Dataset
```

#### æ­¥éª¤ 5ï¼šè¯„ä¼°å’Œæ¨é€

```python
manager = PromptManager()

# è¯„ä¼°ï¼ˆä½¿ç”¨ä¸“å±è¯„ä¼°å™¨ï¼‰
result = manager.evaluate_prompt('summary_generator', use_full_pipeline=True)
print(f"è´¨é‡åˆ†æ•°: {result['quality_score']:.2%}")

# æ¨é€ï¼ˆè‡ªåŠ¨ç‰ˆæœ¬å· + è‡ªåŠ¨å¤‡ä»½ï¼‰
manager.push('summary_generator')
# â†’ ç”Ÿæˆ v1.0.0
# â†’ æ¨é€åˆ° Hub: "summary_generator" å’Œ "summary_generator-v1.0.0"
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **æ¯ä¸ªæç¤ºè¯æœ‰ç‹¬ç«‹é…ç½®**
   - ç‹¬ç«‹ Dataset
   - ä¸“å±è¯„ä¼°å™¨ç»„åˆ
   - è‡ªå®šä¹‰æƒé‡

2. **ä½¿ç”¨æ–°æ–¹æ³•å**
   - `manager.evaluate_prompt()` æ›´æ¸…æ™°

3. **use_full_pipeline å‚æ•°**
   - False: å¿«é€Ÿæ ¼å¼æ£€æŸ¥
   - True: å®Œæ•´æµç¨‹è¯„ä¼°

4. **è‡ªåŠ¨ç‰ˆæœ¬å·**
   - æ¨é€æ—¶è‡ªåŠ¨ç”Ÿæˆ
   - è‡ªåŠ¨åˆ›å»ºå¤‡ä»½
   - è‡ªåŠ¨æ›´æ–° YAML

5. **EvaluationResult å®˜æ–¹ç±»**
   - å¿…éœ€ï¼škey, score
   - æ¨èï¼šcomment
   - @run_evaluator æ˜¯å®˜æ–¹è£…é¥°å™¨

### å¿«é€Ÿå‚è€ƒ

```python
# é…ç½®
# prompts_config.yaml: å®šä¹‰è¯„ä¼°å™¨å’Œæƒé‡

# å¼€å‘
manager.evaluate_prompt('xxx', use_full_pipeline=False)  # å¿«é€Ÿ

# æ¨é€å‰
manager.evaluate_prompt('xxx', use_full_pipeline=True)   # å®Œæ•´
manager.push('xxx')  # è‡ªåŠ¨ç‰ˆæœ¬å·
```

---

**å®Œæˆï¼** ğŸ‰ ç°åœ¨ä½ æœ‰äº†ä¸€ä¸ªçµæ´»ã€å¯é…ç½®ã€è‡ªåŠ¨åŒ–çš„è¯„ä¼°ç³»ç»Ÿã€‚

