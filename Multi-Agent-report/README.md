# ğŸ¤– Multi-Agent æµå¼è¾“å‡ºç³»ç»Ÿå®Œæ•´æŒ‡å—

ä¸€ä¸ªåŸºäº LangGraph çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼Œå®ç°äº†å®Œæ•´çš„æµå¼è¾“å‡ºå’Œæ™ºèƒ½åä½œåŠŸèƒ½ã€‚

## ğŸ“‹ ç›®å½•

- [æ ¸å¿ƒç‰¹æ€§](#æ ¸å¿ƒç‰¹æ€§)
- [å¤š Agent æµå¼è¾“å‡ºå®ç°](#å¤š-agent-æµå¼è¾“å‡ºå®ç°)
- [å¤š Agent ç³»ç»Ÿæ„å»ºä¼˜åŒ–](#å¤š-agent-ç³»ç»Ÿæ„å»ºä¼˜åŒ–)
- [ç³»ç»Ÿæ¶æ„](#ç³»ç»Ÿæ¶æ„)
- [å®‰è£…å’Œä½¿ç”¨](#å®‰è£…å’Œä½¿ç”¨)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

## ğŸ¯ æ ¸å¿ƒç‰¹æ€§

- ğŸ¤– **å¤šæ™ºèƒ½ä½“åä½œ**ï¼šç›‘ç£å™¨ + ä¸“ä¸šåŒ– Agent çš„åˆ†å±‚æ¶æ„
- ğŸ–¨ï¸ **çœŸæ­£çš„æ‰“å­—æœºæ•ˆæœ**ï¼šToken çº§åˆ«çš„å®æ—¶æµå¼è¾“å‡º
- ğŸ”„ **æ™ºèƒ½è·¯ç”±**ï¼šæ ¹æ®ç”¨æˆ·è¾“å…¥è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„ Agent
- ğŸ› ï¸ **å·¥å…·é›†æˆ**ï¼šè®¡ç®—å™¨ã€æœç´¢å¼•æ“ç­‰å¤šç§å·¥å…·æ”¯æŒ
- ğŸ“Š **å®æ—¶è¿›åº¦åé¦ˆ**ï¼šå®Œæ•´çš„æ‰§è¡ŒçŠ¶æ€å’Œè¿›åº¦æ˜¾ç¤º
- ğŸ¯ **ä»»åŠ¡å®Œæˆæ£€æµ‹**ï¼šè‡ªåŠ¨åˆ¤æ–­ä»»åŠ¡å®ŒæˆçŠ¶æ€

---

## ğŸ–¨ï¸ å¤š Agent æµå¼è¾“å‡ºå®ç°

### æ ¸å¿ƒæŒ‘æˆ˜

å¤š Agent ç³»ç»Ÿçš„æµå¼è¾“å‡ºé¢ä¸´ä»¥ä¸‹ä¸»è¦æŒ‘æˆ˜ï¼š

1. **Agent è¾“å‡ºç»“æ„å¤æ‚**ï¼šåŒ…å«å·¥å…·è°ƒç”¨ã€å·¥å…·ç»“æœã€æœ€ç»ˆå›å¤ç­‰å¤šä¸ªé˜¶æ®µ
2. **é…ç½®å†²çª**ï¼šLangGraph çš„ config ä¸ Agent çš„æµå¼é…ç½®ä¸å…¼å®¹
3. **æ¶ˆæ¯ç±»å‹å¤šæ ·**ï¼šAIMessageChunkã€ToolMessageã€HumanMessage ç­‰éœ€è¦åˆ†åˆ«å¤„ç†
4. **çŠ¶æ€åŒæ­¥**ï¼šéœ€è¦å°†æµå¼è¾“å‡ºä¸æ•´ä½“å·¥ä½œæµçŠ¶æ€åŒæ­¥

### ğŸ”§ è§£å†³æ–¹æ¡ˆ

#### 1. Agent è¾“å‡ºç»“æ„åˆ†æ

Agent çš„æµå¼è¾“å‡ºåŒ…å«ä¸‰ä¸ªå…³é”®é˜¶æ®µï¼š

```python
async for chunk in agent.astream(input, stream_mode="messages"):
    if isinstance(chunk, tuple) and len(chunk) == 2:
        message, metadata = chunk
        
        # é˜¶æ®µ1: å·¥å…·è°ƒç”¨å‡†å¤‡
        if type(message).__name__ == "AIMessageChunk" and hasattr(message, 'tool_calls'):
            # å¤„ç†å·¥å…·è°ƒç”¨ä¿¡æ¯
            
        # é˜¶æ®µ2: å·¥å…·æ‰§è¡Œç»“æœ  
        elif type(message).__name__ == "ToolMessage":
            # å¤„ç†å·¥å…·è¿”å›ç»“æœ
            
        # é˜¶æ®µ3: æœ€ç»ˆå›å¤æµå¼è¾“å‡º
        elif type(message).__name__ == "AIMessageChunk" and message.content:
            # å¤„ç† token çº§æµå¼å›å¤
```

#### 2. é…ç½®éš”ç¦»ç­–ç•¥

**å…³é”®å‘ç°**ï¼šLangGraph çš„ config å¯¹è±¡ä¸èƒ½ç›´æ¥ä¼ é€’ç»™ Agent çš„ astream()

```python
# âŒ é”™è¯¯ï¼šä¼šå¯¼è‡´æµå¼è¾“å‡ºå¤±è´¥
async for chunk in agent.astream(input, config=langgraph_config, stream_mode="messages"):

# âœ… æ­£ç¡®ï¼šç§»é™¤ config å‚æ•°
async for chunk in agent.astream(input, stream_mode="messages"):
```

#### 3. å®Œå–„çš„æµå¼å¤„ç†å™¨

```python
async def process_agent_stream(agent, input_dict, writer, agent_type):
    """å®Œå–„çš„ Agent æµå¼å¤„ç†å™¨"""
    
    full_response = ""
    chunk_count = 0
    tool_calls_count = 0
    content_chunks = 0
    current_tool = None
    
    try:
        async for chunk in agent.astream(input_dict, stream_mode="messages"):
            chunk_count += 1
            
            if isinstance(chunk, tuple) and len(chunk) == 2:
                message, metadata = chunk
                msg_type = type(message).__name__
                
                # å¤„ç†ä¸åŒç±»å‹çš„æ¶ˆæ¯
                if msg_type == "AIMessageChunk":
                    if hasattr(message, 'tool_calls') and message.tool_calls:
                        # å·¥å…·è°ƒç”¨é˜¶æ®µ
                        tool_calls_count += 1
                        current_tool = message.tool_calls[0].get('name', 'unknown_tool')
                        
                        writer({
                            "type": "tool_call",
                            "agent": agent_type,
                            "tool": current_tool,
                            "status": f"æ­£åœ¨è°ƒç”¨å·¥å…·: {current_tool}",
                            "progress": 40
                        })
                    
                    elif hasattr(message, 'content') and message.content:
                        # AI æœ€ç»ˆå›å¤çš„æµå¼è¾“å‡º
                        content = str(message.content)
                        full_response += content
                        content_chunks += 1
                        
                        writer({
                            "type": "streaming_content",
                            "agent": agent_type,
                            "content": content,
                            "status": f"Agentç”Ÿæˆå›å¤ä¸­...",
                            "progress": 70
                        })
                
                elif msg_type == "ToolMessage":
                    # å·¥å…·æ‰§è¡Œç»“æœ
                    if hasattr(message, 'content') and message.content:
                        tool_result = str(message.content)
                        full_response += tool_result
                        
                        writer({
                            "type": "tool_result",
                            "agent": agent_type,
                            "tool": current_tool,
                            "result": tool_result,
                            "status": f"å·¥å…·æ‰§è¡Œå®Œæˆ: {current_tool}",
                            "progress": 60
                        })
        
        return {
            "success": True,
            "content": full_response,
            "stats": {
                "total_chunks": chunk_count,
                "tool_calls": tool_calls_count,
                "content_chunks": content_chunks
            }
        }
        
    except Exception as e:
        logger.error(f"Agentæµå¼æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "success": False,
            "error": str(e),
            "content": ""
        }
```

#### 4. æ‰“å­—æœºæ•ˆæœå®ç°

```python
# åœ¨æµå¼å¤„ç†ä¸­å®ç°çœŸæ­£çš„æ‰“å­—æœºæ•ˆæœ
elif mode == "messages":
    token, metadata = data
    if hasattr(token, 'content') and token.content:
        node_name = metadata.get('langgraph_node', '') if metadata else ''
        if node_name in ['agent_execution', 'result_integration', 'supervisor']:
            # ç›´æ¥è¾“å‡ºtokenå†…å®¹ï¼Œä¸æ¢è¡Œï¼Œå®ç°æ‰“å­—æœºæ•ˆæœ
            print(token.content, end='', flush=True)
```

### ğŸ“Š æµå¼è¾“å‡ºç»Ÿè®¡ç¤ºä¾‹

æˆåŠŸçš„æµå¼ç»Ÿè®¡ç¤ºä¾‹ï¼š
- **è®¡ç®—ä»»åŠ¡**ï¼š`æ€»chunk=27, å·¥å…·è°ƒç”¨=1, å†…å®¹chunk=15, å“åº”é•¿åº¦=34`
- **æœç´¢ä»»åŠ¡**ï¼š`æ€»chunk=297, å·¥å…·è°ƒç”¨=1, å†…å®¹chunk=268, å“åº”é•¿åº¦=1883`
- **åˆ†æä»»åŠ¡**ï¼š`æ€»chunk=81, å·¥å…·è°ƒç”¨=1, å†…å®¹chunk=57, å“åº”é•¿åº¦=157`

---

## ğŸ—ï¸ å¤š Agent ç³»ç»Ÿæ„å»ºä¼˜åŒ–

### è®¾è®¡åŸåˆ™

#### 1. åˆ†å±‚æ¶æ„è®¾è®¡

```python
# ç›‘ç£å™¨å±‚ï¼šè´Ÿè´£æ™ºèƒ½è·¯ç”±å’Œä»»åŠ¡åè°ƒ
class SupervisorAgent:
    def analyze_input(self, user_input):
        # åˆ†æç”¨æˆ·æ„å›¾
        # é€‰æ‹©åˆé€‚çš„ä¸“ä¸š Agent
        # è¿”å›è·¯ç”±å†³ç­–
        
# ä¸“ä¸š Agent å±‚ï¼šè´Ÿè´£å…·ä½“ä»»åŠ¡æ‰§è¡Œ
class SpecializedAgent:
    def __init__(self, tools, expertise):
        self.tools = tools
        self.expertise = expertise
    
    def execute_task(self, task):
        # ä½¿ç”¨ä¸“ä¸šå·¥å…·æ‰§è¡Œä»»åŠ¡
        # è¿”å›ä¸“ä¸šç»“æœ
```

#### 2. å·¥å…·é›†æˆç­–ç•¥

```python
# å·¥å…·å®šä¹‰æ ‡å‡†åŒ–
@tool
def calculator(expression: str) -> str:
    """æ‰§è¡Œæ•°å­¦è®¡ç®—"""
    try:
        result = eval(expression)
        return f"ğŸ”¢ è®¡ç®—ç»“æœï¼š{expression} = {result}"
    except Exception as e:
        return f"âŒ è®¡ç®—é”™è¯¯ï¼š{e}"

@tool  
def tavily_search(query: str) -> str:
    """æœç´¢æœ€æ–°ä¿¡æ¯"""
    search = TavilySearchResults(max_results=3)
    results = search.invoke(query)
    # æ ¼å¼åŒ–æœç´¢ç»“æœ
    return formatted_results
```

#### 3. çŠ¶æ€ç®¡ç†ä¼˜åŒ–

```python
class MultiAgentState(TypedDict):
    """å¤šæ™ºèƒ½ä½“çŠ¶æ€å®šä¹‰"""
    messages: Annotated[list, add_messages]
    user_input: str
    current_agent: str
    execution_path: list
    agent_results: dict
    final_result: str
    iteration_count: int
    max_iterations: int
    context: dict
    error_log: list
    supervisor_reasoning: str
    next_action: str
    task_completed: bool
```

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

#### 1. å¹¶å‘å¤„ç†

```python
# å¯¹äºç‹¬ç«‹çš„ä»»åŠ¡ï¼Œå¯ä»¥å¹¶å‘æ‰§è¡Œå¤šä¸ª Agent
async def parallel_agent_execution(tasks):
    results = await asyncio.gather(*[
        agent.execute(task) for agent, task in tasks
    ])
    return results
```

#### 2. ç¼“å­˜æœºåˆ¶

```python
# ç¼“å­˜å¸¸ç”¨çš„è®¡ç®—ç»“æœå’Œæœç´¢ç»“æœ
class AgentCache:
    def __init__(self):
        self.cache = {}
    
    def get_cached_result(self, key):
        return self.cache.get(key)
    
    def cache_result(self, key, result):
        self.cache[key] = result
```

#### 3. é”™è¯¯å¤„ç†å’Œé‡è¯•

```python
class ErrorHandler:
    @staticmethod
    async def handle_agent_error(agent, input_dict, error, max_retries=3):
        """Agent é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
        for attempt in range(max_retries):
            try:
                result = await agent.ainvoke(input_dict)
                return result
            except Exception as e:
                if attempt == max_retries - 1:
                    return f"Agent æ‰§è¡Œå¤±è´¥: {e}"
                await asyncio.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
```

### å¯æ‰©å±•æ€§è®¾è®¡

#### 1. æ’ä»¶åŒ– Agent

```python
class AgentRegistry:
    """Agent æ³¨å†Œè¡¨ï¼Œæ”¯æŒåŠ¨æ€æ·»åŠ æ–°çš„ Agent"""
    
    def __init__(self):
        self.agents = {}
    
    def register_agent(self, name, agent_class, tools):
        self.agents[name] = {
            "class": agent_class,
            "tools": tools,
            "capabilities": agent_class.get_capabilities()
        }
    
    def get_agent(self, name):
        return self.agents.get(name)
```

#### 2. åŠ¨æ€å·¥å…·åŠ è½½

```python
class ToolManager:
    """å·¥å…·ç®¡ç†å™¨ï¼Œæ”¯æŒåŠ¨æ€åŠ è½½å’Œé…ç½®å·¥å…·"""
    
    def __init__(self):
        self.tools = {}
    
    def load_tool(self, tool_name, tool_config):
        # åŠ¨æ€åŠ è½½å·¥å…·
        tool = self.create_tool(tool_name, tool_config)
        self.tools[tool_name] = tool
    
    def get_tools_for_agent(self, agent_type):
        # æ ¹æ® Agent ç±»å‹è¿”å›ç›¸åº”çš„å·¥å…·é›†
        return [tool for tool in self.tools.values() 
                if agent_type in tool.supported_agents]
```

### ç›‘æ§å’Œè°ƒè¯•

#### 1. æ‰§è¡Œè¿½è¸ª

```python
class ExecutionTracker:
    """æ‰§è¡Œè¿½è¸ªå™¨ï¼Œè®°å½• Agent æ‰§è¡Œè¿‡ç¨‹"""
    
    def __init__(self):
        self.execution_log = []
    
    def log_agent_start(self, agent_name, input_data):
        self.execution_log.append({
            "timestamp": time.time(),
            "event": "agent_start",
            "agent": agent_name,
            "input": input_data
        })
    
    def log_agent_complete(self, agent_name, result, duration):
        self.execution_log.append({
            "timestamp": time.time(),
            "event": "agent_complete", 
            "agent": agent_name,
            "result": result,
            "duration": duration
        })
```

#### 2. æ€§èƒ½ç›‘æ§

```python
class PerformanceMonitor:
    """æ€§èƒ½ç›‘æ§å™¨"""
    
    def __init__(self):
        self.metrics = {}
    
    def record_execution_time(self, agent_name, duration):
        if agent_name not in self.metrics:
            self.metrics[agent_name] = []
        self.metrics[agent_name].append(duration)
    
    def get_average_execution_time(self, agent_name):
        times = self.metrics.get(agent_name, [])
        return sum(times) / len(times) if times else 0
```

---

## ğŸ›ï¸ ç³»ç»Ÿæ¶æ„

### æ ¸å¿ƒç»„ä»¶

```python
# 1. å¤šæ™ºèƒ½ä½“çŠ¶æ€ç®¡ç†
class MultiAgentState(TypedDict):
    messages: Annotated[list, add_messages]
    user_input: str
    current_agent: str
    execution_path: list
    agent_results: dict
    final_result: str
    iteration_count: int
    max_iterations: int
    context: dict
    error_log: list
    supervisor_reasoning: str
    next_action: str
    task_completed: bool

# 2. Agent åˆ›å»ºå·¥å‚
def create_agents():
    """åˆ›å»ºæ‰€æœ‰ä¸“ä¸šåŒ– Agent"""
    return {
        "analysis": create_react_agent(
            llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
            tools=[calculator],
            state_modifier="You are an analysis expert..."
        ),
        "search": create_react_agent(
            llm=ChatOpenAI(model="gpt-4o-mini", temperature=0),
            tools=[tavily_search],
            state_modifier="You are a search expert..."
        )
    }

# 3. å·¥ä½œæµå›¾æ„å»º
def create_multi_agent_graph():
    """åˆ›å»ºå¤šæ™ºèƒ½ä½“å·¥ä½œæµå›¾"""
    workflow = StateGraph(MultiAgentState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("supervisor", supervisor_node)
    workflow.add_node("agent_execution", agent_execution_node)
    workflow.add_node("result_integration", result_integration_node)

    # æ·»åŠ è¾¹å’Œæ¡ä»¶è·¯ç”±
    workflow.add_edge(START, "supervisor")
    workflow.add_conditional_edges(
        "supervisor",
        route_to_agent,
        {
            "analysis": "agent_execution",
            "search": "agent_execution",
            "finish": "result_integration"
        }
    )
    workflow.add_edge("agent_execution", "supervisor")
    workflow.add_edge("result_integration", END)

    # ç¼–è¯‘å›¾
    return workflow.compile(checkpointer=MemorySaver())
```

### Agent èŠ‚ç‚¹å®ç°

```python
async def supervisor_node(state: MultiAgentState) -> MultiAgentState:
    """ç›‘ç£å™¨èŠ‚ç‚¹ï¼šæ™ºèƒ½è·¯ç”±å’Œä»»åŠ¡åè°ƒ"""

    # æ„å»ºç›‘ç£å™¨æç¤º
    system_message = """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½ä»»åŠ¡è°ƒåº¦å™¨ã€‚åˆ†æç”¨æˆ·è¾“å…¥ï¼Œå†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š

    å¯é€‰è¡ŒåŠ¨ï¼š
    - analysis: æ•°æ®åˆ†æã€è®¡ç®—ã€é€»è¾‘æ¨ç†
    - search: ä¿¡æ¯æœç´¢ã€æœ€æ–°èµ„è®¯æŸ¥è¯¢
    - finish: ä»»åŠ¡å®Œæˆï¼Œå‡†å¤‡è¾“å‡ºç»“æœ

    è¿”å›æ ¼å¼ï¼šåªè¿”å›è¡ŒåŠ¨åç§°ï¼Œå¦‚ 'analysis' æˆ– 'search' æˆ– 'finish'
    """

    # LLM å†³ç­–
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    messages = [SystemMessage(content=system_message)] + state["messages"]
    response = await llm.ainvoke(messages)

    # è§£æå†³ç­–
    decision = response.content.strip().lower()

    return {
        **state,
        "next_action": decision,
        "supervisor_reasoning": f"æ£€æµ‹åˆ°{decision}éœ€æ±‚"
    }

async def agent_execution_node(state: MultiAgentState) -> MultiAgentState:
    """Agent æ‰§è¡ŒèŠ‚ç‚¹ï¼šæ‰§è¡Œå…·ä½“ä»»åŠ¡"""

    next_action = state.get("next_action")
    agents = create_agents()
    agent = agents.get(next_action)

    if not agent:
        return {**state, "error_log": [f"æœªæ‰¾åˆ°Agent: {next_action}"]}

    # å‡†å¤‡ Agent è¾“å…¥
    agent_input = {"messages": [HumanMessage(content=state["user_input"])]}

    # æ‰§è¡Œ Agentï¼ˆå¸¦æµå¼è¾“å‡ºï¼‰
    full_response = ""
    try:
        async for chunk in agent.astream(agent_input, stream_mode="messages"):
            if isinstance(chunk, tuple) and len(chunk) == 2:
                message, metadata = chunk
                msg_type = type(message).__name__

                if msg_type == "AIMessageChunk" and hasattr(message, 'content') and message.content:
                    full_response += str(message.content)
                elif msg_type == "ToolMessage" and hasattr(message, 'content') and message.content:
                    full_response += str(message.content)

    except Exception as e:
        logger.error(f"Agentæ‰§è¡Œå¤±è´¥: {e}")
        full_response = "Agentæ‰§è¡Œå®Œæˆ"

    # æ›´æ–°çŠ¶æ€
    result_text = full_response.strip() if full_response.strip() else "Agentæ‰§è¡Œå®Œæˆ"

    return {
        **state,
        "current_agent": next_action,
        "execution_path": state["execution_path"] + [next_action],
        "agent_results": {**state["agent_results"], next_action: result_text},
        "iteration_count": state["iteration_count"] + 1
    }

async def result_integration_node(state: MultiAgentState) -> MultiAgentState:
    """ç»“æœæ•´åˆèŠ‚ç‚¹ï¼šæ•´åˆæ‰€æœ‰ Agent ç»“æœ"""

    # æ„å»ºæ•´åˆæç¤º
    agent_results = state.get("agent_results", {})
    results_summary = "\n".join([
        f"- {agent}: {result}"
        for agent, result in agent_results.items()
    ])

    system_message = f"""åŸºäºä»¥ä¸‹Agentæ‰§è¡Œç»“æœï¼Œç”Ÿæˆæœ€ç»ˆæ•´åˆç­”æ¡ˆï¼š

{results_summary}

è¦æ±‚ï¼š
1. æ•´åˆæ‰€æœ‰ç›¸å…³ä¿¡æ¯
2. æä¾›æ¸…æ™°ã€å‡†ç¡®çš„æœ€ç»ˆç­”æ¡ˆ
3. ä¿æŒä¸“ä¸šå’Œå‹å¥½çš„è¯­è°ƒ
"""

    # LLM æ•´åˆ
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
    messages = [
        SystemMessage(content=system_message),
        HumanMessage(content=state["user_input"])
    ]

    response = await llm.ainvoke(messages)

    return {
        **state,
        "final_result": response.content,
        "task_completed": True
    }
```

---

## ğŸš€ å®‰è£…å’Œä½¿ç”¨

### ç¯å¢ƒè¦æ±‚

- Python 3.8+
- LangChain >= 0.1.0
- LangGraph >= 0.1.0
- OpenAI API Key
- Tavily API Key

### å®‰è£…ä¾èµ–

```bash
pip install langchain langgraph openai tavily-python
```

### é…ç½®

```python
import os

# è®¾ç½® API Keys
os.environ["OPENAI_API_KEY"] = "your-openai-api-key"
os.environ["TAVILY_API_KEY"] = "your-tavily-api-key"

# æˆ–ä½¿ç”¨é…ç½®æ–‡ä»¶
from config import OPENAI_API_KEY, TAVILY_API_KEY
```

### åŸºæœ¬ä½¿ç”¨

```python
import asyncio
from graph import create_multi_agent_graph

async def main():
    # åˆ›å»ºåº”ç”¨
    app = create_multi_agent_graph()

    # å‡†å¤‡è¾“å…¥
    initial_state = {
        "messages": [],
        "user_input": "æœç´¢Pythonæœ€æ–°ç‰¹æ€§å¹¶åˆ†æå…¶ä¼˜åŠ¿",
        "current_agent": "",
        "execution_path": [],
        "agent_results": {},
        "final_result": "",
        "iteration_count": 0,
        "max_iterations": 3,
        "context": {},
        "error_log": [],
        "supervisor_reasoning": "",
        "next_action": "",
        "task_completed": False
    }

    # é…ç½®
    config = {"configurable": {"thread_id": "example_session"}}

    # æ‰§è¡Œ
    result = await app.ainvoke(initial_state, config=config)

    print(f"âœ… æœ€ç»ˆç»“æœ: {result['final_result']}")
    print(f"ğŸ“Š æ‰§è¡Œè·¯å¾„: {' â†’ '.join(result['execution_path'])}")
    print(f"ğŸ”„ è¿­ä»£æ¬¡æ•°: {result['iteration_count']}")

# è¿è¡Œ
asyncio.run(main())
```

### æµå¼è¾“å‡ºä½¿ç”¨

```python
import asyncio
from graph import create_multi_agent_graph

async def stream_example():
    app = create_multi_agent_graph()

    initial_state = {
        "messages": [],
        "user_input": "è®¡ç®— 25 * 4 çš„ç»“æœ",
        "current_agent": "",
        "execution_path": [],
        "agent_results": {},
        "final_result": "",
        "iteration_count": 0,
        "max_iterations": 3,
        "context": {},
        "error_log": [],
        "supervisor_reasoning": "",
        "next_action": "",
        "task_completed": False
    }

    config = {"configurable": {"thread_id": "stream_session"}}

    print("ğŸ–¨ï¸ æµå¼è¾“å‡ºå¼€å§‹:")

    async for chunk in app.astream(initial_state, config=config, stream_mode=["messages"]):
        if isinstance(chunk, tuple) and len(chunk) == 2:
            mode, data = chunk

            if mode == "messages":
                token, metadata = data
                if hasattr(token, 'content') and token.content:
                    node_name = metadata.get('langgraph_node', '') if metadata else ''
                    if node_name in ['agent_execution', 'result_integration', 'supervisor']:
                        # æ‰“å­—æœºæ•ˆæœï¼šç›´æ¥è¾“å‡ºï¼Œä¸æ¢è¡Œ
                        print(token.content, end='', flush=True)

    print("\n\nâœ… æµå¼è¾“å‡ºå®Œæˆ")

# è¿è¡Œ
asyncio.run(stream_example())
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. è®¾è®¡åŸåˆ™

- **åˆ†ç¦»å…³æ³¨ç‚¹**ï¼šæµå¼å¤„ç†ä¸ä¸šåŠ¡é€»è¾‘åˆ†ç¦»
- **é”™è¯¯å®¹é”™**ï¼šå§‹ç»ˆæä¾›å›é€€æœºåˆ¶
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†ç¼“å†²å’Œæ‰¹é‡å¤„ç†
- **ç”¨æˆ·ä½“éªŒ**ï¼šæä¾›æ¸…æ™°çš„è¿›åº¦åé¦ˆ

### 2. å®æ–½æ­¥éª¤

1. **åˆ†æ Agent è¾“å‡ºç»“æ„** â†’ ç†è§£æ‰€æœ‰å¯èƒ½çš„æ¶ˆæ¯ç±»å‹
2. **è®¾è®¡æµå¼å¤„ç†å™¨** â†’ é’ˆå¯¹ä¸åŒæ¶ˆæ¯ç±»å‹è®¾è®¡å¤„ç†é€»è¾‘
3. **å®ç°é…ç½®éš”ç¦»** â†’ é¿å… LangGraph config å†²çª
4. **æ·»åŠ é”™è¯¯å¤„ç†** â†’ å®ç°ä¼˜é›…çš„å›é€€æœºåˆ¶
5. **ä¼˜åŒ–æ€§èƒ½** â†’ æ‰¹é‡å¤„ç†å’Œç¼“å†²ç­–ç•¥
6. **å…¨é¢æµ‹è¯•** â†’ è¦†ç›–æ­£å¸¸å’Œå¼‚å¸¸æƒ…å†µ

### 3. å…³é”®è¦ç‚¹

- âœ… **ç§»é™¤ config å‚æ•°**ï¼šAgent.astream() ä¸è¦ä¼ é€’ LangGraph çš„ config
- âœ… **åˆ†ç±»å¤„ç†æ¶ˆæ¯**ï¼šAIMessageChunkã€ToolMessage éœ€è¦ä¸åŒå¤„ç†é€»è¾‘
- âœ… **æ ‡å‡†åŒ–æ¶ˆæ¯æ ¼å¼**ï¼šç»Ÿä¸€çš„æµå¼æ¶ˆæ¯ç»“æ„
- âœ… **æ€§èƒ½ä¼˜åŒ–**ï¼šåˆç†çš„ç¼“å†²å’Œæ‰¹é‡å‘é€ç­–ç•¥
- âœ… **æ‰“å­—æœºæ•ˆæœ**ï¼šä½¿ç”¨ `print(content, end='', flush=True)` å®ç°

### 4. å¸¸è§é—®é¢˜è§£å†³

#### é—®é¢˜1ï¼šæµå¼è¾“å‡ºä¸ºç©º
```python
# åŸå› ï¼šconfig å‚æ•°å†²çª
# è§£å†³ï¼šç§»é™¤ config å‚æ•°
async for chunk in agent.astream(input_dict, stream_mode="messages"):  # âœ… æ­£ç¡®
```

#### é—®é¢˜2ï¼šæ‰“å­—æœºæ•ˆæœä¸è¿ç»­
```python
# åŸå› ï¼šé¢å¤–çš„æ¢è¡Œ
# è§£å†³ï¼šåªåœ¨é messages æ¨¡å¼æ—¶æ¢è¡Œ
if not (isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == "messages"):
    print()  # åªåœ¨å¿…è¦æ—¶æ¢è¡Œ
```

#### é—®é¢˜3ï¼šèŠ‚ç‚¹åç§°è¿‡æ»¤é”™è¯¯
```python
# åŸå› ï¼šèŠ‚ç‚¹åç§°ä¸åŒ¹é…
# è§£å†³ï¼šæ£€æŸ¥å®é™…çš„èŠ‚ç‚¹åç§°
node_name = metadata.get('langgraph_node', '') if metadata else ''
if node_name in ['agent_execution', 'result_integration', 'supervisor']:  # âœ… æ­£ç¡®
```

---

## ğŸ“ é¡¹ç›®ç»“æ„

```
Multi-Agent-report/
â”œâ”€â”€ README.md              # é¡¹ç›®æ–‡æ¡£
â”œâ”€â”€ README_COMPLETE.md     # å®Œæ•´æŠ€æœ¯æŒ‡å—
â”œâ”€â”€ graph.py              # å¤šæ™ºèƒ½ä½“å›¾å®šä¹‰
â”œâ”€â”€ tools.py              # å·¥å…·å®šä¹‰
â”œâ”€â”€ config.py             # é…ç½®æ–‡ä»¶
â””â”€â”€ test.py              # æµ‹è¯•è„šæœ¬ï¼ˆå«æ‰“å­—æœºæ•ˆæœï¼‰
```

---

## ğŸ‰ æ€»ç»“

è¿™å¥—å¤š Agent æµå¼è¾“å‡ºç³»ç»Ÿå®ç°äº†ï¼š

1. **å®Œæ•´çš„æµå¼ä½“éªŒ**ï¼šä»å·¥å…·è°ƒç”¨åˆ°æœ€ç»ˆå›å¤çš„å…¨ç¨‹æµå¼è¾“å‡º
2. **çœŸæ­£çš„æ‰“å­—æœºæ•ˆæœ**ï¼šToken çº§åˆ«çš„å®æ—¶æ˜¾ç¤º
3. **æ™ºèƒ½åä½œ**ï¼šå¤šä¸ªä¸“ä¸šåŒ– Agent çš„æ— ç¼åä½œ
4. **é«˜æ€§èƒ½æ¶æ„**ï¼šå¼‚æ­¥å¤„ç†ã€é”™è¯¯å®¹é”™ã€çŠ¶æ€ç®¡ç†
5. **æ˜“äºæ‰©å±•**ï¼šæ’ä»¶åŒ–è®¾è®¡ï¼Œæ”¯æŒåŠ¨æ€æ·»åŠ æ–°çš„ Agent å’Œå·¥å…·

é€šè¿‡è¿™å¥—æ–¹æ¡ˆï¼Œæ‚¨å¯ä»¥æ„å»ºå‡ºæ—¢æ™ºèƒ½åˆå…·æœ‰å‡ºè‰²ç”¨æˆ·ä½“éªŒçš„å¤š Agent ç³»ç»Ÿï¼ğŸš€
