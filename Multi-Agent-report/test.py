"""
æµ‹è¯•æµå¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ
"""

import asyncio
import time
from graph import create_multi_agent_graph
from langgraph.checkpoint.memory import InMemorySaver
from langchain_core.messages import HumanMessage

async def test_streaming_multiagent():
    """æµ‹è¯•æµå¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ"""
    print("ğŸš€ æµ‹è¯•æµå¼å¤šæ™ºèƒ½ä½“ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºå›¾
    checkpointer = InMemorySaver()
    app = create_multi_agent_graph(checkpointer)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "è®¡ç®— 25 * 4 çš„ç»“æœ",
        "æœç´¢Pythonçš„ä¼˜åŠ¿",
        "åˆ†æè¿™å¥è¯çš„æƒ…æ„Ÿï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæˆ‘å¾ˆå¼€å¿ƒï¼"
    ]
    
    for i, user_input in enumerate(test_cases, 1):
        print(f"\nğŸ§ª æµ‹è¯•æ¡ˆä¾‹ {i}: {user_input}")
        print("-" * 50)
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state = {
            "messages": [HumanMessage(content=user_input)],
            "user_input": user_input,
            "current_agent": "",
            "execution_path": [],
            "agent_results": {},
            "final_result": "",
            "quality_score": 0.0,
            "iteration_count": 0,
            "max_iterations": 3,
            "context": {},
            "error_log": [],
            "supervisor_reasoning": "",
            "next_action": "",
            "task_completed": False
        }
        
        config = {"configurable": {"thread_id": f"test_{i}_{int(time.time())}"}} 
        start_time = time.time()
        step_count = 0

        try:
            # æµå¼æ‰§è¡Œ - ä½¿ç”¨messagesæ¨¡å¼è·å¾—tokençº§æµå¼è¾“å‡º
            async for chunk in app.astream(initial_state, config=config, stream_mode=["updates", "messages", "custom"]):
                step_count += 1
                current_time = time.time() - start_time
                # å¤„ç†ä¸åŒçš„æµå¼æ¨¡å¼
                if isinstance(chunk, tuple) and len(chunk) == 2:
                    # å¤šæ¨¡å¼æµå¼è¾“å‡ºï¼š(mode, data)
                    mode, data = chunk

                    if mode == "updates":
                        # åªæ˜¾ç¤ºé‡è¦çš„èŠ‚ç‚¹æ›´æ–°
                        for node_name, node_data in data.items():
                            if isinstance(node_data, dict):
                                current_agent = node_data.get('current_agent', '')
                                next_action = node_data.get('next_action', '')
                                final_result = node_data.get('final_result', '')

                                if current_agent:
                                    print(f"  ğŸ¤– å½“å‰Agent: {current_agent}")
                                if next_action:
                                    print(f"  â¡ï¸ ä¸‹ä¸€æ­¥: {next_action}")
                                if final_result:
                                    print(f"  ğŸ¯ æœ€ç»ˆç»“æœ: {final_result[:100]}{'...' if len(final_result) > 100 else ''}")

                    elif mode == "messages":
                        # å¤„ç†LLM tokenæµå¼è¾“å‡º - æ‰“å­—æœºæ•ˆæœ
                        token, metadata = data
                        if hasattr(token, 'content') and token.content:
                            # æ˜¾ç¤ºæ‰€æœ‰ Agent ç›¸å…³èŠ‚ç‚¹çš„æ‰“å­—æœºæ•ˆæœ
                            node_name = metadata.get('langgraph_node', '') if metadata else ''
                            if node_name in ['agent_execution', 'result_integration', 'supervisor']:
                                # ç›´æ¥è¾“å‡ºtokenå†…å®¹ï¼Œä¸æ¢è¡Œï¼Œå®ç°æ‰“å­—æœºæ•ˆæœ
                                print(token.content, end='', flush=True)

                    elif mode == "custom":
                        # å¤„ç†è‡ªå®šä¹‰æµå¼è¾“å‡ºï¼ˆè¿›åº¦æ›´æ–°ç­‰ï¼‰
                        if isinstance(data, dict) and data.get("step"):
                            status = data.get("status", "")
                            progress = data.get("progress", 0)
                            if status:
                                print(f"  ğŸ“Š {status} (è¿›åº¦: {progress}%)")

                # åªåœ¨é messages æ¨¡å¼æ—¶æ¢è¡Œ
                if not (isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == "messages"):
                    print()

                # å¦‚æœè¶…è¿‡20æ­¥ï¼Œåœæ­¢ï¼ˆé˜²æ­¢æ— é™å¾ªç¯ï¼‰
                if step_count > 2000:
                    print("âš ï¸ è¾¾åˆ°æœ€å¤§æ­¥æ•°é™åˆ¶ï¼Œåœæ­¢æ‰§è¡Œ")
                    break

            total_time = time.time() - start_time
            print(f"âœ… æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’ï¼Œå…±{step_count}æ­¥")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
        
        if i < len(test_cases):
            print("\n" + "=" * 60)
            await asyncio.sleep(1)  # çŸ­æš‚æš‚åœ
    
    print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    asyncio.run(test_streaming_multiagent())
