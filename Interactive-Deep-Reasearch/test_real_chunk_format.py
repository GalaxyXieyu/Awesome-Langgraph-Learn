#!/usr/bin/env python3
"""
æµ‹è¯•çœŸå®çš„chunkæ ¼å¼ï¼Œæ¨¡æ‹Ÿç”¨æˆ·æåˆ°çš„å®é™…æ•°æ®
"""

import time
from stream_writer import create_workflow_processor


class MockAIMessageChunk:
    """æ¨¡æ‹ŸçœŸå®çš„AIMessageChunkå¯¹è±¡"""
    
    def __init__(self, content: str):
        self.content = content
        self.additional_kwargs = {}
        self.response_metadata = {}
        self.id = f'run--{hash(content)}'
        self.usage_metadata = {
            'input_tokens': 0, 
            'output_tokens': len(content.split()), 
            'total_tokens': len(content.split()),
            'input_token_details': {}, 
            'output_token_details': {}
        }
        self.__class__.__name__ = "AIMessageChunk"
    
    def __repr__(self):
        return f"AIMessageChunk(content='{self.content}', additional_kwargs={{}}, response_metadata={{}}, id='{self.id}', usage_metadata={self.usage_metadata})"


class OutputCapture:
    """æ•è·writerè¾“å‡º"""
    
    def __init__(self):
        self.messages = []
    
    def __call__(self, message):
        """æ¨¡æ‹Ÿstream writer"""
        self.messages.append(message)
        # åªæ˜¾ç¤ºcontent_streamingæ¶ˆæ¯
        if message.get('message_type') == 'content_streaming':
            content = message.get('content', '')
            print(f"âœ¨ æµå¼è¾“å‡º: '{content}'")


def test_real_chunk_format():
    """æµ‹è¯•çœŸå®çš„chunkæ ¼å¼"""
    print("ğŸ§ª æµ‹è¯•çœŸå®chunkæ ¼å¼å¤„ç†")
    print("=" * 50)
    
    # åˆ›å»ºprocessorå¹¶æ•è·è¾“å‡º
    processor = create_workflow_processor("intelligent_research", "æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")
    
    output_capture = OutputCapture()
    processor.writer.writer = output_capture
    
    # æ¨¡æ‹Ÿç”¨æˆ·æåˆ°çš„çœŸå®æ•°æ®æ ¼å¼
    real_chunks = [
        # 1. ç”¨æˆ·æåˆ°çš„æ ¼å¼
        (
            ('content_creation:9fe31c96-a020-55e4-fdb6-1c5f31ba8aa9',), 
            'messages', 
            (
                MockAIMessageChunk(' "'), 
                {
                    'thread_id': 'research_1754791851', 
                    'langgraph_step': 1, 
                    'langgraph_node': 'intelligent_supervisor',
                    'langgraph_triggers': ('branch:to:intelligent_supervisor',), 
                    'langgraph_path': ('__pregel_pull', 'intelligent_supervisor'), 
                    'langgraph_checkpoint_ns': 'content_creation:9fe31c96-a020-55e4-fdb6-1c5f31ba8aa9|intelligent_supervisor:a73540ef-1e9e-7dad-66a9-107b496859e4', 
                    'checkpoint_ns': 'content_creation:9fe31c96-a020-55e4-fdb6-1c5f31ba8aa9', 
                    'ls_provider': 'openai', 
                    'ls_model_name': 'gpt-4o-mini', 
                    'ls_model_type': 'chat', 
                    'ls_temperature': 0.7
                }
            )
        ),
        
        # 2. æ›´å¤šçš„æµå¼å†…å®¹chunk
        (
            ('content_creation:9fe31c96-a020-55e4-fdb6-1c5f31ba8aa9',), 
            'messages',
            (MockAIMessageChunk('äººå·¥æ™ºèƒ½'), {'thread_id': 'research_1754791851'})
        ),
        
        (
            ('writing:31a0ffdd-f5f2-a583-c34b-3d03e3691dba',), 
            'messages',
            (MockAIMessageChunk('æŠ€æœ¯å‘å±•'), {'node': 'writer'})
        ),
        
        (
            ('writing:31a0ffdd-f5f2-a583-c34b-3d03e3691dba',), 
            'messages',
            (MockAIMessageChunk('æ­£åœ¨å¿«é€Ÿæ¼”è¿›'), {'node': 'writer'})
        ),
        
        # 3. ä¸€äº›customæ ¼å¼æ¶ˆæ¯ï¼ˆç”¨æˆ·è¯´è¿™äº›èƒ½æ­£å¸¸å·¥ä½œï¼‰
        ('custom', {
            'message_type': 'reasoning', 
            'content': 'æ™ºèƒ½è°ƒåº¦åˆ†æå®Œæˆï¼šå¼€å§‹ç ”ç©¶äººå·¥æ™ºèƒ½æŠ€æœ¯åŸºç¡€', 
            'node': 'intelligent_research',
            'timestamp': time.time(), 
            'duration': 1.5
        })
    ]
    
    print(f"ğŸ“Š å¤„ç† {len(real_chunks)} ä¸ªçœŸå®æ ¼å¼çš„chunk...")
    
    for i, chunk in enumerate(real_chunks, 1):
        print(f"\nğŸ” === Chunk #{i} ===")
        
        if isinstance(chunk, tuple) and len(chunk) == 2 and chunk[0] == 'custom':
            print(f"ğŸ“‹ Customæ¶ˆæ¯: {chunk[1].get('message_type', 'unknown')}")
            print(f"ğŸ’¬ å†…å®¹: '{chunk[1].get('content', '')[:50]}...'")
        elif isinstance(chunk, tuple) and len(chunk) == 3:
            subgraph_id, chunk_type, chunk_data = chunk
            if isinstance(chunk_data, tuple) and len(chunk_data) == 2:
                message, metadata = chunk_data
                print(f"ğŸ“‹ å­å›¾æ¶ˆæ¯:")
                print(f"   - å­å›¾ID: {subgraph_id}")
                print(f"   - ç±»å‹: {chunk_type}")
                print(f"   - æ¶ˆæ¯ç±»å‹: {type(message).__name__}")
                print(f"   - å†…å®¹: '{message.content}'")
        
        # å¤„ç†chunk
        result = processor.process_chunk(chunk)
    
    print(f"\nğŸ“ˆ æ€»è¾“å‡ºæ¶ˆæ¯æ•°: {len(output_capture.messages)}")
    
    # ç»Ÿè®¡æ¶ˆæ¯ç±»å‹
    msg_types = {}
    for msg in output_capture.messages:
        msg_type = msg.get('message_type', 'unknown')
        msg_types[msg_type] = msg_types.get(msg_type, 0) + 1
    
    print(f"\nğŸ“Š æ¶ˆæ¯ç±»å‹ç»Ÿè®¡:")
    for msg_type, count in msg_types.items():
        print(f"   - {msg_type}: {count}æ¡")
    
    # æ˜¾ç¤ºæ‰€æœ‰content_streamingæ¶ˆæ¯
    content_msgs = [msg for msg in output_capture.messages if msg.get('message_type') == 'content_streaming']
    print(f"\nğŸ“ æµå¼å†…å®¹æ¶ˆæ¯ ({len(content_msgs)}æ¡):")
    for i, msg in enumerate(content_msgs, 1):
        content = msg.get('content', '')
        print(f"   {i}. '{content}'")
    
    print("\nâœ… çœŸå®æ ¼å¼æµ‹è¯•å®Œæˆï¼")
    print("ğŸ‰ ç°åœ¨æ‰€æœ‰AIMessageChunkéƒ½èƒ½æ­£ç¡®è½¬æ¢ä¸ºcontent_streamingæ ¼å¼è¾“å‡ºï¼")


if __name__ == "__main__":
    test_real_chunk_format()