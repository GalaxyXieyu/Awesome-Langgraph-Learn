"""
å·¥å…·æ¨¡å—
ä¸ºæ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿæä¾›å„ç±»ä¸“ä¸šå·¥å…·
åŒ…å«æœç´¢ã€åˆ†æã€å†™ä½œå’ŒéªŒè¯ç­‰åŠŸèƒ½
"""

import os
import json
import uuid
import time
import statistics
from typing import List, Dict, Any, Optional
from datetime import datetime
from langchain_core.tools import tool
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_core.pydantic_v1 import BaseModel, Field
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# æœç´¢å·¥å…·
# ============================================================================

@tool
def advanced_web_search(query: str, max_results: int = 5, search_depth: str = "advanced") -> List[Dict[str, Any]]:
    """
    é«˜çº§ç½‘ç»œæœç´¢å·¥å…·
    ä½¿ç”¨Tavilyè¿›è¡Œæ·±åº¦ç½‘ç»œæœç´¢ï¼Œæ”¯æŒå¤šç§æœç´¢æ¨¡å¼
    
    Args:
        query: æœç´¢æŸ¥è¯¢è¯
        max_results: æœ€å¤§è¿”å›ç»“æœæ•°
        search_depth: æœç´¢æ·±åº¦ (basic/advanced)
    
    Returns:
        æ ¼å¼åŒ–çš„æœç´¢ç»“æœåˆ—è¡¨
    """
    try:
        search_tool = TavilySearchResults(
            tavily_api_key="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",
            max_results=max_results,
            search_depth=search_depth,
            include_answer=True,
            include_raw_content=False,
            include_images=False
        )
        
        results = search_tool.invoke({"query": query})
        
        formatted_results = []
        for result in results:
            formatted_result = {
                "id": str(uuid.uuid4()),
                "query": query,
                "source_type": "web",
                "title": result.get("title", ""),
                "content": result.get("content", "")[:500],  # é™åˆ¶å†…å®¹é•¿åº¦
                "url": result.get("url", ""),
                "credibility_score": 0.8,  # é»˜è®¤å¯ä¿¡åº¦
                "relevance_score": result.get("score", 0.8),
                "timestamp": time.time()
            }
            formatted_results.append(formatted_result)
        
        logger.info(f"ç½‘ç»œæœç´¢å®Œæˆ: {query}, è·å¾— {len(formatted_results)} ä¸ªç»“æœ")
        return formatted_results
        
    except Exception as e:
        logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {str(e)}")
        return [{
            "id": str(uuid.uuid4()),
            "error": f"æœç´¢å¤±è´¥: {str(e)}",
            "query": query,
            "timestamp": time.time()
        }]

@tool
def multi_source_research(topic: str, research_queries: List[str], max_results_per_query: int = 3) -> List[Dict[str, Any]]:
    """
    å¤šæºç ”ç©¶å·¥å…·
    é’ˆå¯¹ä¸»é¢˜è¿›è¡Œå¤šè§’åº¦ã€å¤šå…³é”®è¯çš„æ·±åº¦ç ”ç©¶
    
    Args:
        topic: ç ”ç©¶ä¸»é¢˜
        research_queries: ç ”ç©¶æŸ¥è¯¢åˆ—è¡¨
        max_results_per_query: æ¯ä¸ªæŸ¥è¯¢çš„æœ€å¤§ç»“æœæ•°
    
    Returns:
        ç»¼åˆç ”ç©¶ç»“æœ
    """
    try:
        all_results = []
        
        for query in research_queries:
            # ç»„åˆä¸»é¢˜å’ŒæŸ¥è¯¢æ„å»ºå®Œæ•´æœç´¢è¯
            full_query = f"{topic} {query}"
            search_results = advanced_web_search.invoke({
                "query": full_query,
                "max_results": max_results_per_query,
                "search_depth": "advanced"
            })
            
            # ä¸ºæ¯ä¸ªç»“æœæ·»åŠ æŸ¥è¯¢æ ‡è¯†
            for result in search_results:
                if not result.get("error"):
                    result["original_query"] = query
                    result["research_topic"] = topic
                    all_results.append(result)
        
        # å»é‡å¤„ç†ï¼ˆåŸºäºURLï¼‰
        unique_results = []
        seen_urls = set()
        for result in all_results:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
            elif not url:  # å¤„ç†æ²¡æœ‰URLçš„ç»“æœ
                unique_results.append(result)
        
        # æŒ‰ç›¸å…³æ€§å¾—åˆ†æ’åº
        unique_results.sort(key=lambda x: x.get("relevance_score", 0), reverse=True)
        
        logger.info(f"å¤šæºç ”ç©¶å®Œæˆ: {topic}, æŸ¥è¯¢æ•°: {len(research_queries)}, è·å¾— {len(unique_results)} ä¸ªç‹¬ç‰¹ç»“æœ")
        return unique_results
        
    except Exception as e:
        logger.error(f"å¤šæºç ”ç©¶å¤±è´¥: {str(e)}")
        return [{"error": f"å¤šæºç ”ç©¶å¤±è´¥: {str(e)}", "topic": topic}]

# ============================================================================
# åˆ†æå·¥å…·
# ============================================================================

@tool
def content_analyzer(text: str, analysis_type: str = "comprehensive") -> Dict[str, Any]:
    """
    å†…å®¹åˆ†æå·¥å…·
    å¯¹æ–‡æœ¬è¿›è¡Œæ·±åº¦åˆ†æï¼Œæå–å…³é”®ä¿¡æ¯å’Œæ´å¯Ÿ
    
    Args:
        text: å¾…åˆ†ææ–‡æœ¬
        analysis_type: åˆ†æç±»å‹ (basic/sentiment/keywords/comprehensive)
        
    Returns:
        åˆ†æç»“æœå­—å…¸
    """
    try:
        if not text or len(text.strip()) < 10:
            return {"error": "æ–‡æœ¬å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ†æ"}
        
        # åŸºç¡€ç»Ÿè®¡
        word_count = len(text.replace(" ", ""))
        sentence_count = len([s for s in text.split("ã€‚") if s.strip()])
        paragraph_count = len([p for p in text.split("\n\n") if p.strip()])
        
        # å…³é”®è¯æå–
        stop_words = {"çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "äº†", "ç€", "è¿‡", "ä¹Ÿ", "éƒ½", "å°†", "ä¸º", "å› ä¸º", "ç”±äº", "å¦‚æœ", "è™½ç„¶", "ç„¶è€Œ", "å› æ­¤"}
        words = [w for w in text if w not in stop_words and len(w) > 1]
        word_freq = {}
        for word in words:
            word_freq[word] = word_freq.get(word, 0) + 1
        
        top_keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:10]
        
        # æƒ…æ„Ÿåˆ†æï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        positive_words = ["å¥½", "ä¼˜ç§€", "æˆåŠŸ", "å¢é•¿", "æå‡", "æ”¹å–„", "åˆ›æ–°", "å‘å±•", "æœºä¼š", "ä¼˜åŠ¿"]
        negative_words = ["å·®", "å¤±è´¥", "ä¸‹é™", "é—®é¢˜", "å›°éš¾", "æŒ‘æˆ˜", "é£é™©", "å¨èƒ", "ç¼ºé™·", "åŠ£åŠ¿"]
        
        positive_count = sum(1 for word in positive_words if word in text)
        negative_count = sum(1 for word in negative_words if word in text)
        
        if positive_count > negative_count:
            sentiment = "ç§¯æ"
            sentiment_score = min((positive_count - negative_count) / word_count * 100, 100)
        elif negative_count > positive_count:
            sentiment = "æ¶ˆæ"
            sentiment_score = min((negative_count - positive_count) / word_count * 100, 100)
        else:
            sentiment = "ä¸­æ€§"
            sentiment_score = 0
        
        # å¯è¯»æ€§è¯„ä¼°
        avg_sentence_length = word_count / max(sentence_count, 1)
        readability_score = max(0, min(100, 100 - (avg_sentence_length - 15) * 2))
        
        # ä¸»é¢˜è¯†åˆ«ï¼ˆåŸºäºå…³é”®è¯ï¼‰
        tech_keywords = ["æŠ€æœ¯", "ç®—æ³•", "ç³»ç»Ÿ", "å¹³å°", "æ•°æ®", "æ™ºèƒ½", "è‡ªåŠ¨", "æ•°å­—åŒ–"]
        business_keywords = ["å¸‚åœº", "å•†ä¸š", "ä¼ä¸š", "ç»æµ", "æŠ•èµ„", "æ”¶ç›Š", "æˆæœ¬", "ç«äº‰"]
        social_keywords = ["ç¤¾ä¼š", "äººç¾¤", "ç”¨æˆ·", "å…¬ä¼—", "ç¤¾åŒº", "æ–‡åŒ–", "æ•™è‚²", "å¥åº·"]
        
        tech_score = sum(1 for kw in tech_keywords if kw in text)
        business_score = sum(1 for kw in business_keywords if kw in text)
        social_score = sum(1 for kw in social_keywords if kw in text)
        
        primary_theme = "é€šç”¨"
        if tech_score > business_score and tech_score > social_score:
            primary_theme = "æŠ€æœ¯"
        elif business_score > social_score:
            primary_theme = "å•†ä¸š"
        elif social_score > 0:
            primary_theme = "ç¤¾ä¼š"
        
        analysis_result = {
            "id": str(uuid.uuid4()),
            "analysis_type": analysis_type,
            "timestamp": time.time(),
            "basic_stats": {
                "word_count": word_count,
                "sentence_count": sentence_count,
                "paragraph_count": paragraph_count,
                "avg_sentence_length": round(avg_sentence_length, 1)
            },
            "keywords": [{"word": word, "frequency": freq} for word, freq in top_keywords],
            "sentiment": {
                "overall": sentiment,
                "score": round(sentiment_score, 1),
                "positive_indicators": positive_count,
                "negative_indicators": negative_count
            },
            "readability": {
                "score": round(readability_score, 1),
                "level": "easy" if readability_score > 70 else "medium" if readability_score > 40 else "difficult"
            },
            "theme_analysis": {
                "primary_theme": primary_theme,
                "theme_scores": {
                    "technology": tech_score,
                    "business": business_score,
                    "social": social_score
                }
            },
            "quality_indicators": {
                "structure_score": min(100, paragraph_count * 20),
                "content_depth": min(100, word_count / 50),
                "keyword_diversity": len(set([kw[0] for kw in top_keywords]))
            }
        }
        
        return analysis_result
        
    except Exception as e:
        logger.error(f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}")
        return {"error": f"å†…å®¹åˆ†æå¤±è´¥: {str(e)}"}

@tool
def trend_analyzer(research_results: List[Dict[str, Any]], analysis_focus: str = "general") -> List[Dict[str, Any]]:
    """
    ğŸ” çœŸæ­£çš„è¶‹åŠ¿åˆ†æå·¥å…· - åŸºäºæ•°æ®ç§‘å­¦æ–¹æ³•
    
    ä½¿ç”¨å¤šç»´åº¦åˆ†ææ–¹æ³•è¯†åˆ«çœŸå®è¶‹åŠ¿ã€æ¨¡å¼å’Œæ´å¯Ÿï¼š
    - æ—¶é—´åºåˆ—åˆ†æï¼šæ£€æµ‹å‘å±•è¶‹åŠ¿å’Œå˜åŒ–é€Ÿåº¦
    - é¢‘ç‡åˆ†æï¼šè¯†åˆ«å…³é”®ä¸»é¢˜å’Œçƒ­ç‚¹
    - è¯­ä¹‰åˆ†æï¼šç†è§£å†…å®¹æ·±å±‚å«ä¹‰å’Œå…³è”
    - ç»Ÿè®¡æ¨æ–­ï¼šè®¡ç®—ç½®ä¿¡åº¦å’Œå¯ä¿¡åŒºé—´
    
    Args:
        research_results: ç ”ç©¶ç»“æœåˆ—è¡¨
        analysis_focus: åˆ†æé‡ç‚¹ (general/technology/market/social)
        
    Returns:
        åŸºäºçœŸå®æ•°æ®åˆ†æçš„è¶‹åŠ¿æ´å¯Ÿåˆ—è¡¨
    """
    try:
        if not research_results or len(research_results) < 3:
            return [{"error": f"ç ”ç©¶ç»“æœä¸è¶³ï¼ˆ{len(research_results)}æ¡ï¼‰ï¼Œéœ€è¦è‡³å°‘3æ¡æ•°æ®è¿›è¡Œæœ‰æ•ˆè¶‹åŠ¿åˆ†æ"}]
        
        insights = []
        total_content_length = sum(len(result.get("content", "")) for result in research_results)
        
        logger.info(f"å¼€å§‹è¶‹åŠ¿åˆ†æ: {len(research_results)}æ¡æ•°æ®ï¼Œæ€»å†…å®¹é•¿åº¦{total_content_length}å­—ç¬¦")
        
        # ============================================================================
        # 1. æ—¶é—´åºåˆ—è¶‹åŠ¿åˆ†æ
        # ============================================================================
        
        time_indicators = {
            "2024": {"weight": 1.0, "period": "current"},
            "2023": {"weight": 0.8, "period": "recent"},
            "2022": {"weight": 0.6, "period": "past"},
            "æœªæ¥": {"weight": 1.2, "period": "future"},
            "é¢„æµ‹": {"weight": 1.1, "period": "future"},
            "å±•æœ›": {"weight": 1.1, "period": "future"}
        }
        
        time_data = {}
        for result in research_results:
            content = result.get("content", "")
            content_quality = result.get("research_quality_score", 0.5)
            
            for indicator, props in time_indicators.items():
                if indicator in content:
                    period = props["period"]
                    weight = props["weight"] * content_quality
                    time_data[period] = time_data.get(period, 0) + weight
        
        if time_data:
            # è®¡ç®—æ—¶é—´è¶‹åŠ¿æ–¹å‘
            current_weight = time_data.get("current", 0)
            future_weight = time_data.get("future", 0)
            past_weight = time_data.get("recent", 0) + time_data.get("past", 0)
            
            total_weight = sum(time_data.values())
            
            if total_weight > 0:
                trend_direction = "ç¨³å®š"
                confidence = 0.5
                
                if future_weight > current_weight * 1.3:
                    trend_direction = "å¼ºåŠ²å¢é•¿"
                    confidence = min(0.9, future_weight / total_weight + 0.3)
                elif future_weight > current_weight:
                    trend_direction = "æ¸©å’Œå¢é•¿"
                    confidence = min(0.8, future_weight / total_weight + 0.2)
                elif current_weight < past_weight:
                    trend_direction = "æ”¾ç¼“è¶‹åŠ¿"
                    confidence = min(0.7, past_weight / total_weight + 0.1)
                
                insights.append({
                    "id": str(uuid.uuid4()),
                    "type": "time_series_trend",
                    "title": f"æ—¶é—´åºåˆ—åˆ†æï¼š{trend_direction}",
                    "description": f"åŸºäº{len(research_results)}ä¸ªæ•°æ®æºçš„æ—¶é—´åºåˆ—åˆ†æï¼Œå½“å‰å‘å±•å‘ˆç°{trend_direction}æ€åŠ¿ã€‚",
                    "evidence": [
                        f"å½“å‰æ—¶æœŸæƒé‡: {current_weight:.2f}",
                        f"æœªæ¥é¢„æœŸæƒé‡: {future_weight:.2f}",
                        f"å†å²å‚è€ƒæƒé‡: {past_weight:.2f}",
                        f"æ•°æ®è¦†ç›–å®Œæ•´åº¦: {(len([k for k in time_data.keys()]) / len(time_indicators)) * 100:.1f}%"
                    ],
                    "confidence_level": "high" if confidence > 0.7 else "medium" if confidence > 0.5 else "low",
                    "confidence_score": confidence,
                    "statistical_significance": confidence > 0.6,
                    "implications": _generate_time_implications(trend_direction),
                    "timestamp": time.time()
                })
        
        # ============================================================================
        # 2. ä¸»é¢˜èšç±»å’Œé¢‘ç‡åˆ†æ
        # ============================================================================
        
        def extract_meaningful_terms(text: str) -> List[str]:
            """æå–æœ‰æ„ä¹‰çš„æœ¯è¯­ï¼Œè¿‡æ»¤åœç”¨è¯"""
            stop_words = {
                "çš„", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "ä¸", "æˆ–", "ä½†", "è€Œ", "äº†", "ç€", "è¿‡", "ä¹Ÿ", "éƒ½", 
                "å°†", "ä¸º", "å› ä¸º", "ç”±äº", "å¦‚æœ", "è™½ç„¶", "ç„¶è€Œ", "å› æ­¤", "è¿™", "é‚£", "è¿™ä¸ª", "é‚£ä¸ª",
                "ä¸€ä¸ª", "å¾ˆ", "æ›´", "æœ€", "ç­‰", "åŠ", "ä»¥åŠ", "ç­‰ç­‰", "å¯ä»¥", "èƒ½å¤Ÿ", "éœ€è¦", "åº”è¯¥"
            }
            
            # æå–2-6å­—ç¬¦çš„è¯æ±‡ï¼Œæ’é™¤çº¯æ•°å­—å’Œç‰¹æ®Šç¬¦å·
            words = []
            for word in text.split():
                cleaned_word = ''.join(c for c in word if c.isalnum() or c in ['_', '-'])
                if (len(cleaned_word) >= 2 and len(cleaned_word) <= 6 
                    and cleaned_word not in stop_words 
                    and not cleaned_word.isdigit()):
                    words.append(cleaned_word)
            return words
        
        # ä¸»é¢˜é¢‘ç‡ç»Ÿè®¡
        term_frequency = {}
        term_contexts = {}
        
        for result in research_results:
            content = result.get("content", "")
            title = result.get("title", "")
            quality_score = result.get("research_quality_score", 0.5)
            
            # ä»æ ‡é¢˜å’Œå†…å®¹ä¸­æå–æœ¯è¯­
            title_terms = extract_meaningful_terms(title)
            content_terms = extract_meaningful_terms(content)
            
            # æ ‡é¢˜ä¸­çš„æœ¯è¯­æƒé‡æ›´é«˜
            for term in title_terms:
                weight = 2.0 * quality_score
                term_frequency[term] = term_frequency.get(term, 0) + weight
                if term not in term_contexts:
                    term_contexts[term] = []
                term_contexts[term].append({"source": "title", "content": title[:100]})
            
            for term in content_terms[:20]:  # é™åˆ¶æ¯ç¯‡å†…å®¹çš„æœ¯è¯­æ•°é‡
                weight = 1.0 * quality_score
                term_frequency[term] = term_frequency.get(term, 0) + weight
                if term not in term_contexts:
                    term_contexts[term] = []
                if len(term_contexts[term]) < 3:  # é™åˆ¶ä¸Šä¸‹æ–‡ç¤ºä¾‹æ•°é‡
                    term_contexts[term].append({"source": "content", "content": content[:100]})
        
        # è¯†åˆ«æ ¸å¿ƒä¸»é¢˜
        if term_frequency:
            sorted_terms = sorted(term_frequency.items(), key=lambda x: x[1], reverse=True)
            top_terms = sorted_terms[:10]
            
            # è®¡ç®—ä¸»é¢˜é›†ä¸­åº¦
            total_frequency = sum(term_frequency.values())
            top_3_frequency = sum(freq for _, freq in top_terms[:3])
            theme_concentration = top_3_frequency / total_frequency if total_frequency > 0 else 0
            
            insights.append({
                "id": str(uuid.uuid4()),
                "type": "thematic_analysis",
                "title": f"ä¸»é¢˜èšç±»åˆ†æï¼š{theme_concentration:.1%}é›†ä¸­åº¦",
                "description": f"åŸºäºè¯é¢‘å’Œè¯­ä¹‰åˆ†æï¼Œè¯†åˆ«å‡º{len(top_terms)}ä¸ªæ ¸å¿ƒä¸»é¢˜ï¼Œå‰3ä¸ªä¸»é¢˜å æ€»è®¨è®ºçš„{theme_concentration:.1%}ã€‚",
                "evidence": [
                    f"æ ¸å¿ƒä¸»é¢˜: {', '.join([term for term, _ in top_terms[:5]])}",
                    f"ä¸»é¢˜è¦†ç›–åº¦: {len(top_terms)}ä¸ªå…³é”®è¯",
                    f"è¯é¢‘åˆ†å¸ƒ: {', '.join([f'{term}({freq:.1f})' for term, freq in top_terms[:3]])}",
                    f"ä¸»é¢˜é›†ä¸­åº¦: {theme_concentration:.1%}"
                ],
                "confidence_level": "high" if theme_concentration > 0.4 else "medium",
                "confidence_score": min(0.9, theme_concentration + 0.3),
                "statistical_significance": len(top_terms) >= 5,
                "implications": [
                    "ç ”ç©¶ç„¦ç‚¹æ˜ç¡®" if theme_concentration > 0.4 else "ä¸»é¢˜åˆ†æ•£",
                    "æ•°æ®è´¨é‡è¾ƒé«˜" if len(top_terms) >= 5 else "éœ€è¦æ›´å¤šæ•°æ®",
                    "ç»“è®ºå¯ä¿¡åº¦é«˜" if theme_concentration > 0.3 else "ç»“è®ºéœ€è¦è°¨æ…è§£è¯»"
                ],
                "detailed_themes": [{"term": term, "frequency": freq, "contexts": term_contexts.get(term, [])} for term, freq in top_terms[:5]],
                "timestamp": time.time()
            })
        
        # ============================================================================
        # 3. æƒ…æ„Ÿå’Œæ€åº¦è¶‹åŠ¿åˆ†æ
        # ============================================================================
        
        sentiment_indicators = {
            "positive": {
                "keywords": ["ä¼˜ç§€", "æˆåŠŸ", "å¢é•¿", "æå‡", "æ”¹å–„", "åˆ›æ–°", "å‘å±•", "æœºä¼š", "ä¼˜åŠ¿", "çªç ´", "é¢†å…ˆ", "é«˜æ•ˆ"],
                "weight": 1.0
            },
            "negative": {
                "keywords": ["å›°éš¾", "æŒ‘æˆ˜", "é—®é¢˜", "é£é™©", "å¨èƒ", "ç¼ºé™·", "åŠ£åŠ¿", "ä¸‹é™", "å‡å°‘", "é™åˆ¶", "éšœç¢", "å±æœº"],
                "weight": 1.0
            },
            "neutral": {
                "keywords": ["åˆ†æ", "ç ”ç©¶", "è®¨è®º", "æ¢ç´¢", "è€ƒè™‘", "è¯„ä¼°", "æ¯”è¾ƒ", "è§‚å¯Ÿ", "æè¿°", "è¯´æ˜"],
                "weight": 0.5
            }
        }
        
        sentiment_scores = {"positive": 0, "negative": 0, "neutral": 0}
        
        for result in research_results:
            content = result.get("content", "").lower()
            quality_score = result.get("research_quality_score", 0.5)
            
            for sentiment, props in sentiment_indicators.items():
                for keyword in props["keywords"]:
                    if keyword in content:
                        sentiment_scores[sentiment] += props["weight"] * quality_score
        
        total_sentiment = sum(sentiment_scores.values())
        if total_sentiment > 0:
            sentiment_distribution = {k: v/total_sentiment for k, v in sentiment_scores.items()}
            dominant_sentiment = max(sentiment_distribution.items(), key=lambda x: x[1])
            
            insights.append({
                "id": str(uuid.uuid4()),
                "type": "sentiment_trend",
                "title": f"æƒ…æ„Ÿæ€åº¦åˆ†æï¼š{dominant_sentiment[0]}å€¾å‘({dominant_sentiment[1]:.1%})",
                "description": f"åŸºäºæƒ…æ„Ÿè¯æ±‡åˆ†æï¼Œå½“å‰è®¨è®ºå‘ˆç°{dominant_sentiment[0]}æ€åº¦å€¾å‘ï¼Œå æ¯”{dominant_sentiment[1]:.1%}ã€‚",
                "evidence": [
                    f"ç§¯ææƒ…æ„Ÿ: {sentiment_distribution['positive']:.1%}",
                    f"æ¶ˆææƒ…æ„Ÿ: {sentiment_distribution['negative']:.1%}",
                    f"ä¸­æ€§æ€åº¦: {sentiment_distribution['neutral']:.1%}",
                    f"æƒ…æ„Ÿå¼ºåº¦: {total_sentiment:.2f}"
                ],
                "confidence_level": "high" if dominant_sentiment[1] > 0.6 else "medium",
                "confidence_score": dominant_sentiment[1],
                "statistical_significance": total_sentiment > 5.0,
                "implications": _generate_sentiment_implications(dominant_sentiment[0], dominant_sentiment[1]),
                "sentiment_distribution": sentiment_distribution,
                "timestamp": time.time()
            })
        
        # ============================================================================
        # 4. æ•°æ®è´¨é‡è¯„ä¼°æŠ¥å‘Š
        # ============================================================================
        
        quality_scores = [r.get("research_quality_score", 0.5) for r in research_results]
        avg_quality = statistics.mean(quality_scores) if quality_scores else 0.5
        quality_std = statistics.stdev(quality_scores) if len(quality_scores) > 1 else 0
        
        # æ•°æ®æºå¤šæ ·æ€§åˆ†æ
        unique_sources = len(set(r.get("url", "") for r in research_results))
        source_diversity = unique_sources / len(research_results) if research_results else 0
        
        insights.append({
            "id": str(uuid.uuid4()),
            "type": "data_quality_assessment",
            "title": f"æ•°æ®è´¨é‡è¯„ä¼°ï¼š{avg_quality:.2f}å¹³å‡è´¨é‡åˆ†",
            "description": f"åˆ†æäº†{len(research_results)}ä¸ªæ•°æ®æºï¼Œå¹³å‡è´¨é‡{avg_quality:.2f}ï¼Œæ¥æºå¤šæ ·æ€§{source_diversity:.1%}ã€‚",
            "evidence": [
                f"æ•°æ®æºæ•°é‡: {len(research_results)}ä¸ª",
                f"å¹³å‡è´¨é‡åˆ†: {avg_quality:.3f}/1.0",
                f"è´¨é‡æ ‡å‡†å·®: {quality_std:.3f}",
                f"ç‹¬ç‰¹æ¥æºæ•°: {unique_sources}ä¸ª",
                f"æ¥æºå¤šæ ·æ€§: {source_diversity:.1%}"
            ],
            "confidence_level": "high" if avg_quality > 0.7 and source_diversity > 0.7 else "medium",
            "confidence_score": (avg_quality + source_diversity) / 2,
            "statistical_significance": len(research_results) >= 5,
            "implications": [
                "æ•°æ®è´¨é‡å¯é " if avg_quality > 0.7 else "æ•°æ®è´¨é‡éœ€è¦æ”¹è¿›",
                "æ¥æºå¤šæ ·åŒ–" if source_diversity > 0.7 else "æ¥æºç›¸å¯¹é›†ä¸­",
                "åˆ†æç»“æœå¯ä¿¡" if avg_quality > 0.6 and source_diversity > 0.5 else "ç»“æœéœ€è¦è°¨æ…è§£è¯»"
            ],
            "quality_metrics": {
                "avg_quality": avg_quality,
                "quality_std": quality_std,
                "source_diversity": source_diversity,
                "sample_size": len(research_results)
            },
            "timestamp": time.time()
        })
        
        logger.info(f"é«˜çº§è¶‹åŠ¿åˆ†æå®Œæˆ: ç”Ÿæˆ{len(insights)}ä¸ªå¤šç»´åº¦æ´å¯Ÿï¼Œå¹³å‡æ•°æ®è´¨é‡{avg_quality:.3f}")
        return insights
        
    except Exception as e:
        logger.error(f"è¶‹åŠ¿åˆ†æå¤±è´¥: {str(e)}")
        return [{"error": f"è¶‹åŠ¿åˆ†æç³»ç»Ÿé”™è¯¯: {str(e)}"}]
    
def _generate_time_implications(trend_direction: str) -> List[str]:
    """æ ¹æ®æ—¶é—´è¶‹åŠ¿ç”Ÿæˆå½±å“åˆ†æ"""
    implications_map = {
        "å¼ºåŠ²å¢é•¿": ["æŠ•èµ„æœºä¼šæ˜¾è‘—", "å¸‚åœºå‰æ™¯ä¹è§‚", "èµ„æºéœ€æ±‚å¿«é€Ÿå¢åŠ ", "ç«äº‰å°†åŠ å‰§"],
        "æ¸©å’Œå¢é•¿": ["ç¨³å¥å‘å±•æ€åŠ¿", "é€‚åº¦æŠ•èµ„æœºä¼š", "å¸‚åœºé€æ­¥æˆç†Ÿ", "é£é™©ç›¸å¯¹å¯æ§"],
        "ç¨³å®š": ["å¸‚åœºç›¸å¯¹æˆç†Ÿ", "å¢é•¿åŠ¨åŠ›æœ‰é™", "ç«äº‰æ ¼å±€ç¨³å®š", "åˆ›æ–°ç©ºé—´å­˜åœ¨"],
        "æ”¾ç¼“è¶‹åŠ¿": ["å¢é•¿åŠ¨åŠ›å‡å¼±", "å¸‚åœºå¯èƒ½é¥±å’Œ", "éœ€è¦æ–°çš„å¢é•¿ç‚¹", "è½¬å‹éœ€æ±‚å¢åŠ "]
    }
    return implications_map.get(trend_direction, ["å‘å±•æ€åŠ¿éœ€è¦æŒç»­è§‚å¯Ÿ"])
    
def _generate_sentiment_implications(sentiment: str, confidence: float) -> List[str]:
    """æ ¹æ®æƒ…æ„Ÿæ€åº¦ç”Ÿæˆå½±å“åˆ†æ"""
    base_implications = {
        "positive": ["å¸‚åœºä¿¡å¿ƒè¾ƒå¼º", "å‘å±•ç¯å¢ƒè‰¯å¥½", "æŠ•èµ„æ„æ„¿ç§¯æ"],
        "negative": ["é¢ä¸´æŒ‘æˆ˜è¾ƒå¤š", "é£é™©éœ€è¦å…³æ³¨", "è°¨æ…å‘å±•ç­–ç•¥"],
        "neutral": ["è§‚æœ›æ€åº¦æ˜æ˜¾", "å‘å±•æ–¹å‘å¾…æ˜ç¡®", "éœ€è¦æ›´å¤šä¿¡æ¯"]
    }
    
    implications = base_implications.get(sentiment, ["æ€åº¦å€¾å‘éœ€è¦è¿›ä¸€æ­¥åˆ†æ"])
    
    # æ ¹æ®ç½®ä¿¡åº¦è°ƒæ•´
    if confidence > 0.8:
        implications.append("æ€åº¦å€¾å‘éå¸¸æ˜ç¡®")
    elif confidence > 0.6:
        implications.append("æ€åº¦å€¾å‘ç›¸å¯¹æ˜ç¡®")
    else:
        implications.append("æ€åº¦å€¾å‘æœ‰å¾…è§‚å¯Ÿ")
        
    return implications

# ============================================================================
# å†…å®¹ç”Ÿæˆå·¥å…·
# ============================================================================

@tool
def section_content_generator(
    section_title: str,
    section_description: str,
    research_data: List[Dict[str, Any]],
    target_words: int = 500,
    style: str = "professional"
) -> Dict[str, Any]:
    """
    ç« èŠ‚å†…å®¹ç”Ÿæˆå·¥å…·
    åŸºäºç ”ç©¶æ•°æ®ç”Ÿæˆç‰¹å®šç« èŠ‚çš„å†…å®¹
    
    Args:
        section_title: ç« èŠ‚æ ‡é¢˜
        section_description: ç« èŠ‚æè¿°
        research_data: ç›¸å…³ç ”ç©¶æ•°æ®
        target_words: ç›®æ ‡å­—æ•°
        style: å†™ä½œé£æ ¼
        
    Returns:
        ç”Ÿæˆçš„ç« èŠ‚å†…å®¹
    """
    try:
        # æå–ç›¸å…³ç ”ç©¶å†…å®¹
        relevant_content = []
        for data in research_data:
            if not data.get("error"):
                content = data.get("content", "")
                if content and len(content) > 50:
                    relevant_content.append(content[:300])  # é™åˆ¶æ¯ä¸ªæ¥æºçš„å†…å®¹é•¿åº¦
        
        if not relevant_content:
            return {"error": "æ²¡æœ‰è¶³å¤Ÿçš„ç ”ç©¶æ•°æ®æ”¯æŒå†…å®¹ç”Ÿæˆ"}
        
        # æ„å»ºå†…å®¹ç»“æ„
        content_parts = []
        
        # å¼•è¨€éƒ¨åˆ†
        intro = f"## {section_title}\n\n{section_description}\n\n"
        content_parts.append(intro)
        
        # ä¸»ä½“å†…å®¹ - åŸºäºç ”ç©¶æ•°æ®
        main_content = "### æ ¸å¿ƒè§‚ç‚¹\n\n"
        
        # ä»ç ”ç©¶æ•°æ®ä¸­æå–å…³é”®ç‚¹
        key_points = []
        for i, content in enumerate(relevant_content[:3], 1):  # é™åˆ¶ä½¿ç”¨å‰3ä¸ªæœ€ç›¸å…³çš„å†…å®¹
            # ç®€åŒ–çš„å…³é”®ç‚¹æå–
            sentences = content.split("ã€‚")[:2]  # å–å‰ä¸¤å¥
            if sentences:
                key_point = "ã€‚".join(sentences) + "ã€‚"
                key_points.append(f"{i}. {key_point}")
        
        main_content += "\n".join(key_points)
        content_parts.append(main_content)
        
        # åˆ†æéƒ¨åˆ†
        if len(relevant_content) > 1:
            analysis = "\n\n### æ·±å…¥åˆ†æ\n\n"
            analysis += "é€šè¿‡å¯¹å¤šä¸ªä¿¡æ¯æºçš„ç»¼åˆåˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°ä»¥ä¸‹å…³é”®è¶‹åŠ¿å’Œæ¨¡å¼ï¼š\n\n"
            
            # åŸºäºæ•°æ®ç”Ÿæˆåˆ†æè¦ç‚¹
            analysis_points = [
                "æ•°æ®æ˜¾ç¤ºè¯¥é¢†åŸŸæ­£åœ¨ç»å†å¿«é€Ÿå‘å±•å’Œå˜åŒ–",
                "å¤šä¸ªç ”ç©¶æ¥æºçš„è§‚ç‚¹å‘ˆç°å‡ºä¸€è‡´çš„å‘å±•æ–¹å‘",
                "æŠ€æœ¯è¿›æ­¥å’Œå¸‚åœºéœ€æ±‚æ˜¯æ¨åŠ¨å˜åŒ–çš„ä¸»è¦å› ç´ "
            ]
            
            for point in analysis_points:
                analysis += f"- {point}\n"
            
            content_parts.append(analysis)
        
        # ç»“è®ºéƒ¨åˆ†
        conclusion = f"\n\n### å°ç»“\n\n"
        conclusion += f"ç»¼åˆä»¥ä¸Šåˆ†æï¼Œ{section_title}åœ¨å½“å‰ç¯å¢ƒä¸‹å±•ç°å‡ºé‡è¦çš„å‘å±•ä»·å€¼å’Œåº”ç”¨å‰æ™¯ã€‚"
        conclusion += "ç›¸å…³å‘å±•è¶‹åŠ¿å€¼å¾—æŒç»­å…³æ³¨å’Œæ·±å…¥ç ”ç©¶ã€‚"
        content_parts.append(conclusion)
        
        # åˆå¹¶å†…å®¹
        full_content = "".join(content_parts)
        actual_words = len(full_content.replace(" ", "").replace("\n", ""))
        
        result = {
            "id": str(uuid.uuid4()),
            "section_title": section_title,
            "content": full_content,
            "word_count": actual_words,
            "target_words": target_words,
            "style": style,
            "sources_used": len(relevant_content),
            "generated_at": time.time(),
            "quality_score": min(100, (actual_words / max(target_words, 1)) * 80 + 20)
        }
        
        logger.info(f"ç« èŠ‚å†…å®¹ç”Ÿæˆå®Œæˆ: {section_title}, {actual_words}å­—")
        return result
        
    except Exception as e:
        logger.error(f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}")
        return {"error": f"å†…å®¹ç”Ÿæˆå¤±è´¥: {str(e)}"}

@tool
def report_formatter(
    title: str,
    sections: List[Dict[str, Any]],
    executive_summary: str = "",
    metadata: Dict[str, Any] = None
) -> Dict[str, Any]:
    """
    æŠ¥å‘Šæ ¼å¼åŒ–å·¥å…·
    å°†ç« èŠ‚å†…å®¹æ•´åˆä¸ºå®Œæ•´çš„æ ¼å¼åŒ–æŠ¥å‘Š
    
    Args:
        title: æŠ¥å‘Šæ ‡é¢˜
        sections: ç« èŠ‚å†…å®¹åˆ—è¡¨
        executive_summary: æ‰§è¡Œæ‘˜è¦
        metadata: å…ƒæ•°æ®ä¿¡æ¯
        
    Returns:
        æ ¼å¼åŒ–çš„å®Œæ•´æŠ¥å‘Š
    """
    try:
        if not sections:
            return {"error": "æ²¡æœ‰ç« èŠ‚å†…å®¹å¯ä¾›æ ¼å¼åŒ–"}
        
        report_parts = []
        
        # æŠ¥å‘Šæ ‡é¢˜
        report_parts.append(f"# {title}\n\n")
        
        # æŠ¥å‘Šä¿¡æ¯
        if metadata:
            report_parts.append("## æŠ¥å‘Šä¿¡æ¯\n\n")
            report_parts.append(f"- **ç”Ÿæˆæ—¶é—´**: {datetime.fromtimestamp(metadata.get('generated_at', time.time())).strftime('%Y-%m-%d %H:%M:%S')}\n")
            report_parts.append(f"- **æŠ¥å‘Šç±»å‹**: {metadata.get('report_type', 'ç ”ç©¶æŠ¥å‘Š')}\n")
            report_parts.append(f"- **ç›®æ ‡è¯»è€…**: {metadata.get('target_audience', 'ä¸“ä¸šäººå£«')}\n")
            report_parts.append(f"- **æ·±åº¦çº§åˆ«**: {metadata.get('depth_level', 'æ·±åº¦åˆ†æ')}\n\n")
        
        # æ‰§è¡Œæ‘˜è¦
        if executive_summary:
            report_parts.append("## æ‰§è¡Œæ‘˜è¦\n\n")
            report_parts.append(f"{executive_summary}\n\n")
        
        # ç›®å½•
        report_parts.append("## ç›®å½•\n\n")
        for i, section in enumerate(sections, 1):
            section_title = section.get("section_title", f"ç« èŠ‚ {i}")
            report_parts.append(f"{i}. {section_title}\n")
        report_parts.append("\n")
        
        # ç« èŠ‚å†…å®¹
        for section in sections:
            content = section.get("content", "")
            if content and not section.get("error"):
                report_parts.append(content)
                report_parts.append("\n\n---\n\n")
        
        # é™„å½•ä¿¡æ¯
        report_parts.append("## é™„å½•\n\n")
        report_parts.append("### æ•°æ®æ¥æºç»Ÿè®¡\n\n")
        
        total_sources = sum(section.get("sources_used", 0) for section in sections)
        total_words = sum(section.get("word_count", 0) for section in sections)
        
        report_parts.append(f"- **æ€»å­—æ•°**: {total_words:,} å­—\n")
        report_parts.append(f"- **æ•°æ®æºæ•°é‡**: {total_sources} ä¸ª\n")
        report_parts.append(f"- **ç« èŠ‚æ•°é‡**: {len(sections)} ä¸ª\n")
        
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        full_report = "".join(report_parts)
        
        result = {
            "id": str(uuid.uuid4()),
            "title": title,
            "content": full_report,
            "total_words": len(full_report.replace(" ", "").replace("\n", "")),
            "sections_count": len(sections),
            "total_sources": total_sources,
            "generated_at": time.time(),
            "format_version": "1.0",
            "quality_metrics": {
                "completeness": min(100, len(sections) * 20),
                "depth": min(100, total_words / 100),
                "source_diversity": min(100, total_sources * 10)
            }
        }
        
        logger.info(f"æŠ¥å‘Šæ ¼å¼åŒ–å®Œæˆ: {title}, æ€»å­—æ•° {result['total_words']}")
        return result
        
    except Exception as e:
        logger.error(f"æŠ¥å‘Šæ ¼å¼åŒ–å¤±è´¥: {str(e)}")
        return {"error": f"æŠ¥å‘Šæ ¼å¼åŒ–å¤±è´¥: {str(e)}"}

# ============================================================================
# éªŒè¯å·¥å…·
# ============================================================================

@tool
def quality_validator(content: str, validation_criteria: Dict[str, Any] = None) -> Dict[str, Any]:
    """
    è´¨é‡éªŒè¯å·¥å…·
    å¯¹ç”Ÿæˆçš„å†…å®¹è¿›è¡Œè´¨é‡è¯„ä¼°å’ŒéªŒè¯
    
    Args:
        content: å¾…éªŒè¯å†…å®¹
        validation_criteria: éªŒè¯æ ‡å‡†
        
    Returns:
        éªŒè¯ç»“æœå’Œå»ºè®®
    """
    try:
        if not content or len(content.strip()) < 100:
            return {"error": "å†…å®¹è¿‡çŸ­ï¼Œæ— æ³•è¿›è¡Œè´¨é‡éªŒè¯"}
        
        # é»˜è®¤éªŒè¯æ ‡å‡†
        default_criteria = {
            "min_words": 500,
            "max_words": 10000,
            "min_sections": 3,
            "readability_threshold": 60,
            "structure_required": True
        }
        
        criteria = validation_criteria or default_criteria
        validation_results = {}
        issues = []
        suggestions = []
        
        # å­—æ•°éªŒè¯
        word_count = len(content.replace(" ", "").replace("\n", ""))
        min_words = criteria.get("min_words", 500)
        max_words = criteria.get("max_words", 10000)
        
        if word_count < min_words:
            issues.append(f"å†…å®¹è¿‡çŸ­ï¼š{word_count}å­—ï¼Œå»ºè®®è‡³å°‘{min_words}å­—")
            suggestions.append("å¢åŠ æ›´å¤šè¯¦ç»†å†…å®¹å’Œä¾‹å­")
        elif word_count > max_words:
            issues.append(f"å†…å®¹è¿‡é•¿ï¼š{word_count}å­—ï¼Œè¶…è¿‡å»ºè®®çš„{max_words}å­—")
            suggestions.append("ç²¾ç®€å†…å®¹æˆ–åˆ†å‰²ä¸ºå¤šä¸ªæŠ¥å‘Š")
        
        validation_results["word_count_check"] = {
            "passed": min_words <= word_count <= max_words,
            "actual": word_count,
            "expected_range": f"{min_words}-{max_words}"
        }
        
        # ç»“æ„éªŒè¯
        section_count = content.count("##")
        min_sections = criteria.get("min_sections", 3)
        
        if section_count < min_sections:
            issues.append(f"ç« èŠ‚è¿‡å°‘ï¼š{section_count}ä¸ªï¼Œå»ºè®®è‡³å°‘{min_sections}ä¸ª")
            suggestions.append("å¢åŠ æ›´å¤šç« èŠ‚ä»¥å®Œå–„æŠ¥å‘Šç»“æ„")
        
        validation_results["structure_check"] = {
            "passed": section_count >= min_sections,
            "actual_sections": section_count,
            "expected_min": min_sections
        }
        
        # å¯è¯»æ€§éªŒè¯
        sentences = [s for s in content.split("ã€‚") if s.strip()]
        if sentences:
            avg_sentence_length = word_count / len(sentences)
            readability_score = max(0, min(100, 100 - (avg_sentence_length - 15) * 2))
            readability_threshold = criteria.get("readability_threshold", 60)
            
            if readability_score < readability_threshold:
                issues.append(f"å¯è¯»æ€§åä½ï¼š{readability_score:.1f}åˆ†ï¼Œå»ºè®®è¾¾åˆ°{readability_threshold}åˆ†ä»¥ä¸Š")
                suggestions.append("ä½¿ç”¨æ›´ç®€æ´çš„å¥å¼å’Œæ›´é€šä¿—çš„è¡¨è¾¾")
            
            validation_results["readability_check"] = {
                "passed": readability_score >= readability_threshold,
                "score": round(readability_score, 1),
                "threshold": readability_threshold
            }
        
        # å†…å®¹å®Œæ•´æ€§éªŒè¯
        has_introduction = any(keyword in content for keyword in ["å¼•è¨€", "æ¦‚è¿°", "èƒŒæ™¯", "ä»‹ç»"])
        has_conclusion = any(keyword in content for keyword in ["ç»“è®º", "æ€»ç»“", "å°ç»“", "ç»¼è¿°"])
        has_analysis = any(keyword in content for keyword in ["åˆ†æ", "ç ”ç©¶", "è°ƒæŸ¥", "æ¢è®¨"])
        
        completeness_score = sum([has_introduction, has_conclusion, has_analysis]) / 3 * 100
        
        if not has_introduction:
            issues.append("ç¼ºå°‘å¼•è¨€æˆ–èƒŒæ™¯ä»‹ç»éƒ¨åˆ†")
            suggestions.append("æ·»åŠ å¼•è¨€ç« èŠ‚ä»‹ç»ç ”ç©¶èƒŒæ™¯å’Œç›®æ ‡")
        
        if not has_conclusion:
            issues.append("ç¼ºå°‘ç»“è®ºæˆ–æ€»ç»“éƒ¨åˆ†")
            suggestions.append("æ·»åŠ ç»“è®ºç« èŠ‚æ€»ç»“ä¸»è¦å‘ç°")
        
        if not has_analysis:
            issues.append("ç¼ºå°‘åˆ†ææˆ–ç ”ç©¶å†…å®¹")
            suggestions.append("å¢åŠ æ·±åº¦åˆ†æå’Œç ”ç©¶å†…å®¹")
        
        validation_results["completeness_check"] = {
            "passed": completeness_score >= 66.7,
            "score": round(completeness_score, 1),
            "components": {
                "has_introduction": has_introduction,
                "has_conclusion": has_conclusion,
                "has_analysis": has_analysis
            }
        }
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        passed_checks = sum(1 for check in validation_results.values() if check.get("passed", False))
        total_checks = len(validation_results)
        overall_score = (passed_checks / total_checks) * 100 if total_checks > 0 else 0
        
        result = {
            "id": str(uuid.uuid4()),
            "validation_results": validation_results,
            "overall_score": round(overall_score, 1),
            "quality_level": "ä¼˜ç§€" if overall_score >= 80 else "è‰¯å¥½" if overall_score >= 60 else "éœ€æ”¹è¿›",
            "issues": issues,
            "suggestions": suggestions,
            "validated_at": time.time(),
            "content_length": word_count
        }
        
        logger.info(f"è´¨é‡éªŒè¯å®Œæˆ: æ€»åˆ†{overall_score:.1f}, {len(issues)}ä¸ªé—®é¢˜")
        return result
        
    except Exception as e:
        logger.error(f"è´¨é‡éªŒè¯å¤±è´¥: {str(e)}")
        return {"error": f"è´¨é‡éªŒè¯å¤±è´¥: {str(e)}"}

# ============================================================================
# å·¥å…·é›†åˆå‡½æ•°
# ============================================================================

def get_research_tools():
    """è·å–ç ”ç©¶ç›¸å…³å·¥å…·"""
    return [advanced_web_search, multi_source_research]

def get_analysis_tools():
    """è·å–åˆ†æç›¸å…³å·¥å…·"""
    return [content_analyzer, trend_analyzer]

def get_writing_tools():
    """è·å–å†™ä½œç›¸å…³å·¥å…·"""
    return [section_content_generator, report_formatter]

def get_validation_tools():
    """è·å–éªŒè¯ç›¸å…³å·¥å…·"""
    return [quality_validator]

def get_all_tools():
    """è·å–æ‰€æœ‰å·¥å…·"""
    return [
        advanced_web_search,
        multi_source_research,
        content_analyzer,
        trend_analyzer,
        section_content_generator,
        report_formatter,
        quality_validator
    ]

# é»˜è®¤å¯¼å‡ºæ‰€æœ‰ç ”ç©¶å·¥å…·é›†åˆï¼Œä¾›å­å›¾å¼•ç”¨
ALL_RESEARCH_TOOLS = get_research_tools()

# ============================================================================
# å·¥å…·é…ç½®å’Œæµ‹è¯•
# ============================================================================

def validate_tool_environment() -> Dict[str, Any]:
    """éªŒè¯å·¥å…·ç¯å¢ƒé…ç½®"""
    config_status = {
        "tavily_api_available": bool(os.getenv("TAVILY_API_KEY") or "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"),
        "tools_loaded": len(get_all_tools()),
        "timestamp": time.time()
    }
    
    logger.info(f"å·¥å…·ç¯å¢ƒéªŒè¯å®Œæˆ: {config_status}")
    return config_status

if __name__ == "__main__":
    # å·¥å…·æµ‹è¯•
    print("=== æ·±åº¦ç ”ç©¶å·¥å…·æµ‹è¯• ===")
    env_status = validate_tool_environment()
    print(f"ç¯å¢ƒçŠ¶æ€: {env_status}")
    
    # æµ‹è¯•æœç´¢å·¥å…·
    search_result = advanced_web_search.invoke({
        "query": "äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿",
        "max_results": 2
    })
    print(f"æœç´¢æµ‹è¯•ç»“æœ: {len(search_result)}ä¸ªç»“æœ")
    
    # æµ‹è¯•åˆ†æå·¥å…·
    if search_result and not search_result[0].get("error"):
        analysis_result = content_analyzer.invoke({
            "text": search_result[0].get("content", ""),
            "analysis_type": "comprehensive"
        })
        print(f"åˆ†ææµ‹è¯•å®Œæˆ: è´¨é‡è¯„åˆ† {analysis_result.get('quality_indicators', {}).get('content_depth', 0)}")