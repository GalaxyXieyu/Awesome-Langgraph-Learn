"""
æ ¸å¿ƒå›¾æ¨¡å—
é«˜çº§äº¤äº’å¼æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ä¸»è¦å·¥ä½œæµå›¾
é›†æˆMulti-Agentåä½œã€Human-in-loopäº¤äº’å’Œæµå¼è¾“å‡º
"""

import json
import time
import asyncio
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime

from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer
from langgraph.types import interrupt
import logging

def safe_get_stream_writer():
    """å®‰å…¨è·å–æµå†™å…¥å™¨ï¼Œé¿å…ä¸Šä¸‹æ–‡é”™è¯¯"""
    try:
        return get_stream_writer()
    except Exception:
        # å¦‚æœæ²¡æœ‰æµä¸Šä¸‹æ–‡ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„å†™å…¥å™¨
        return lambda x: None

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from state import (
    DeepResearchState, ReportMode, AgentType, InteractionType, TaskStatus,
    ReportOutline, ReportSection, ResearchResult, AnalysisInsight,
    update_performance_metrics, add_research_result,
    add_analysis_insight, update_task_status, add_user_interaction
)
from tools import (
    get_research_tools, get_analysis_tools, get_writing_tools, get_validation_tools,
    advanced_web_search, multi_source_research, content_analyzer, trend_analyzer,
    section_content_generator, report_formatter, quality_validator
)

# å¯¼å…¥å­å›¾æ¨¡å—
from subgraph.research.graph import (
    create_intelligent_section_research_graph,
    create_intelligent_initial_state,
    IntelligentSectionState
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm() -> ChatOpenAI:
    """åˆ›å»ºLLMå®ä¾‹"""
    return ChatOpenAI(
        model="qwen2.5-72b-instruct-awq",
        temperature=0.7,
        base_url="https://llm.3qiao.vip:23436/v1",
        api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197",
    )

# ç¼–è¯‘å­å›¾ï¼ˆå…¨å±€å˜é‡ï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼‰
_intelligent_section_subgraph = None

def get_intelligent_section_subgraph():
    """è·å–æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾å®ä¾‹"""
    global _intelligent_section_subgraph
    if _intelligent_section_subgraph is None:
        workflow = create_intelligent_section_research_graph()
        _intelligent_section_subgraph = workflow.compile()
    return _intelligent_section_subgraph

def call_intelligent_section_research(state: DeepResearchState) -> DeepResearchState:
    """
    è°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾

    è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸»å›¾å’Œå­å›¾ä¹‹é—´çš„çŠ¶æ€è½¬æ¢ï¼š
    1. å°† DeepResearchState è½¬æ¢ä¸º IntelligentSectionState
    2. è°ƒç”¨å­å›¾è¿›è¡Œç« èŠ‚ç ”ç©¶
    3. å°†ç»“æœè½¬æ¢å› DeepResearchState
    """
    try:
        # è·å–å­å›¾å®ä¾‹
        subgraph = get_intelligent_section_subgraph()

        # è·å–å½“å‰å¤„ç†çš„ç« èŠ‚
        current_section_index = state.get("current_section_index", 0)
        sections = state.get("sections", [])

        if current_section_index >= len(sections):
            logger.warning(f"ç« èŠ‚ç´¢å¼• {current_section_index} è¶…å‡ºèŒƒå›´ï¼Œæ€»ç« èŠ‚æ•°: {len(sections)}")
            return state

        current_section = sections[current_section_index]

        # çŠ¶æ€è½¬æ¢ï¼šDeepResearchState -> IntelligentSectionState
        # å‡†å¤‡å‰é¢ç« èŠ‚çš„æ‘˜è¦
        previous_sections_summary = []
        for i in range(current_section_index):
            if i < len(sections) and sections[i].get("content"):
                summary = sections[i].get("content", "")[:200] + "..." if len(sections[i].get("content", "")) > 200 else sections[i].get("content", "")
                previous_sections_summary.append(f"{sections[i].get('title', '')}: {summary}")

        # å‡†å¤‡åç»­ç« èŠ‚å¤§çº²
        upcoming_sections_outline = []
        for i in range(current_section_index + 1, len(sections)):
            if i < len(sections):
                upcoming_sections_outline.append(f"{sections[i].get('title', '')}: {sections[i].get('description', '')}")

        subgraph_input = create_intelligent_initial_state(
            topic=state.get("topic", ""),
            section={
                "title": current_section.get("title", ""),
                "description": current_section.get("description", ""),
                "requirements": current_section.get("requirements", [])
            },
            previous_sections_summary=previous_sections_summary,
            upcoming_sections_outline=upcoming_sections_outline,
            report_main_thread=state.get("outline", {}).get("executive_summary", ""),
            writing_style=state.get("writing_style", "professional"),
            quality_threshold=0.8,
            max_iterations=3
        )

        logger.info(f"å¼€å§‹æ™ºèƒ½ç« èŠ‚ç ”ç©¶: {current_section.get('title', 'æœªçŸ¥ç« èŠ‚')}")

        # è°ƒç”¨å­å›¾
        subgraph_output = subgraph.invoke(subgraph_input)

        # çŠ¶æ€è½¬æ¢ï¼šIntelligentSectionState -> DeepResearchState
        if subgraph_output and subgraph_output.get("final_content"):
            # æ›´æ–°å½“å‰ç« èŠ‚å†…å®¹
            updated_sections = sections.copy()
            updated_sections[current_section_index] = {
                **current_section,
                "content": subgraph_output["final_content"],
                "research_data": subgraph_output.get("research_results", []),
                "quality_score": subgraph_output.get("quality_metrics", {}).get("overall_score", 0.0),
                "status": "completed"
            }

            # åˆå¹¶ç ”ç©¶ç»“æœåˆ°ä¸»çŠ¶æ€
            new_research_results = []
            research_data = subgraph_output.get("research_data", {})
            initial_research = research_data.get("initial_research", [])
            supplementary_research = research_data.get("supplementary_research", [])

            for research_item in initial_research + supplementary_research:
                new_research_results.append(ResearchResult(
                    id=research_item.get("id", str(uuid.uuid4())),
                    query=research_item.get("query", ""),
                    source_type="web",
                    title=research_item.get("title", ""),
                    content=research_item.get("content", ""),
                    url=research_item.get("url", ""),
                    relevance_score=research_item.get("relevance_score", 0.8),
                    timestamp=research_item.get("timestamp", time.time()),
                    section_id=current_section.get("id", "")
                ))

            # æ›´æ–°çŠ¶æ€
            updated_state = {
                **state,
                "sections": updated_sections,
                "current_section_index": current_section_index + 1,
                "research_results": state.get("research_results", []) + new_research_results,
                "performance_metrics": {
                    **state.get("performance_metrics", {}),
                    "sections_completed": current_section_index + 1,
                    "total_sections": len(sections),
                    "last_section_quality": subgraph_output.get("quality_metrics", {}).get("overall_score", 0.0)
                }
            }

            logger.info(f"ç« èŠ‚ç ”ç©¶å®Œæˆ: {current_section.get('title', 'æœªçŸ¥ç« èŠ‚')}, è´¨é‡åˆ†æ•°: {subgraph_output.get('quality_metrics', {}).get('overall_score', 0.0)}")
            return updated_state
        else:
            logger.error("å­å›¾è¿”å›äº†ç©ºç»“æœ")
            return state

    except Exception as e:
        logger.error(f"è°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾æ—¶å‡ºé”™: {e}")
        return state

def prepare_subgraph_state(main_state: DeepResearchState, section: Dict[str, Any], section_index: int, completed_sections: List[Dict[str, Any]]) -> Dict[str, Any]:
    """å‡†å¤‡å­å›¾è¾“å…¥çŠ¶æ€"""

    # å‡†å¤‡å‰é¢ç« èŠ‚çš„æ‘˜è¦
    previous_sections_summary = []
    for completed_section in completed_sections:
        if completed_section.get("content"):
            summary = completed_section["content"][:200] + "..." if len(completed_section["content"]) > 200 else completed_section["content"]
            previous_sections_summary.append(f"{completed_section.get('title', '')}: {summary}")

    # å‡†å¤‡åç»­ç« èŠ‚å¤§çº²
    all_sections = main_state.get("outline", {}).get("sections", [])
    upcoming_sections_outline = []
    for i in range(section_index + 1, len(all_sections)):
        if i < len(all_sections):
            upcoming_sections_outline.append(f"{all_sections[i].get('title', '')}: {all_sections[i].get('description', '')}")

    # ä½¿ç”¨å­å›¾çš„çŠ¶æ€åˆ›å»ºå‡½æ•°
    return create_intelligent_initial_state(
        topic=main_state.get("topic", ""),
        section=section,
        previous_sections_summary=previous_sections_summary,
        upcoming_sections_outline=upcoming_sections_outline,
        report_main_thread=main_state.get("outline", {}).get("executive_summary", "") if main_state.get("outline") else "",
        writing_style=main_state.get("writing_style", "professional"),
        quality_threshold=0.8,
        max_iterations=3
    )

async def call_intelligent_subgraph(subgraph_state: Dict[str, Any]) -> Dict[str, Any]:
    """è°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾"""
    try:
        # è·å–å­å›¾å®ä¾‹
        subgraph = get_intelligent_section_subgraph()

        # è°ƒç”¨å­å›¾
        result = await subgraph.ainvoke(subgraph_state)

        return result

    except Exception as e:
        logger.error(f"è°ƒç”¨æ™ºèƒ½å­å›¾å¤±è´¥: {e}")
        return {}

def convert_research_data_to_results(research_data: List[Dict[str, Any]]) -> List[ResearchResult]:
    """å°†å­å›¾çš„ç ”ç©¶æ•°æ®è½¬æ¢ä¸ºä¸»å›¾çš„ ResearchResult æ ¼å¼"""
    results = []

    for item in research_data:
        try:
            result = ResearchResult(
                id=item.get("id", str(uuid.uuid4())),
                query=item.get("query", ""),
                source_type="web",
                title=item.get("title", ""),
                content=item.get("content", ""),
                url=item.get("url", ""),
                relevance_score=item.get("relevance_score", 0.8),
                timestamp=item.get("timestamp", time.time()),
                section_id=item.get("section_id", "")
            )
            results.append(result)
        except Exception as e:
            logger.warning(f"è½¬æ¢ç ”ç©¶æ•°æ®å¤±è´¥: {e}")
            continue

    return results

async def intelligent_section_processing_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """
    æ™ºèƒ½ç« èŠ‚å¤„ç†èŠ‚ç‚¹ - é€ä¸ªè°ƒç”¨å­å›¾å¤„ç†æ¯ä¸ªç« èŠ‚

    è¿™ä¸ªèŠ‚ç‚¹çš„ä½œç”¨ï¼š
    1. è·å–å¤§çº²ä¸­çš„æ‰€æœ‰ç« èŠ‚
    2. é€ä¸ªè°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
    3. æ¯ä¸ªç« èŠ‚éƒ½ç»è¿‡å®Œæ•´çš„ï¼šç ”ç©¶â†’åˆ†æâ†’ç”Ÿæˆâ†’è´¨é‡è¯„ä¼°â†’ä¼˜åŒ–æµç¨‹
    4. æ±‡æ€»æ‰€æœ‰ç« èŠ‚å½¢æˆå®Œæ•´æŠ¥å‘Š
    """
    writer = safe_get_stream_writer()
    writer({
        "step": "intelligent_section_processing",
        "status": "ğŸ§  å¼€å§‹æ™ºèƒ½ç« èŠ‚å¤„ç†ï¼ˆé›†æˆå­å›¾ï¼‰",
        "progress": 0
    })

    try:
        outline = state.get("outline", {})
        sections = outline.get("sections", []) if outline else []

        if not sections:
            writer({
                "step": "intelligent_section_processing",
                "status": "âŒ æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚ä¿¡æ¯",
                "progress": -1
            })
            return state

        completed_sections = []
        all_research_data = []

        writer({
            "step": "intelligent_section_processing",
            "status": f"ğŸ“š å‡†å¤‡å¤„ç† {len(sections)} ä¸ªç« èŠ‚ï¼Œæ¯ä¸ªç« èŠ‚éƒ½å°†ç»è¿‡å®Œæ•´çš„æ™ºèƒ½ç ”ç©¶æµç¨‹",
            "progress": 10,
            "total_sections": len(sections)
        })

        # é€ä¸ªå¤„ç†æ¯ä¸ªç« èŠ‚
        for section_index, section in enumerate(sections):
            writer({
                "step": "intelligent_section_processing",
                "status": f"ğŸ”¬ å¤„ç†ç« èŠ‚ {section_index + 1}/{len(sections)}: {section.get('title', 'æœªçŸ¥ç« èŠ‚')}",
                "progress": 10 + (section_index / len(sections)) * 80,
                "current_section": section.get('title', 'æœªçŸ¥ç« èŠ‚'),
                "section_index": section_index + 1
            })

            # å‡†å¤‡å­å›¾è¾“å…¥çŠ¶æ€
            subgraph_state = prepare_subgraph_state(state, section, section_index, completed_sections)

            # è°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
            subgraph_result = await call_intelligent_subgraph(subgraph_state)

            if subgraph_result and subgraph_result.get("final_content"):
                # æˆåŠŸå¤„ç†ç« èŠ‚
                section_result = {
                    **section,
                    "content": subgraph_result["final_content"],
                    "research_data": subgraph_result.get("research_data", {}),
                    "quality_metrics": subgraph_result.get("quality_metrics", {}),
                    "processing_time": subgraph_result.get("processing_time", 0),
                    "iteration_count": subgraph_result.get("iteration_count", 0),
                    "status": "completed"
                }
                completed_sections.append(section_result)

                # æ”¶é›†ç ”ç©¶æ•°æ®
                research_data = subgraph_result.get("research_data", {})
                if research_data.get("initial_research_results"):
                    all_research_data.extend(research_data["initial_research_results"])
                if research_data.get("supplementary_research_results"):
                    all_research_data.extend(research_data["supplementary_research_results"])

                writer({
                    "step": "intelligent_section_processing",
                    "status": f"âœ… ç« èŠ‚å®Œæˆ: {section.get('title', 'æœªçŸ¥ç« èŠ‚')} (è´¨é‡: {subgraph_result.get('quality_metrics', {}).get('final_quality_score', 0):.2f})",
                    "progress": 10 + ((section_index + 1) / len(sections)) * 80,
                    "completed_sections": len(completed_sections),
                    "quality_score": subgraph_result.get('quality_metrics', {}).get('final_quality_score', 0)
                })
            else:
                # ç« èŠ‚å¤„ç†å¤±è´¥
                writer({
                    "step": "intelligent_section_processing",
                    "status": f"âš ï¸ ç« èŠ‚å¤„ç†å¤±è´¥: {section.get('title', 'æœªçŸ¥ç« èŠ‚')}",
                    "progress": 10 + ((section_index + 1) / len(sections)) * 80
                })
                logger.warning(f"ç« èŠ‚å¤„ç†å¤±è´¥: {section.get('title', 'æœªçŸ¥ç« èŠ‚')}")

        # æ›´æ–°ä¸»å›¾çŠ¶æ€
        state["sections"] = completed_sections
        state["research_results"] = convert_research_data_to_results(all_research_data)
        state["content_creation_completed"] = True
        state["completed_sections_count"] = len(completed_sections)

        # è®¡ç®—æ•´ä½“è´¨é‡
        avg_quality = sum(s.get("quality_metrics", {}).get("final_quality_score", 0) for s in completed_sections) / max(len(completed_sections), 1)

        writer({
            "step": "intelligent_section_processing",
            "status": f"ğŸ‰ æ™ºèƒ½ç« èŠ‚å¤„ç†å®Œæˆï¼æˆåŠŸå¤„ç† {len(completed_sections)}/{len(sections)} ä¸ªç« èŠ‚",
            "progress": 100,
            "completed_sections": len(completed_sections),
            "total_sections": len(sections),
            "average_quality": avg_quality,
            "total_research_items": len(all_research_data)
        })

        logger.info(f"æ™ºèƒ½ç« èŠ‚å¤„ç†å®Œæˆ: {len(completed_sections)}/{len(sections)} ä¸ªç« èŠ‚, å¹³å‡è´¨é‡: {avg_quality:.3f}")
        return state

    except Exception as e:
        logger.error(f"æ™ºèƒ½ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")
        writer({
            "step": "intelligent_section_processing",
            "status": f"âŒ ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"æ™ºèƒ½ç« èŠ‚å¤„ç†é”™è¯¯: {str(e)}"]
        return state

def create_specialized_agents():
    """åˆ›å»ºä¸“ä¸šåŒ–Agent"""
    llm = create_llm()
    
    # ç ”ç©¶ä¸“å®¶Agent
    researcher_agent = create_react_agent(
        llm,
        tools=get_research_tools(),
        prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶ä¸“å®¶ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
        1. æ·±åº¦æœç´¢å’Œæ”¶é›†ç›¸å…³ä¿¡æ¯
        2. è¯„ä¼°ä¿¡æ¯æ¥æºçš„å¯ä¿¡åº¦å’Œç›¸å…³æ€§
        3. è¯†åˆ«å…³é”®è¶‹åŠ¿å’Œå‘å±•æ¨¡å¼
        4. æä¾›å…¨é¢ä¸”å‡†ç¡®çš„ç ”ç©¶æ•°æ®
        
        å·¥ä½œåŸåˆ™ï¼š
        - ä½¿ç”¨å¤šä¸ªå¯ä¿¡æ¥æºéªŒè¯ä¿¡æ¯
        - å…³æ³¨æœ€æ–°å‘å±•å’Œè¶‹åŠ¿
        - ä¿æŒå®¢è§‚å’Œæ‰¹åˆ¤æ€§æ€ç»´
        - æä¾›è¯¦ç»†çš„æ•°æ®æ”¯æ’‘
        
        è¯·ç¡®ä¿ç ”ç©¶çš„æ·±åº¦å’Œå¹¿åº¦ï¼Œä¸ºåç»­åˆ†ææä¾›å……åˆ†çš„æ•°æ®åŸºç¡€ã€‚"""
    )
    
    # åˆ†æä¸“å®¶Agent
    analyst_agent = create_react_agent(
        llm,
        tools=get_analysis_tools(),
        prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åˆ†æä¸“å®¶ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
        1. æ·±åº¦åˆ†æç ”ç©¶æ•°æ®å’Œä¿¡æ¯
        2. è¯†åˆ«å…³é”®æ´å¯Ÿã€æ¨¡å¼å’Œè¶‹åŠ¿
        3. è¿›è¡Œé¢„æµ‹æ€§åˆ†æå’Œé£é™©è¯„ä¼°
        4. æä¾›æ•°æ®é©±åŠ¨çš„ç»“è®ºå’Œå»ºè®®
        
        åˆ†æé‡ç‚¹ï¼š
        - è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
        - å› æœå…³ç³»åˆ†æ
        - å½±å“å› ç´ è¯„ä¼°
        - é¢„æµ‹æ€§æ´å¯Ÿç”Ÿæˆ
        
        è¯·è¿ç”¨ä¸¥æ ¼çš„åˆ†ææ–¹æ³•ï¼Œç¡®ä¿ç»“è®ºçš„ç§‘å­¦æ€§å’Œå¯é æ€§ã€‚"""
    )
    
    # å†™ä½œä¸“å®¶Agent
    writer_agent = create_react_agent(
        llm,
        tools=get_writing_tools(),
        prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯å†™ä½œä¸“å®¶ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
        1. å°†å¤æ‚ç ”ç©¶å’Œåˆ†æè½¬åŒ–ä¸ºæ¸…æ™°çš„å†…å®¹
        2. ç¡®ä¿å†…å®¹ç»“æ„åˆç†ã€é€»è¾‘æ¸…æ™°
        3. é€‚åº”ä¸åŒè¯»è€…ç¾¤ä½“çš„éœ€æ±‚
        4. ä¿æŒä¸€è‡´çš„ä¸“ä¸šå†™ä½œé£æ ¼
        
        å†™ä½œæ ‡å‡†ï¼š
        - ç»“æ„æ¸…æ™°ã€å±‚æ¬¡åˆ†æ˜
        - è¯­è¨€å‡†ç¡®ã€è¡¨è¾¾æµç•…
        - é€»è¾‘ä¸¥å¯†ã€è®ºè¯å……åˆ†
        - é‡ç‚¹çªå‡ºã€æ˜“äºç†è§£
        
        è¯·ç¡®ä¿ç”Ÿæˆçš„å†…å®¹å…·æœ‰ä¸“ä¸šæ€§ã€å¯è¯»æ€§å’Œå®ç”¨æ€§ã€‚"""
    )
    
    # éªŒè¯ä¸“å®¶Agent
    validator_agent = create_react_agent(
        llm,
        tools=get_validation_tools(),
        prompt="""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è´¨é‡éªŒè¯ä¸“å®¶ã€‚ä½ çš„èŒè´£åŒ…æ‹¬ï¼š
        1. éªŒè¯æŠ¥å‘Šå†…å®¹çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
        2. æ£€æŸ¥é€»è¾‘ä¸€è‡´æ€§å’Œè®ºè¯ä¸¥å¯†æ€§
        3. è¯„ä¼°å†…å®¹è´¨é‡å’Œå¯è¯»æ€§
        4. è¯†åˆ«é—®é¢˜å¹¶æä¾›æ”¹è¿›å»ºè®®
        
        éªŒè¯æ ‡å‡†ï¼š
        - å†…å®¹å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
        - é€»è¾‘ä¸€è‡´æ€§å’Œè¿è´¯æ€§
        - ç»“æ„åˆç†æ€§å’Œå¯è¯»æ€§
        - ä¸“ä¸šæ€§å’Œæƒå¨æ€§
        
        è¯·ä¸¥æ ¼æŒ‰ç…§è´¨é‡æ ‡å‡†è¿›è¡ŒéªŒè¯ï¼Œç¡®ä¿æŠ¥å‘Šçš„ä¸“ä¸šæ°´å‡†ã€‚"""
    )
    
    return {
        AgentType.RESEARCHER: researcher_agent,
        AgentType.ANALYST: analyst_agent,
        AgentType.WRITER: writer_agent,
        AgentType.VALIDATOR: validator_agent
    }

# ============================================================================
# æ™ºèƒ½åè°ƒèŠ‚ç‚¹
# ============================================================================

async def intelligent_supervisor_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """
    æ™ºèƒ½ç›‘ç£åè°ƒèŠ‚ç‚¹
    è´Ÿè´£æ•´ä½“ä»»åŠ¡è§„åˆ’ã€Agentè°ƒåº¦å’Œæµç¨‹æ§åˆ¶
    """
    writer = safe_get_stream_writer()
    writer({
        "step": "supervision",
        "status": "å¼€å§‹æ™ºèƒ½ä»»åŠ¡åè°ƒ",
        "progress": 0,
        "session_id": state["session_id"]
    })
    
    try:
        start_time = time.time()
        llm = create_llm()
        
        # æ„å»ºåè°ƒåˆ†ææç¤º
        supervision_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½çš„æŠ¥å‘Šç”Ÿæˆç›‘ç£åè°ƒå™¨ã€‚åˆ†æå½“å‰çŠ¶æ€å¹¶å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
            
            å¯ç”¨çš„Agentç±»å‹ï¼š
            - researcher: æ·±åº¦ç ”ç©¶å’Œä¿¡æ¯æ”¶é›†
            - analyst: æ•°æ®åˆ†æå’Œæ´å¯Ÿç”Ÿæˆ
            - writer: å†…å®¹å†™ä½œå’ŒæŠ¥å‘Šç”Ÿæˆ
            - validator: è´¨é‡éªŒè¯å’Œæ”¹è¿›
            
            å¯ç”¨çš„è¡ŒåŠ¨ï¼š
            - outline_generation: ç”ŸæˆæŠ¥å‘Šå¤§çº²
            - research_execution: æ‰§è¡Œæ·±åº¦ç ”ç©¶
            - analysis_generation: ç”Ÿæˆåˆ†ææ´å¯Ÿ
            - content_creation: åˆ›å»ºæŠ¥å‘Šå†…å®¹
            - quality_validation: è´¨é‡éªŒè¯
            - finish: å®ŒæˆæŠ¥å‘Šç”Ÿæˆ
            
            è¯·åˆ†æå½“å‰çŠ¶æ€å¹¶è¿”å›æ¨èçš„ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼ˆåªè¿”å›è¡ŒåŠ¨åç§°ï¼‰ã€‚
            """),
            ("human", """
            å½“å‰çŠ¶æ€åˆ†æï¼š
            - ä¸»é¢˜ï¼š{topic}
            - å½“å‰æ­¥éª¤ï¼š{current_step}
            - æ‰§è¡Œè·¯å¾„ï¼š{execution_path}
            - å·²æœ‰å¤§çº²ï¼š{has_outline}
            - ç ”ç©¶ç»“æœæ•°ï¼š{research_count}
            - åˆ†ææ´å¯Ÿæ•°ï¼š{insights_count}
            - å·²æœ‰æœ€ç»ˆæŠ¥å‘Šï¼š{has_final_report}
            - è¿è¡Œæ¨¡å¼ï¼š{mode}
            
            è¯·æ ¹æ®ä»¥ä¸Šä¿¡æ¯æ¨èä¸‹ä¸€æ­¥è¡ŒåŠ¨ã€‚
            """)
        ])
        
        # å‡†å¤‡çŠ¶æ€åˆ†ææ•°æ®
        analysis_data = {
            "topic": state["topic"],
            "current_step": state["current_step"],
            "execution_path": " â†’ ".join(state["execution_path"]) if state["execution_path"] else "æ— ",
            "has_outline": "æ˜¯" if state["outline"] else "å¦",
            "research_count": len(state["research_results"]),
            "insights_count": len(state["analysis_insights"]),
            "has_final_report": "æ˜¯" if state["final_report"] else "å¦",
            "mode": state["mode"]
        }
        
        writer({
            "step": "supervision",
            "status": "åˆ†æå½“å‰çŠ¶æ€...",
            "progress": 30,
            "analysis_data": analysis_data
        })
        
        # æµå¼ç”Ÿæˆåè°ƒå†³ç­–
        full_response = ""
        chunk_count = 0
        async for chunk in llm.astream(supervision_prompt.format_messages(**analysis_data), config=config):
            if chunk.content:
                full_response += chunk.content
                chunk_count += 1
                
                # å‡å°‘è¿›åº¦æ›´æ–°é¢‘ç‡ï¼Œåªåœ¨ç‰¹å®šçš„chunkæ•°æ—¶æ›´æ–°
                if chunk_count % 5 == 0:
                    writer({
                        "step": "supervision",
                        "status": "æ­£åœ¨åˆ¶å®šåè°ƒå†³ç­–...",
                        "progress": min(80, 60 + (chunk_count // 5) * 4),
                        "reasoning": full_response[:200] + "..." if len(full_response) > 200 else full_response
                    })
        
        # è§£æå†³ç­–
        decision_text = full_response.lower().strip()
        
        # æ™ºèƒ½å†³ç­–é€»è¾‘
        if not state["outline"]:
            next_action = "outline_generation"
            reasoning = "éœ€è¦é¦–å…ˆç”ŸæˆæŠ¥å‘Šå¤§çº²"
        elif len(state["research_results"]) < 5:  # éœ€è¦è¶³å¤Ÿçš„ç ”ç©¶æ•°æ®
            next_action = "research_execution"
            reasoning = "éœ€è¦è¿›è¡Œæ·±åº¦ç ”ç©¶æ”¶é›†æ•°æ®"
        elif len(state["analysis_insights"]) < 3:  # éœ€è¦åˆ†ææ´å¯Ÿ
            next_action = "analysis_generation"
            reasoning = "éœ€è¦ç”Ÿæˆåˆ†ææ´å¯Ÿ"
        elif not state["final_report"]:
            next_action = "content_creation"
            reasoning = "éœ€è¦åˆ›å»ºæœ€ç»ˆæŠ¥å‘Šå†…å®¹"
        elif state["final_report"] and not state.get("validated", False):
            next_action = "quality_validation"
            reasoning = "éœ€è¦è¿›è¡Œè´¨é‡éªŒè¯"
        else:
            next_action = "finish"
            reasoning = "æŠ¥å‘Šç”Ÿæˆæµç¨‹å®Œæˆ"
        
        # åŸºäºå“åº”å†…å®¹è¿›è¡Œå¾®è°ƒ
        if "outline" in decision_text and not state["outline"]:
            next_action = "outline_generation"
        elif "research" in decision_text:
            next_action = "research_execution"
        elif "analysis" in decision_text or "insight" in decision_text:
            next_action = "analysis_generation"
        elif "content" in decision_text or "write" in decision_text:
            next_action = "content_creation"
        elif "validation" in decision_text or "quality" in decision_text:
            next_action = "quality_validation"
        elif "finish" in decision_text or "complete" in decision_text:
            next_action = "finish"
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "supervisor", execution_time)
        
        state["current_step"] = f"supervised_{next_action}"
        state["execution_path"] = state["execution_path"] + ["supervisor"]
        state["agent_results"]["supervisor"] = {
            "next_action": next_action,
            "reasoning": reasoning,
            "execution_time": execution_time
        }
        
        # æ·»åŠ ç›‘ç£æ¶ˆæ¯
        supervision_message = f"""
        ğŸ§  æ™ºèƒ½ç›‘ç£åè°ƒå®Œæˆï¼š
        
        ğŸ“Š å½“å‰çŠ¶æ€åˆ†æï¼š
        - å¤§çº²çŠ¶æ€ï¼š{'å·²ç”Ÿæˆ' if state['outline'] else 'å¾…ç”Ÿæˆ'}
        - ç ”ç©¶æ•°æ®ï¼š{len(state['research_results'])}æ¡
        - åˆ†ææ´å¯Ÿï¼š{len(state['analysis_insights'])}ä¸ª
        - æŠ¥å‘ŠçŠ¶æ€ï¼š{'å·²ç”Ÿæˆ' if state['final_report'] else 'å¾…ç”Ÿæˆ'}
        
        ğŸ¯ å†³ç­–ç»“æœï¼š
        - ä¸‹ä¸€æ­¥è¡ŒåŠ¨ï¼š{next_action}
        - å†³ç­–ç†ç”±ï¼š{reasoning}
        - æ‰§è¡Œæ—¶é—´ï¼š{execution_time:.2f}ç§’
        """
        
        state["messages"] = state["messages"] + [AIMessage(content=supervision_message)]
        
        writer({
            "step": "supervision",
            "status": "ç›‘ç£åè°ƒå®Œæˆ",
            "progress": 100,
            "next_action": next_action,
            "reasoning": reasoning,
            "execution_time": execution_time
        })
        
        logger.info(f"ç›‘ç£åè°ƒå®Œæˆ: {next_action} - {reasoning}")
        return state
        
    except Exception as e:
        logger.error(f"ç›‘ç£åè°ƒå¤±è´¥: {str(e)}")
        writer({
            "step": "supervision",
            "status": f"ç›‘ç£åè°ƒå¤±è´¥: {str(e)}",
            "progress": -1
        })
        
        state["error_log"] = state["error_log"] + [f"ç›‘ç£åè°ƒé”™è¯¯: {str(e)}"]
        state["current_step"] = "supervision_failed"
        return state

# ============================================================================
# å¤§çº²ç”ŸæˆèŠ‚ç‚¹
# ============================================================================

async def outline_generation_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """å¤§çº²ç”ŸæˆèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "outline_generation",
        "status": "å¼€å§‹ç”Ÿæˆæ·±åº¦ç ”ç©¶å¤§çº²",
        "progress": 0
    })
    
    try:
        start_time = time.time()
        llm = create_llm()
        parser = JsonOutputParser(pydantic_object=ReportOutline)
        
        # æ„å»ºé«˜çº§å¤§çº²ç”Ÿæˆæç¤º
        outline_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šå¤§çº²è®¾è®¡ä¸“å®¶ã€‚è¯·ç”Ÿæˆè¯¦ç»†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚
            
            è¦æ±‚ï¼š
            1. å¤§çº²ç¬¦åˆ{report_type}æŠ¥å‘Šçš„æ ‡å‡†ç»“æ„
            2. é’ˆå¯¹{target_audience}è¯»è€…ç¾¤ä½“è®¾è®¡
            3. ç ”ç©¶æ·±åº¦ä¸º{depth_level}çº§åˆ«
            4. ç›®æ ‡å­—æ•°çº¦{target_length}å­—
            5. æ¯ä¸ªç« èŠ‚åŒ…å«ç ”ç©¶æŸ¥è¯¢å…³é”®è¯
            6. ç« èŠ‚ä¼˜å…ˆçº§åˆç†åˆ†é…
            
            {format_instructions}"""),
            ("human", """
            è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸“ä¸šçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š
            
            ç ”ç©¶ä¸»é¢˜ï¼š{topic}
            æŠ¥å‘Šç±»å‹ï¼š{report_type}
            ç›®æ ‡è¯»è€…ï¼š{target_audience}
            æ·±åº¦çº§åˆ«ï¼š{depth_level}
            
            è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€ä¾¿äºæ·±åº¦ç ”ç©¶ã€‚
            """)
        ])
        
        input_data = {
            "topic": state["topic"],
            "report_type": state["report_type"],
            "target_audience": state["target_audience"],
            "depth_level": state["depth_level"],
            "target_length": state["target_length"],
            "format_instructions": parser.get_format_instructions()
        }
        
        writer({
            "step": "outline_generation",
            "status": "æ­£åœ¨ç”Ÿæˆä¸“ä¸šå¤§çº²...",
            "progress": 30
        })
        
        # åˆ›å»ºLLMé“¾å¹¶æµå¼æ‰§è¡Œ
        llm_chain = outline_prompt | llm | parser
        
        outline_data = None
        chunk_count = 0
        current_outline_display = ""
        
        async for chunk in llm_chain.astream(input_data, config=config):
            outline_data = chunk
            chunk_count += 1
            # å®æ—¶æ˜¾ç¤ºå¤§çº²å†…å®¹ï¼ˆæ¯5ä¸ªchunkæ›´æ–°ä¸€æ¬¡ä»¥å‡å°‘é¢‘ç‡ï¼‰
            if chunk_count % 5 == 0:
                # æ„å»ºå½“å‰å¤§çº²çš„æ˜¾ç¤ºæ–‡æœ¬
                if hasattr(chunk, 'title'):
                    current_outline_display = f"ğŸ¯ æ ‡é¢˜ï¼š{chunk.title}\n"
                if hasattr(chunk, 'sections') and chunk.sections:
                    current_outline_display += f"ğŸ“š ç« èŠ‚({len(chunk.sections)}ä¸ª):\n"
                    for i, section in enumerate(chunk.sections[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç« èŠ‚
                        if hasattr(section, 'title'):
                            current_outline_display += f"  {i}. {section.title}\n"
                            if hasattr(section, 'description'):
                                current_outline_display += f"     {section.description}\n"
                    if len(chunk.sections) > 3:
                        current_outline_display += f"  ... è¿˜æœ‰{len(chunk.sections)-3}ä¸ªç« èŠ‚"
                
                writer({
                    "step": "outline_generation",
                    "status": "æ­£åœ¨æ„å»ºå¤§çº²ç»“æ„...",
                    "content": chunk,
                    "progress": min(90, 30 + (chunk_count // 5) * 10),
                    "current_outline": current_outline_display,
                    "chunk_count": chunk_count
                })
                
                # å¦‚æœå¤§çº²åŸºæœ¬å®Œæ•´ï¼Œæå‰æ˜¾ç¤º
                if hasattr(chunk, 'title') and hasattr(chunk, 'sections') and len(chunk.sections) >= 3:
                    writer({
                        "step": "outline_generation",
                        "status": "å¤§çº²ç»“æ„å·²ç”Ÿæˆï¼Œæ­£åœ¨å®Œå–„ç»†èŠ‚...",
                        "progress": 85,
                        "partial_outline": chunk,
                        "streaming_content": current_outline_display
                    })
        
        # å¤„ç†ç”Ÿæˆç»“æœ
        if not outline_data:
            # åˆ›å»ºé»˜è®¤å¤§çº²
            outline_data = ReportOutline(
                title=f"{state['topic']} - æ·±åº¦ç ”ç©¶æŠ¥å‘Š",
                executive_summary=f"æœ¬æŠ¥å‘Šå¯¹{state['topic']}è¿›è¡Œå…¨é¢æ·±å…¥çš„ç ”ç©¶åˆ†æï¼Œä¸º{state['target_audience']}æä¾›ä¸“ä¸šæ´å¯Ÿã€‚",
                sections=[
                    ReportSection(
                        id="background",
                        title="ç ”ç©¶èƒŒæ™¯ä¸ç°çŠ¶",
                        description="åˆ†æç ”ç©¶èƒŒæ™¯ã€å‘å±•å†ç¨‹å’Œå½“å‰çŠ¶æ€",
                        key_points=["å†å²å‘å±•", "ç°çŠ¶åˆ†æ", "å…³é”®ç‰¹å¾"],
                        research_queries=[f"{state['topic']} å‘å±•å†å²", f"{state['topic']} ç°çŠ¶åˆ†æ", f"{state['topic']} å¸‚åœºè§„æ¨¡"],
                        priority=5
                    ),
                    ReportSection(
                        id="deep_analysis",
                        title="æ·±åº¦åˆ†æä¸æ´å¯Ÿ",
                        description="è¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿å’Œæ¨¡å¼",
                        key_points=["æ ¸å¿ƒé©±åŠ¨å› ç´ ", "å‘å±•è¶‹åŠ¿", "å½±å“å› ç´ "],
                        research_queries=[f"{state['topic']} è¶‹åŠ¿åˆ†æ", f"{state['topic']} å½±å“å› ç´ ", f"{state['topic']} å‘å±•é¢„æµ‹"],
                        priority=5
                    ),
                    ReportSection(
                        id="case_studies",
                        title="æ¡ˆä¾‹ç ”ç©¶ä¸åº”ç”¨",
                        description="åˆ†æå…¸å‹æ¡ˆä¾‹å’Œå®é™…åº”ç”¨æƒ…å†µ",
                        key_points=["æˆåŠŸæ¡ˆä¾‹", "åº”ç”¨åœºæ™¯", "å®æ–½ç»éªŒ"],
                        research_queries=[f"{state['topic']} æ¡ˆä¾‹ç ”ç©¶", f"{state['topic']} åº”ç”¨å®ä¾‹", f"{state['topic']} æœ€ä½³å®è·µ"],
                        priority=4
                    ),
                    ReportSection(
                        id="challenges_opportunities",
                        title="æŒ‘æˆ˜ä¸æœºé‡åˆ†æ",
                        description="è¯†åˆ«é¢ä¸´çš„æŒ‘æˆ˜å’Œå‘å±•æœºé‡",
                        key_points=["ä¸»è¦æŒ‘æˆ˜", "å‘å±•æœºé‡", "é£é™©è¯„ä¼°"],
                        research_queries=[f"{state['topic']} æŒ‘æˆ˜åˆ†æ", f"{state['topic']} å‘å±•æœºä¼š", f"{state['topic']} é£é™©è¯„ä¼°"],
                        priority=4
                    ),
                    ReportSection(
                        id="future_outlook",
                        title="æœªæ¥å±•æœ›ä¸å»ºè®®",
                        description="é¢„æµ‹æœªæ¥å‘å±•å¹¶æå‡ºä¸“ä¸šå»ºè®®",
                        key_points=["å‘å±•é¢„æµ‹", "æˆ˜ç•¥å»ºè®®", "è¡ŒåŠ¨è®¡åˆ’"],
                        research_queries=[f"{state['topic']} æœªæ¥å‘å±•", f"{state['topic']} å‘å±•å»ºè®®", f"{state['topic']} æˆ˜ç•¥è§„åˆ’"],
                        priority=3
                    )
                ],
                methodology="é‡‡ç”¨æ–‡çŒ®ç ”ç©¶ã€æ¡ˆä¾‹åˆ†æã€è¶‹åŠ¿é¢„æµ‹å’Œä¸“å®¶æ´å¯Ÿç›¸ç»“åˆçš„ç»¼åˆç ”ç©¶æ–¹æ³•",
                target_audience=state["target_audience"],
                estimated_length=state["target_length"]
            )
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if hasattr(outline_data, 'dict'):
            outline_dict = outline_data.dict()
        else:
            outline_dict = dict(outline_data) if outline_data else {}
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "outline_generator", execution_time)
        update_task_status(state, "outline_generation", TaskStatus.COMPLETED)
        
        state["outline"] = outline_dict
        state["current_step"] = "outline_generated"
        state["execution_path"] = state["execution_path"] + ["outline_generation"]
        
        # åˆ›å»ºå¤§çº²å±•ç¤ºæ¶ˆæ¯
        sections_text = "\n".join([
            f"  {i+1}. {section['title']}\n     {section['description']}\n     å…³é”®è¯: {', '.join(section['research_queries'][:3])}"
            for i, section in enumerate(outline_dict.get("sections", []))
        ])
        
        outline_message = f"""
        ğŸ“‹ æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆï¼š
        
        ğŸ¯ æŠ¥å‘Šæ ‡é¢˜ï¼š{outline_dict.get('title', 'æœªçŸ¥')}
        
        ğŸ“ æ‰§è¡Œæ‘˜è¦ï¼š
        {outline_dict.get('executive_summary', 'æ— æ‘˜è¦')}
        
        ğŸ“š ç ”ç©¶ç« èŠ‚ï¼š
        {sections_text}
        
        ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline_dict.get('methodology', 'æœªæŒ‡å®š')}
        ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline_dict.get('estimated_length', 0):,}å­—
        â±ï¸ ç”Ÿæˆæ—¶é—´ï¼š{execution_time:.2f}ç§’
        """
        
        state["messages"] = state["messages"] + [AIMessage(content=outline_message)]
        
        writer({
            "step": "outline_generation",
            "status": "æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆ",
            "progress": 100,
            "sections_count": len(outline_dict.get("sections", [])),
            "execution_time": execution_time,
            "content": {
                "type": "outline",
                "data": outline_dict,
                "display_text": outline_message
            }
        })
        
        logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_dict.get('sections', []))}ä¸ªç« èŠ‚")
        return state
        
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        writer({
            "step": "outline_generation",
            "status": f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}",
            "progress": -1
        })
        
        state["error_log"] = state["error_log"] + [f"å¤§çº²ç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "outline_generation_failed"
        update_task_status(state, "outline_generation", TaskStatus.FAILED)
        return state

# ============================================================================
# äº¤äº’ç¡®è®¤èŠ‚ç‚¹
# ============================================================================

def create_interaction_node(interaction_type: InteractionType):
    """åˆ›å»ºäº¤äº’ç¡®è®¤èŠ‚ç‚¹çš„å·¥å‚å‡½æ•°"""
    
    def interaction_node(state: DeepResearchState) -> DeepResearchState:
        """é€šç”¨äº¤äº’ç¡®è®¤èŠ‚ç‚¹"""
        writer = safe_get_stream_writer()
        
        interaction_config = get_interaction_config(interaction_type)
        mode = state["mode"]
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": f"å¤„ç†{interaction_config['title']}",
            "progress": 0,
            "interaction_type": interaction_type.value,
            "mode": mode.value
        })
        
        # Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡
        if mode == ReportMode.COPILOT:
            state["approval_status"][interaction_type.value] = True
            state["user_feedback"][interaction_type.value] = {"approved": True, "auto": True}
            
            writer({
                "step": f"interaction_{interaction_type.value}",
                "status": "Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡",
                "progress": 100,
                "auto_approved": True
            })
            
            state["messages"] = state["messages"] + [
                AIMessage(content=f"ğŸ¤– Copilotæ¨¡å¼ï¼š{interaction_config['copilot_message']}")
            ]
            
            return state
        
        # äº¤äº’æ¨¡å¼éœ€è¦ç”¨æˆ·ç¡®è®¤
        message_content = format_interaction_message(state, interaction_type, interaction_config)
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": "ç­‰å¾…ç”¨æˆ·ç¡®è®¤",
            "progress": 50,
            "awaiting_user_input": True
        })
        
        # ä½¿ç”¨interruptç­‰å¾…ç”¨æˆ·è¾“å…¥
        user_response = interrupt({
            "type": interaction_type.value,
            "title": interaction_config["title"],
            "message": message_content,
            "options": interaction_config["options"],
            "default": interaction_config.get("default", "continue")
        })
        
        # å¤„ç†ç”¨æˆ·å“åº”
        approved = user_response.get("approved", True) if isinstance(user_response, dict) else True
        state["approval_status"][interaction_type.value] = approved
        state["user_feedback"][interaction_type.value] = user_response
        
        # è®°å½•äº¤äº’å†å²
        add_user_interaction(state, interaction_type.value, user_response)
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": "ç”¨æˆ·ç¡®è®¤å®Œæˆ",
            "progress": 100,
            "user_response": user_response,
            "approved": approved
        })
        
        # æ·»åŠ ç¡®è®¤æ¶ˆæ¯
        status_emoji = "âœ…" if approved else "âŒ"
        confirmation_message = f"{status_emoji} {interaction_config['title']}ï¼š{'ç¡®è®¤é€šè¿‡' if approved else 'è¢«æ‹’ç»'}"
        
        state["messages"] = state["messages"] + [AIMessage(content=confirmation_message)]
        
        return state
    
    return interaction_node

def get_interaction_config(interaction_type: InteractionType) -> Dict[str, Any]:
    """è·å–äº¤äº’é…ç½®"""
    configs = {
        InteractionType.OUTLINE_CONFIRMATION: {
            "title": "å¤§çº²ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤æŠ¥å‘Šå¤§çº²ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶",
            "options": ["ç¡®è®¤ç»§ç»­", "ä¿®æ”¹å¤§çº²", "é‡æ–°ç”Ÿæˆ"],
            "default": "ç¡®è®¤ç»§ç»­"
        },
        InteractionType.RESEARCH_PERMISSION: {
            "title": "ç ”ç©¶æƒé™ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨å…è®¸è¿›è¡Œæ·±åº¦ç ”ç©¶å’Œæ•°æ®æ”¶é›†",
            "options": ["å…è®¸ç ”ç©¶", "è·³è¿‡ç ”ç©¶", "é™åˆ¶èŒƒå›´"],
            "default": "å…è®¸ç ”ç©¶"
        },
        InteractionType.ANALYSIS_APPROVAL: {
            "title": "åˆ†æç»“æœå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤åˆ†æç»“æœï¼Œç»§ç»­å†…å®¹ç”Ÿæˆ",
            "options": ["ç¡®è®¤åˆ†æ", "é‡æ–°åˆ†æ", "è°ƒæ•´æ–¹å‘"],
            "default": "ç¡®è®¤åˆ†æ"
        },
        InteractionType.CONTENT_REVIEW: {
            "title": "å†…å®¹å®¡æŸ¥",
            "copilot_message": "è‡ªåŠ¨é€šè¿‡å†…å®¹å®¡æŸ¥ï¼Œå‡†å¤‡æœ€ç»ˆæŠ¥å‘Š",
            "options": ["é€šè¿‡å®¡æŸ¥", "ä¿®æ”¹å†…å®¹", "é‡å†™ç« èŠ‚"],
            "default": "é€šè¿‡å®¡æŸ¥"
        },
        InteractionType.FINAL_APPROVAL: {
            "title": "æœ€ç»ˆå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨å®Œæˆæœ€ç»ˆå®¡æ‰¹ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "options": ["æœ€ç»ˆç¡®è®¤", "å†æ¬¡ä¿®æ”¹", "ç”Ÿæˆæ–°ç‰ˆæœ¬"],
            "default": "æœ€ç»ˆç¡®è®¤"
        }
    }
    return configs.get(interaction_type, {})

def format_interaction_message(state: DeepResearchState, interaction_type: InteractionType, config: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–äº¤äº’æ¶ˆæ¯"""
    if interaction_type == InteractionType.OUTLINE_CONFIRMATION:
        outline = state.get("outline", {})
        sections_text = "\n".join([
            f"  {i+1}. {section.get('title', 'æœªçŸ¥ç« èŠ‚')}\n     {section.get('description', 'æ— æè¿°')}"
            for i, section in enumerate(outline.get("sections", []))
        ])
        return f"""
            è¯·ç¡®è®¤ä»¥ä¸‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š

            ğŸ“‹ æ ‡é¢˜ï¼š{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}

            ğŸ“ æ‘˜è¦ï¼š{outline.get('executive_summary', 'æ— æ‘˜è¦')}

            ğŸ“š ç« èŠ‚ç»“æ„ï¼š
            {sections_text}

            ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline.get('methodology', 'æœªæŒ‡å®š')}
            ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline.get('estimated_length', 0):,}å­—
            ğŸ‘¥ ç›®æ ‡è¯»è€…ï¼š{outline.get('target_audience', 'æœªçŸ¥')}
        """
    elif interaction_type == InteractionType.RESEARCH_PERMISSION:
        return f"""
            æ˜¯å¦å…è®¸å¯¹ä¸»é¢˜ã€Œ{state['topic']}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Ÿ

            ç ”ç©¶å°†åŒ…æ‹¬ï¼š
            - å¤šæºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†
            - ç›¸å…³æ¡ˆä¾‹å’Œæ•°æ®åˆ†æ
            - è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
            - ä¸“ä¸šæ´å¯Ÿç”Ÿæˆ

            é¢„è®¡ç ”ç©¶æ—¶é—´ï¼š5-10åˆ†é’Ÿ
        """
    else:
        return f"è¯·ç¡®è®¤{config['title']}ç›¸å…³è®¾ç½®"

# åˆ›å»ºäº¤äº’èŠ‚ç‚¹å®ä¾‹
outline_confirmation_node = create_interaction_node(InteractionType.OUTLINE_CONFIRMATION)
research_permission_node = create_interaction_node(InteractionType.RESEARCH_PERMISSION)
analysis_approval_node = create_interaction_node(InteractionType.ANALYSIS_APPROVAL)
content_review_node = create_interaction_node(InteractionType.CONTENT_REVIEW)
final_approval_node = create_interaction_node(InteractionType.FINAL_APPROVAL)

# ============================================================================
# å›¾æ„å»ºå’Œè·¯ç”±é€»è¾‘
# ============================================================================




# æ³¨æ„ï¼šåŸæ¥çš„ content_creation_node å·²è¢«åˆ é™¤
# ç°åœ¨ä½¿ç”¨ enhanced_content_creation_nodeï¼ˆé›†æˆäº†æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾ï¼‰


async def analysis_generation_node(state: DeepResearchState) -> DeepResearchState:
    """åˆ†ææ´å¯Ÿç”ŸæˆèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "analysis_generation",
        "status": "å¼€å§‹ç”Ÿæˆåˆ†ææ´å¯Ÿ",
        "progress": 0
    })
    
    try:
        start_time = time.time()
        
        research_results = state.get("research_results", [])
        if not research_results:
            writer({
                "step": "analysis_generation",
                "status": "æ²¡æœ‰ç ”ç©¶æ•°æ®å¯ä¾›åˆ†æ",
                "progress": 100
            })
            return state
        
        writer({
            "step": "analysis_generation", 
            "status": "æ­£åœ¨è¿›è¡Œè¶‹åŠ¿åˆ†æ...",
            "progress": 30
        })
        
        # ä½¿ç”¨è¶‹åŠ¿åˆ†æå·¥å…·
        insights = trend_analyzer.invoke({
            "research_results": research_results,
            "analysis_focus": state.get("report_type", "general")
        })
        
        writer({
            "step": "analysis_generation",
            "status": "æ­£åœ¨ç”Ÿæˆæ´å¯ŸæŠ¥å‘Š...",
            "progress": 70
        })
        
        # æ·»åŠ æ´å¯Ÿåˆ°çŠ¶æ€
        for insight in insights:
            if not insight.get("error"):
                add_analysis_insight(state, insight)
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "analyst", execution_time)
        update_task_status(state, "analysis_generation", TaskStatus.COMPLETED)
        
        state["current_step"] = "analysis_completed"
        state["execution_path"] = state["execution_path"] + ["analysis_generation"]
        
        # æ·»åŠ åˆ†æå®Œæˆæ¶ˆæ¯
        analysis_message = f"""
        ğŸ“ˆ åˆ†ææ´å¯Ÿç”Ÿæˆå®Œæˆï¼š
        
        ğŸ” æ´å¯Ÿç»Ÿè®¡ï¼š
        - ç”Ÿæˆæ´å¯Ÿï¼š{len([i for i in insights if not i.get('error')])}ä¸ª
        - æ•°æ®æ¥æºï¼š{len(research_results)}æ¡ç ”ç©¶ç»“æœ
        - æ‰§è¡Œæ—¶é—´ï¼š{execution_time:.2f}ç§’
        
        ğŸ’¡ ä¸»è¦æ´å¯Ÿç±»å‹ï¼š
        {chr(10).join([f"  â€¢ {insight.get('type', 'æœªçŸ¥')}: {insight.get('title', 'æœªçŸ¥æ ‡é¢˜')}" for insight in insights[:3] if not insight.get('error')])}
        """
        
        state["messages"] = state["messages"] + [AIMessage(content=analysis_message)]
        
        writer({
            "step": "analysis_generation",
            "status": "åˆ†ææ´å¯Ÿç”Ÿæˆå®Œæˆ",
            "progress": 100,
            "insights_count": len([i for i in insights if not i.get('error')]),
            "execution_time": execution_time
        })
        
        logger.info(f"åˆ†æç”Ÿæˆå®Œæˆ: {len(insights)}ä¸ªæ´å¯Ÿ")
        return state
        
    except Exception as e:
        logger.error(f"åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}")
        writer({
            "step": "analysis_generation",
            "status": f"åˆ†æç”Ÿæˆå¤±è´¥: {str(e)}",
            "progress": -1
        })
        
        state["error_log"] = state["error_log"] + [f"åˆ†æç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "analysis_failed"
        update_task_status(state, "analysis_generation", TaskStatus.FAILED)
        return state

# ============================================================================
# è·¯ç”±å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬
# ============================================================================

def route_after_outline_confirmation(state: DeepResearchState) -> str:
    """å¤§çº²ç¡®è®¤åçš„è·¯ç”± - ç®€åŒ–ç‰ˆæœ¬"""
    if not state["approval_status"].get("outline_confirmation", True):
        return "outline_generation"  # é‡æ–°ç”Ÿæˆå¤§çº²
    return "content_creation"  # ç›´æ¥è¿›å…¥å†…å®¹åˆ›å»ºï¼ˆé›†æˆäº†å­å›¾ï¼‰



# ============================================================================
# å›¾æ„å»ºå‡½æ•°
# ============================================================================

def create_deep_research_graph(checkpointer: Optional[InMemorySaver] = None):
    """åˆ›å»ºæ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå›¾ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾"""
    workflow = StateGraph(DeepResearchState)

    # æ·»åŠ ç®€åŒ–çš„æ ¸å¿ƒèŠ‚ç‚¹ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
    workflow.add_node("outline_generation", outline_generation_node)
    workflow.add_node("outline_confirmation", outline_confirmation_node)
    # ä½¿ç”¨æ™ºèƒ½ç« èŠ‚å¤„ç†èŠ‚ç‚¹ï¼ˆé›†æˆäº†å®Œæ•´çš„ç« èŠ‚ç ”ç©¶å­å›¾ï¼‰
    workflow.add_node("content_creation", intelligent_section_processing_node)
    
    # è®¾ç½®ç®€åŒ–çš„æµç¨‹ï¼šå¤§çº²ç”Ÿæˆ â†’ å¤§çº²ç¡®è®¤ â†’ å†…å®¹åˆ›å»º
    workflow.add_edge(START, "outline_generation")
    workflow.add_edge("outline_generation", "outline_confirmation")

    # å¤§çº²ç¡®è®¤åçš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "outline_confirmation",
        route_after_outline_confirmation,
        {
            "outline_generation": "outline_generation",
            "content_creation": "content_creation"
        }
    )
    
    workflow.add_edge("content_creation", END)
    
    # ç¼–è¯‘å›¾
    if checkpointer is None:
        checkpointer = InMemorySaver()
    
    app = workflow.compile(checkpointer=checkpointer)
    return app
