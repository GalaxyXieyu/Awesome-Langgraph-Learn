"""
æ ¸å¿ƒå›¾æ¨¡å—
é«˜çº§äº¤äº’å¼æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ä¸»è¦å·¥ä½œæµå›¾
é›†æˆMulti-Agentåä½œã€Human-in-loopäº¤äº’å’Œæµå¼è¾“å‡º
"""

import json
import time
import asyncio
import uuid
from typing import Dict, Any, List, Optional
from datetime import datetime

from langgraph.graph import StateGraph, END, START
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer
from langgraph.types import interrupt
import logging

def safe_get_stream_writer():
    """å®‰å…¨è·å–æµå†™å…¥å™¨ï¼Œé¿å…ä¸Šä¸‹æ–‡é”™è¯¯"""
    try:
        return get_stream_writer()
    except Exception:
        # å¦‚æœæ²¡æœ‰æµä¸Šä¸‹æ–‡ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„å†™å…¥å™¨
        return lambda x: None

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from state import (
    DeepResearchState, ReportMode, TaskStatus, InteractionType,
    ReportOutline, ReportSection, ResearchResult,
    update_performance_metrics, add_research_result,
    update_task_status, add_user_interaction
)
# ç®€åŒ–å·¥å…·å¯¼å…¥ - å¤§éƒ¨åˆ†åŠŸèƒ½ç”±updateå­å›¾å¤„ç†

# å¯¼å…¥å­å›¾æ¨¡å— - ä½¿ç”¨updateå­å›¾æ›¿ä»£researchå­å›¾
from subgraph.update.graph import (
    create_intelligent_research_graph,
    IntelligentResearchState
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm() -> ChatOpenAI:
    """åˆ›å»ºLLMå®ä¾‹"""
    return ChatOpenAI(
        model="qwen2.5-72b-instruct-awq",
        temperature=0.7,
        base_url="https://llm.3qiao.vip:23436/v1",
        api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197",
    )

# ç¼–è¯‘å­å›¾ï¼ˆå…¨å±€å˜é‡ï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼‰- ä½¿ç”¨updateå­å›¾
_intelligent_research_subgraph = None

def get_intelligent_research_subgraph():
    """è·å–æ™ºèƒ½ç ”ç©¶å­å›¾å®ä¾‹"""
    global _intelligent_research_subgraph
    if _intelligent_research_subgraph is None:
        workflow = create_intelligent_research_graph()
        _intelligent_research_subgraph = workflow.compile()
    return _intelligent_research_subgraph

def create_update_subgraph_state(state: DeepResearchState) -> Dict[str, Any]:
    """
    åˆ›å»ºupdateå­å›¾çš„åˆå§‹çŠ¶æ€
    å°†ä¸»å›¾çŠ¶æ€è½¬æ¢ä¸ºupdateå­å›¾æ‰€éœ€çš„çŠ¶æ€æ ¼å¼
    """
    outline = state.get("outline", {})
    sections = outline.get("sections", []) if outline else []

    # è½¬æ¢ç« èŠ‚æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½æœ‰id
    formatted_sections = []
    for i, section in enumerate(sections):
        formatted_section = {
            "id": section.get("id", f"section_{i}"),
            "title": section.get("title", f"ç« èŠ‚ {i+1}"),
            "description": section.get("description", ""),
            "key_points": section.get("key_points", []),
            "research_queries": section.get("research_queries", [])
        }
        formatted_sections.append(formatted_section)

    # åˆ›å»ºupdateå­å›¾çŠ¶æ€
    subgraph_state = {
        "messages": [],
        "user_input": f"è¯·ä¸ºä¸»é¢˜'{state.get('topic', '')}'ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Š",
        "topic": state.get("topic", ""),
        "sections": formatted_sections,
        "current_section_index": 0,
        "research_results": {},
        "writing_results": {},
        "polishing_results": {},
        "final_report": {},
        "execution_path": [],
        "iteration_count": 0,
        "max_iterations": 10,
        "next_action": "research",
        "task_completed": False,
        "error_log": [],
        "section_attempts": {}
    }

    return subgraph_state

async def call_intelligent_research_subgraph(state: DeepResearchState) -> DeepResearchState:
    """
    è°ƒç”¨æ™ºèƒ½ç ”ç©¶å­å›¾ï¼ˆupdateç‰ˆæœ¬ï¼‰

    è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸»å›¾å’Œå­å›¾ä¹‹é—´çš„çŠ¶æ€è½¬æ¢ï¼š
    1. å°† DeepResearchState è½¬æ¢ä¸º IntelligentResearchState
    2. è°ƒç”¨updateå­å›¾è¿›è¡Œç ”ç©¶å’Œå†™ä½œ
    3. å°†ç»“æœè½¬æ¢å› DeepResearchState
    """
    try:
        # è·å–å­å›¾å®ä¾‹
        subgraph = get_intelligent_research_subgraph()

        # åˆ›å»ºå­å›¾è¾“å…¥çŠ¶æ€
        subgraph_input = create_update_subgraph_state(state)

        logger.info(f"å¼€å§‹æ™ºèƒ½ç ”ç©¶: {state.get('topic', 'æœªçŸ¥ä¸»é¢˜')}")

        # è°ƒç”¨å­å›¾
        subgraph_output = await subgraph.ainvoke(subgraph_input)

        # çŠ¶æ€è½¬æ¢ï¼šIntelligentResearchState -> DeepResearchState
        if subgraph_output and subgraph_output.get("final_report"):
            final_report = subgraph_output["final_report"]
            sections_data = final_report.get("sections", [])

            # è½¬æ¢ç« èŠ‚æ•°æ®æ ¼å¼
            updated_sections = []
            for section_data in sections_data:
                updated_section = {
                    "title": section_data.get("title", ""),
                    "content": section_data.get("content", ""),
                    "word_count": section_data.get("word_count", 0),
                    "status": "completed"
                }
                updated_sections.append(updated_section)

            # åˆå¹¶ç ”ç©¶ç»“æœåˆ°ä¸»çŠ¶æ€
            new_research_results = []
            research_results = subgraph_output.get("research_results", {})

            for section_id, research_data in research_results.items():
                new_research_results.append(ResearchResult(
                    id=str(uuid.uuid4()),
                    query=f"ç ”ç©¶ç« èŠ‚: {research_data.get('title', '')}",
                    source_type="subgraph",
                    title=research_data.get("title", ""),
                    content=research_data.get("content", ""),
                    url="",
                    relevance_score=0.9,
                    timestamp=research_data.get("timestamp", time.time()),
                    section_id=section_id
                ))

            # æ›´æ–°çŠ¶æ€
            updated_state = {
                **state,
                "sections": updated_sections,
                "research_results": state.get("research_results", []) + new_research_results,
                "content_creation_completed": True,
                "completed_sections_count": len(updated_sections),
                "performance_metrics": {
                    **state.get("performance_metrics", {}),
                    "sections_completed": len(updated_sections),
                    "total_sections": len(updated_sections),
                    "total_words": final_report.get("total_words", 0)
                }
            }

            logger.info(f"æ™ºèƒ½ç ”ç©¶å®Œæˆ: {len(updated_sections)}ä¸ªç« èŠ‚, æ€»å­—æ•°: {final_report.get('total_words', 0)}")
            return updated_state
        else:
            logger.error("å­å›¾è¿”å›äº†ç©ºç»“æœ")
            return state

    except Exception as e:
        logger.error(f"è°ƒç”¨æ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾æ—¶å‡ºé”™: {e}")
        return state

def convert_research_data_to_results(research_data: List[Dict[str, Any]]) -> List[ResearchResult]:
    """å°†å­å›¾çš„ç ”ç©¶æ•°æ®è½¬æ¢ä¸ºä¸»å›¾çš„ ResearchResult æ ¼å¼"""
    results = []

    for item in research_data:
        try:
            result = ResearchResult(
                id=item.get("id", str(uuid.uuid4())),
                query=item.get("query", ""),
                source_type="web",
                title=item.get("title", ""),
                content=item.get("content", ""),
                url=item.get("url", ""),
                relevance_score=item.get("relevance_score", 0.8),
                timestamp=item.get("timestamp", time.time()),
                section_id=item.get("section_id", "")
            )
            results.append(result)
        except Exception as e:
            logger.warning(f"è½¬æ¢ç ”ç©¶æ•°æ®å¤±è´¥: {e}")
            continue

    return results

async def intelligent_section_processing_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """
    æ™ºèƒ½ç« èŠ‚å¤„ç†èŠ‚ç‚¹ - ä½¿ç”¨updateå­å›¾è¿›è¡Œæ•´ä½“ç ”ç©¶å’Œå†™ä½œ

    è¿™ä¸ªèŠ‚ç‚¹çš„ä½œç”¨ï¼š
    1. è·å–å¤§çº²ä¸­çš„æ‰€æœ‰ç« èŠ‚
    2. è°ƒç”¨updateå­å›¾è¿›è¡Œæ™ºèƒ½ç ”ç©¶å’Œå†™ä½œ
    3. å­å›¾å†…éƒ¨å¤„ç†ï¼šæ™ºèƒ½Supervisor â†’ ç ”ç©¶ â†’ å†™ä½œ â†’ æ•´åˆ
    4. è¿”å›å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Š
    """
    writer = safe_get_stream_writer()
    writer({
        "step": "intelligent_section_processing",
        "status": "ğŸ§  å¼€å§‹æ™ºèƒ½ç ”ç©¶å¤„ç†ï¼ˆä½¿ç”¨updateå­å›¾ï¼‰",
        "progress": 0
    })

    try:
        outline = state.get("outline", {})
        sections = outline.get("sections", []) if outline else []

        if not sections:
            writer({
                "step": "intelligent_section_processing",
                "status": "âŒ æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚ä¿¡æ¯",
                "progress": -1
            })
            return state

        writer({
            "step": "intelligent_section_processing",
            "status": f"ğŸ“š å‡†å¤‡ä½¿ç”¨æ™ºèƒ½Supervisorå¤„ç† {len(sections)} ä¸ªç« èŠ‚",
            "progress": 10,
            "total_sections": len(sections)
        })

        # è°ƒç”¨updateå­å›¾è¿›è¡Œæ•´ä½“å¤„ç†
        writer({
            "step": "intelligent_section_processing",
            "status": "ğŸ”¬ å¯åŠ¨æ™ºèƒ½ç ”ç©¶å­å›¾...",
            "progress": 20
        })

        # ç›´æ¥è°ƒç”¨å­å›¾ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼ï¼‰
        updated_state = await call_intelligent_research_subgraph(state)

        # æ£€æŸ¥å¤„ç†ç»“æœ
        if updated_state.get("content_creation_completed"):
            completed_sections = updated_state.get("sections", [])
            total_words = updated_state.get("performance_metrics", {}).get("total_words", 0)

            writer({
                "step": "intelligent_section_processing",
                "status": f"ğŸ‰ æ™ºèƒ½ç ”ç©¶å®Œæˆï¼æˆåŠŸå¤„ç† {len(completed_sections)} ä¸ªç« èŠ‚",
                "progress": 100,
                "completed_sections": len(completed_sections),
                "total_sections": len(sections),
                "total_words": total_words
            })

            logger.info(f"æ™ºèƒ½ç ”ç©¶å®Œæˆ: {len(completed_sections)} ä¸ªç« èŠ‚, æ€»å­—æ•°: {total_words}")
            return updated_state
        else:
            writer({
                "step": "intelligent_section_processing",
                "status": "âš ï¸ å­å›¾å¤„ç†æœªå®Œæˆï¼Œè¿”å›åŸçŠ¶æ€",
                "progress": 50
            })
            logger.warning("å­å›¾å¤„ç†æœªå®Œæˆ")
            return state

    except Exception as e:
        logger.error(f"æ™ºèƒ½ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")
        writer({
            "step": "intelligent_section_processing",
            "status": f"âŒ ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"æ™ºèƒ½ç« èŠ‚å¤„ç†é”™è¯¯: {str(e)}"]
        return state

# ============================================================================
# å¤§çº²ç”ŸæˆèŠ‚ç‚¹
# ============================================================================

async def outline_generation_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """å¤§çº²ç”ŸæˆèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "outline_generation",
        "status": "å¼€å§‹ç”Ÿæˆæ·±åº¦ç ”ç©¶å¤§çº²",
        "progress": 0
    })
    
    try:
        start_time = time.time()
        llm = create_llm()
        parser = JsonOutputParser(pydantic_object=ReportOutline)
        
        # æ„å»ºé«˜çº§å¤§çº²ç”Ÿæˆæç¤º
        outline_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šå¤§çº²è®¾è®¡ä¸“å®¶ã€‚è¯·ç”Ÿæˆè¯¦ç»†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚
            
            è¦æ±‚ï¼š
            1. å¤§çº²ç¬¦åˆ{report_type}æŠ¥å‘Šçš„æ ‡å‡†ç»“æ„
            2. é’ˆå¯¹{target_audience}è¯»è€…ç¾¤ä½“è®¾è®¡
            3. ç ”ç©¶æ·±åº¦ä¸º{depth_level}çº§åˆ«
            4. ç›®æ ‡å­—æ•°çº¦{target_length}å­—
            5. æ¯ä¸ªç« èŠ‚åŒ…å«ç ”ç©¶æŸ¥è¯¢å…³é”®è¯
            6. ç« èŠ‚ä¼˜å…ˆçº§åˆç†åˆ†é…
            
            {format_instructions}"""),
            ("human", """
            è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸“ä¸šçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š
            
            ç ”ç©¶ä¸»é¢˜ï¼š{topic}
            æŠ¥å‘Šç±»å‹ï¼š{report_type}
            ç›®æ ‡è¯»è€…ï¼š{target_audience}
            æ·±åº¦çº§åˆ«ï¼š{depth_level}
            
            è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€ä¾¿äºæ·±åº¦ç ”ç©¶ã€‚
            """)
        ])
        
        input_data = {
            "topic": state["topic"],
            "report_type": state["report_type"],
            "target_audience": state["target_audience"],
            "depth_level": state["depth_level"],
            "target_length": state["target_length"],
            "format_instructions": parser.get_format_instructions()
        }
        
        writer({
            "step": "outline_generation",
            "status": "æ­£åœ¨ç”Ÿæˆä¸“ä¸šå¤§çº²...",
            "progress": 30
        })
        
        # åˆ›å»ºLLMé“¾å¹¶æµå¼æ‰§è¡Œ
        llm_chain = outline_prompt | llm | parser
        
        outline_data = None
        chunk_count = 0
        current_outline_display = ""
        
        async for chunk in llm_chain.astream(input_data, config=config):
            outline_data = chunk
            chunk_count += 1
            # å®æ—¶æ˜¾ç¤ºå¤§çº²å†…å®¹ï¼ˆæ¯5ä¸ªchunkæ›´æ–°ä¸€æ¬¡ä»¥å‡å°‘é¢‘ç‡ï¼‰
            if chunk_count % 5 == 0:
                # æ„å»ºå½“å‰å¤§çº²çš„æ˜¾ç¤ºæ–‡æœ¬
                if hasattr(chunk, 'title'):
                    current_outline_display = f"ğŸ¯ æ ‡é¢˜ï¼š{chunk.title}\n"
                if hasattr(chunk, 'sections') and chunk.sections:
                    current_outline_display += f"ğŸ“š ç« èŠ‚({len(chunk.sections)}ä¸ª):\n"
                    for i, section in enumerate(chunk.sections[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç« èŠ‚
                        if hasattr(section, 'title'):
                            current_outline_display += f"  {i}. {section.title}\n"
                            if hasattr(section, 'description'):
                                current_outline_display += f"     {section.description}\n"
                    if len(chunk.sections) > 3:
                        current_outline_display += f"  ... è¿˜æœ‰{len(chunk.sections)-3}ä¸ªç« èŠ‚"
                
                writer({
                    "step": "outline_generation",
                    "status": "æ­£åœ¨æ„å»ºå¤§çº²ç»“æ„...",
                    "content": chunk,
                    "progress": min(90, 30 + (chunk_count // 5) * 10),
                    "current_outline": current_outline_display,
                    "chunk_count": chunk_count
                })
                
                # å¦‚æœå¤§çº²åŸºæœ¬å®Œæ•´ï¼Œæå‰æ˜¾ç¤º
                if hasattr(chunk, 'title') and hasattr(chunk, 'sections') and len(chunk.sections) >= 3:
                    writer({
                        "step": "outline_generation",
                        "status": "å¤§çº²ç»“æ„å·²ç”Ÿæˆï¼Œæ­£åœ¨å®Œå–„ç»†èŠ‚...",
                        "progress": 85,
                        "partial_outline": chunk,
                        "streaming_content": current_outline_display
                    })
        
        # å¤„ç†ç”Ÿæˆç»“æœ
        if not outline_data:
            # åˆ›å»ºé»˜è®¤å¤§çº²
            outline_data = ReportOutline(
                title=f"{state['topic']} - æ·±åº¦ç ”ç©¶æŠ¥å‘Š",
                executive_summary=f"æœ¬æŠ¥å‘Šå¯¹{state['topic']}è¿›è¡Œå…¨é¢æ·±å…¥çš„ç ”ç©¶åˆ†æï¼Œä¸º{state['target_audience']}æä¾›ä¸“ä¸šæ´å¯Ÿã€‚",
                sections=[
                    ReportSection(
                        id="background",
                        title="ç ”ç©¶èƒŒæ™¯ä¸ç°çŠ¶",
                        description="åˆ†æç ”ç©¶èƒŒæ™¯ã€å‘å±•å†ç¨‹å’Œå½“å‰çŠ¶æ€",
                        key_points=["å†å²å‘å±•", "ç°çŠ¶åˆ†æ", "å…³é”®ç‰¹å¾"],
                        research_queries=[f"{state['topic']} å‘å±•å†å²", f"{state['topic']} ç°çŠ¶åˆ†æ", f"{state['topic']} å¸‚åœºè§„æ¨¡"],
                        priority=5
                    ),
                    ReportSection(
                        id="deep_analysis",
                        title="æ·±åº¦åˆ†æä¸æ´å¯Ÿ",
                        description="è¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿å’Œæ¨¡å¼",
                        key_points=["æ ¸å¿ƒé©±åŠ¨å› ç´ ", "å‘å±•è¶‹åŠ¿", "å½±å“å› ç´ "],
                        research_queries=[f"{state['topic']} è¶‹åŠ¿åˆ†æ", f"{state['topic']} å½±å“å› ç´ ", f"{state['topic']} å‘å±•é¢„æµ‹"],
                        priority=5
                    ),
                    ReportSection(
                        id="case_studies",
                        title="æ¡ˆä¾‹ç ”ç©¶ä¸åº”ç”¨",
                        description="åˆ†æå…¸å‹æ¡ˆä¾‹å’Œå®é™…åº”ç”¨æƒ…å†µ",
                        key_points=["æˆåŠŸæ¡ˆä¾‹", "åº”ç”¨åœºæ™¯", "å®æ–½ç»éªŒ"],
                        research_queries=[f"{state['topic']} æ¡ˆä¾‹ç ”ç©¶", f"{state['topic']} åº”ç”¨å®ä¾‹", f"{state['topic']} æœ€ä½³å®è·µ"],
                        priority=4
                    ),
                    ReportSection(
                        id="challenges_opportunities",
                        title="æŒ‘æˆ˜ä¸æœºé‡åˆ†æ",
                        description="è¯†åˆ«é¢ä¸´çš„æŒ‘æˆ˜å’Œå‘å±•æœºé‡",
                        key_points=["ä¸»è¦æŒ‘æˆ˜", "å‘å±•æœºé‡", "é£é™©è¯„ä¼°"],
                        research_queries=[f"{state['topic']} æŒ‘æˆ˜åˆ†æ", f"{state['topic']} å‘å±•æœºä¼š", f"{state['topic']} é£é™©è¯„ä¼°"],
                        priority=4
                    ),
                    ReportSection(
                        id="future_outlook",
                        title="æœªæ¥å±•æœ›ä¸å»ºè®®",
                        description="é¢„æµ‹æœªæ¥å‘å±•å¹¶æå‡ºä¸“ä¸šå»ºè®®",
                        key_points=["å‘å±•é¢„æµ‹", "æˆ˜ç•¥å»ºè®®", "è¡ŒåŠ¨è®¡åˆ’"],
                        research_queries=[f"{state['topic']} æœªæ¥å‘å±•", f"{state['topic']} å‘å±•å»ºè®®", f"{state['topic']} æˆ˜ç•¥è§„åˆ’"],
                        priority=3
                    )
                ],
                methodology="é‡‡ç”¨æ–‡çŒ®ç ”ç©¶ã€æ¡ˆä¾‹åˆ†æã€è¶‹åŠ¿é¢„æµ‹å’Œä¸“å®¶æ´å¯Ÿç›¸ç»“åˆçš„ç»¼åˆç ”ç©¶æ–¹æ³•",
                target_audience=state["target_audience"],
                estimated_length=state["target_length"]
            )
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if hasattr(outline_data, 'dict'):
            outline_dict = outline_data.dict()
        else:
            outline_dict = dict(outline_data) if outline_data else {}
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "outline_generator", execution_time)
        update_task_status(state, "outline_generation", TaskStatus.COMPLETED)
        
        state["outline"] = outline_dict
        state["current_step"] = "outline_generated"
        state["execution_path"] = state["execution_path"] + ["outline_generation"]
        
        # åˆ›å»ºå¤§çº²å±•ç¤ºæ¶ˆæ¯
        sections_text = "\n".join([
            f"  {i+1}. {section['title']}\n     {section['description']}\n     å…³é”®è¯: {', '.join(section['research_queries'][:3])}"
            for i, section in enumerate(outline_dict.get("sections", []))
        ])
        
        outline_message = f"""
        ğŸ“‹ æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆï¼š
        
        ğŸ¯ æŠ¥å‘Šæ ‡é¢˜ï¼š{outline_dict.get('title', 'æœªçŸ¥')}
        
        ğŸ“ æ‰§è¡Œæ‘˜è¦ï¼š
        {outline_dict.get('executive_summary', 'æ— æ‘˜è¦')}
        
        ğŸ“š ç ”ç©¶ç« èŠ‚ï¼š
        {sections_text}
        
        ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline_dict.get('methodology', 'æœªæŒ‡å®š')}
        ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline_dict.get('estimated_length', 0):,}å­—
        â±ï¸ ç”Ÿæˆæ—¶é—´ï¼š{execution_time:.2f}ç§’
        """
        
        state["messages"] = state["messages"] + [AIMessage(content=outline_message)]
        
        writer({
            "step": "outline_generation",
            "status": "æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆ",
            "progress": 100,
            "sections_count": len(outline_dict.get("sections", [])),
            "execution_time": execution_time,
            "content": {
                "type": "outline",
                "data": outline_dict,
                "display_text": outline_message
            }
        })
        
        logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_dict.get('sections', []))}ä¸ªç« èŠ‚")
        return state
        
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        writer({
            "step": "outline_generation",
            "status": f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}",
            "progress": -1
        })
        
        state["error_log"] = state["error_log"] + [f"å¤§çº²ç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "outline_generation_failed"
        update_task_status(state, "outline_generation", TaskStatus.FAILED)
        return state

# ============================================================================
# äº¤äº’ç¡®è®¤èŠ‚ç‚¹
# ============================================================================

def create_interaction_node(interaction_type: InteractionType):
    """åˆ›å»ºäº¤äº’ç¡®è®¤èŠ‚ç‚¹çš„å·¥å‚å‡½æ•°"""
    
    def interaction_node(state: DeepResearchState) -> DeepResearchState:
        """é€šç”¨äº¤äº’ç¡®è®¤èŠ‚ç‚¹"""
        writer = safe_get_stream_writer()
        
        interaction_config = get_interaction_config(interaction_type)
        mode = state["mode"]
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": f"å¤„ç†{interaction_config['title']}",
            "progress": 0,
            "interaction_type": interaction_type.value,
            "mode": mode.value
        })
        
        # Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡
        if mode == ReportMode.COPILOT:
            state["approval_status"][interaction_type.value] = True
            state["user_feedback"][interaction_type.value] = {"approved": True, "auto": True}
            
            writer({
                "step": f"interaction_{interaction_type.value}",
                "status": "Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡",
                "progress": 100,
                "auto_approved": True
            })
            
            state["messages"] = state["messages"] + [
                AIMessage(content=f"ğŸ¤– Copilotæ¨¡å¼ï¼š{interaction_config['copilot_message']}")
            ]
            
            return state
        
        # äº¤äº’æ¨¡å¼éœ€è¦ç”¨æˆ·ç¡®è®¤
        message_content = format_interaction_message(state, interaction_type, interaction_config)
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": "ç­‰å¾…ç”¨æˆ·ç¡®è®¤",
            "progress": 50,
            "awaiting_user_input": True
        })
        
        # ä½¿ç”¨interruptç­‰å¾…ç”¨æˆ·è¾“å…¥
        user_response = interrupt({
            "type": interaction_type.value,
            "title": interaction_config["title"],
            "message": message_content,
            "options": interaction_config["options"],
            "default": interaction_config.get("default", "continue")
        })
        
        # å¤„ç†ç”¨æˆ·å“åº”
        approved = user_response.get("approved", True) if isinstance(user_response, dict) else True
        state["approval_status"][interaction_type.value] = approved
        state["user_feedback"][interaction_type.value] = user_response
        
        # è®°å½•äº¤äº’å†å²
        add_user_interaction(state, interaction_type.value, user_response)
        
        writer({
            "step": f"interaction_{interaction_type.value}",
            "status": "ç”¨æˆ·ç¡®è®¤å®Œæˆ",
            "progress": 100,
            "user_response": user_response,
            "approved": approved
        })
        
        # æ·»åŠ ç¡®è®¤æ¶ˆæ¯
        status_emoji = "âœ…" if approved else "âŒ"
        confirmation_message = f"{status_emoji} {interaction_config['title']}ï¼š{'ç¡®è®¤é€šè¿‡' if approved else 'è¢«æ‹’ç»'}"
        
        state["messages"] = state["messages"] + [AIMessage(content=confirmation_message)]
        
        return state
    
    return interaction_node

def get_interaction_config(interaction_type: InteractionType) -> Dict[str, Any]:
    """è·å–äº¤äº’é…ç½®"""
    configs = {
        InteractionType.OUTLINE_CONFIRMATION: {
            "title": "å¤§çº²ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤æŠ¥å‘Šå¤§çº²ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶",
            "options": ["ç¡®è®¤ç»§ç»­", "ä¿®æ”¹å¤§çº²", "é‡æ–°ç”Ÿæˆ"],
            "default": "ç¡®è®¤ç»§ç»­"
        },
        InteractionType.RESEARCH_PERMISSION: {
            "title": "ç ”ç©¶æƒé™ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨å…è®¸è¿›è¡Œæ·±åº¦ç ”ç©¶å’Œæ•°æ®æ”¶é›†",
            "options": ["å…è®¸ç ”ç©¶", "è·³è¿‡ç ”ç©¶", "é™åˆ¶èŒƒå›´"],
            "default": "å…è®¸ç ”ç©¶"
        },
        InteractionType.ANALYSIS_APPROVAL: {
            "title": "åˆ†æç»“æœå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤åˆ†æç»“æœï¼Œç»§ç»­å†…å®¹ç”Ÿæˆ",
            "options": ["ç¡®è®¤åˆ†æ", "é‡æ–°åˆ†æ", "è°ƒæ•´æ–¹å‘"],
            "default": "ç¡®è®¤åˆ†æ"
        },
        InteractionType.CONTENT_REVIEW: {
            "title": "å†…å®¹å®¡æŸ¥",
            "copilot_message": "è‡ªåŠ¨é€šè¿‡å†…å®¹å®¡æŸ¥ï¼Œå‡†å¤‡æœ€ç»ˆæŠ¥å‘Š",
            "options": ["é€šè¿‡å®¡æŸ¥", "ä¿®æ”¹å†…å®¹", "é‡å†™ç« èŠ‚"],
            "default": "é€šè¿‡å®¡æŸ¥"
        },
        InteractionType.FINAL_APPROVAL: {
            "title": "æœ€ç»ˆå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨å®Œæˆæœ€ç»ˆå®¡æ‰¹ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "options": ["æœ€ç»ˆç¡®è®¤", "å†æ¬¡ä¿®æ”¹", "ç”Ÿæˆæ–°ç‰ˆæœ¬"],
            "default": "æœ€ç»ˆç¡®è®¤"
        }
    }
    return configs.get(interaction_type, {})

def format_interaction_message(state: DeepResearchState, interaction_type: InteractionType, config: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–äº¤äº’æ¶ˆæ¯"""
    if interaction_type == InteractionType.OUTLINE_CONFIRMATION:
        outline = state.get("outline", {})
        sections_text = "\n".join([
            f"  {i+1}. {section.get('title', 'æœªçŸ¥ç« èŠ‚')}\n     {section.get('description', 'æ— æè¿°')}"
            for i, section in enumerate(outline.get("sections", []))
        ])
        return f"""
            è¯·ç¡®è®¤ä»¥ä¸‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š

            ğŸ“‹ æ ‡é¢˜ï¼š{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}

            ğŸ“ æ‘˜è¦ï¼š{outline.get('executive_summary', 'æ— æ‘˜è¦')}

            ğŸ“š ç« èŠ‚ç»“æ„ï¼š
            {sections_text}

            ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline.get('methodology', 'æœªæŒ‡å®š')}
            ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline.get('estimated_length', 0):,}å­—
            ğŸ‘¥ ç›®æ ‡è¯»è€…ï¼š{outline.get('target_audience', 'æœªçŸ¥')}
        """
    elif interaction_type == InteractionType.RESEARCH_PERMISSION:
        return f"""
            æ˜¯å¦å…è®¸å¯¹ä¸»é¢˜ã€Œ{state['topic']}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Ÿ

            ç ”ç©¶å°†åŒ…æ‹¬ï¼š
            - å¤šæºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†
            - ç›¸å…³æ¡ˆä¾‹å’Œæ•°æ®åˆ†æ
            - è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
            - ä¸“ä¸šæ´å¯Ÿç”Ÿæˆ

            é¢„è®¡ç ”ç©¶æ—¶é—´ï¼š5-10åˆ†é’Ÿ
        """
    else:
        return f"è¯·ç¡®è®¤{config['title']}ç›¸å…³è®¾ç½®"

# åˆ›å»ºäº¤äº’èŠ‚ç‚¹å®ä¾‹
outline_confirmation_node = create_interaction_node(InteractionType.OUTLINE_CONFIRMATION)

# ============================================================================
# å›¾æ„å»ºå’Œè·¯ç”±é€»è¾‘
# ============================================================================


# åˆ é™¤äº†analysis_generation_node - ç”±updateå­å›¾å¤„ç†åˆ†æåŠŸèƒ½

# ============================================================================
# è·¯ç”±å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬
# ============================================================================

def route_after_outline_confirmation(state: DeepResearchState) -> str:
    """å¤§çº²ç¡®è®¤åçš„è·¯ç”± - ç®€åŒ–ç‰ˆæœ¬"""
    if not state["approval_status"].get("outline_confirmation", True):
        return "outline_generation"  # é‡æ–°ç”Ÿæˆå¤§çº²
    return "content_creation"  # ç›´æ¥è¿›å…¥å†…å®¹åˆ›å»ºï¼ˆé›†æˆäº†å­å›¾ï¼‰

# ============================================================================
# å›¾æ„å»ºå‡½æ•°
# ============================================================================

def create_deep_research_graph():
    """åˆ›å»ºæ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå›¾ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾"""
    workflow = StateGraph(DeepResearchState)

    # æ·»åŠ ç®€åŒ–çš„æ ¸å¿ƒèŠ‚ç‚¹ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
    workflow.add_node("outline_generation", outline_generation_node)
    workflow.add_node("outline_confirmation", outline_confirmation_node)
    # ç›´æ¥ä½¿ç”¨å­å›¾è°ƒç”¨å‡½æ•°ä½œä¸ºèŠ‚ç‚¹ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ–¹å¼ï¼‰
    workflow.add_node("content_creation", call_intelligent_research_subgraph)
    
    # è®¾ç½®ç®€åŒ–çš„æµç¨‹ï¼šå¤§çº²ç”Ÿæˆ â†’ å¤§çº²ç¡®è®¤ â†’ å†…å®¹åˆ›å»º
    workflow.add_edge(START, "outline_generation")
    workflow.add_edge("outline_generation", "outline_confirmation")

    # å¤§çº²ç¡®è®¤åçš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "outline_confirmation",
        route_after_outline_confirmation,
        {
            "outline_generation": "outline_generation",
            "content_creation": "content_creation"
        }
    )
    
    workflow.add_edge("content_creation", END)
    
    return workflow
