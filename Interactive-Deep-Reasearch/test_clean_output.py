#!/usr/bin/env python3
"""
æµ‹è¯•æ¸…ç†åçš„è¾“å‡º - éªŒè¯ä¸å†æœ‰å™ªéŸ³æ¶ˆæ¯
"""

import time
from stream_writer import create_workflow_processor


class OutputCapture:
    """æ•è·writerè¾“å‡º"""
    
    def __init__(self):
        self.messages = []
    
    def __call__(self, message):
        """æ¨¡æ‹Ÿstream writer"""
        self.messages.append(message)
        msg_type = message.get('message_type', 'unknown')
        content = message.get('content', '')
        
        # é«˜äº®æ˜¾ç¤ºä¸å¸Œæœ›çœ‹åˆ°çš„å†…å®¹
        if any(keyword in content for keyword in ['ğŸ§ ', 'æ™ºèƒ½è°ƒåº¦åˆ†æå®Œæˆ', 'è´¨é‡åé¦ˆ', 'ç½®ä¿¡åº¦']):
            print(f"âŒ å™ªéŸ³æ¶ˆæ¯: [{msg_type}] '{content[:100]}...'")
        else:
            print(f"âœ… æ¸…æ´æ¶ˆæ¯: [{msg_type}] '{content[:50]}{'...' if len(content) > 50 else ''}'")


def test_clean_output():
    """æµ‹è¯•æ¸…ç†åçš„è¾“å‡º"""
    print("ğŸ§ª æµ‹è¯•æ¸…ç†åçš„è¾“å‡ºæ•ˆæœ")
    print("=" * 50)
    
    # åˆ›å»ºprocessor
    processor = create_workflow_processor("intelligent_research", "æ·±åº¦ç ”ç©¶")
    
    output_capture = OutputCapture()
    processor.writer.writer = output_capture
    
    # æ¨¡æ‹Ÿä¸€äº›æ¶ˆæ¯
    test_messages = [
        # æ­¥éª¤å¼€å§‹
        ('custom', {
            'message_type': 'step_start',
            'content': 'å¼€å§‹æ™ºèƒ½ç ”ç©¶å¤„ç†',
            'node': 'intelligent_research',
            'timestamp': time.time()
        }),
        
        # å·¥å…·è°ƒç”¨
        ('custom', {
            'message_type': 'tool_call',
            'content': 'æ­£åœ¨æœç´¢: äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿',
            'node': 'research',
            'timestamp': time.time(),
            'metadata': {
                'tool_name': 'web_search_tool',
                'tool_args': {'query': 'äººå·¥æ™ºèƒ½å‘å±•è¶‹åŠ¿'}
            }
        }),
        
        # å†…å®¹æµ
        ('custom', {
            'message_type': 'content_streaming',
            'content': 'äººå·¥æ™ºèƒ½æŠ€æœ¯æ­£åœ¨å¿«é€Ÿå‘å±•...',
            'node': 'writing',
            'timestamp': time.time()
        }),
        
        # æ­¥éª¤å®Œæˆ
        ('custom', {
            'message_type': 'step_complete',
            'content': 'ç ”ç©¶åˆ†æå®Œæˆ',
            'node': 'research',
            'timestamp': time.time(),
            'metadata': {
                'duration': 2.3,
                'word_count': 150
            }
        }),
        
        # è¿™ç§æ¶ˆæ¯åº”è¯¥è¢«æ ‡è®°ä¸ºå™ªéŸ³ï¼ˆå¦‚æœè¿˜å­˜åœ¨ï¼‰
        ('custom', {
            'message_type': 'reasoning',
            'content': 'ğŸ§  æ™ºèƒ½è°ƒåº¦åˆ†æå®Œæˆï¼šå†³ç­–ä¼˜åŒ–ä¸­...',
            'node': 'supervisor',
            'timestamp': time.time()
        })
    ]
    
    print(f"ğŸ“Š å¤„ç† {len(test_messages)} ä¸ªæµ‹è¯•æ¶ˆæ¯...")
    print()
    
    for i, message in enumerate(test_messages, 1):
        print(f"ğŸ” æ¶ˆæ¯ #{i}:")
        processor.process_chunk(message)
    
    print(f"\nğŸ“ˆ æ€»è¾“å‡º: {len(output_capture.messages)} æ¡æ¶ˆæ¯")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å™ªéŸ³æ¶ˆæ¯
    noise_messages = []
    clean_messages = []
    
    for msg in output_capture.messages:
        content = msg.get('content', '')
        if any(keyword in content for keyword in ['ğŸ§ ', 'æ™ºèƒ½è°ƒåº¦åˆ†æå®Œæˆ', 'è´¨é‡åé¦ˆ', 'ç½®ä¿¡åº¦']):
            noise_messages.append(msg)
        else:
            clean_messages.append(msg)
    
    print(f"\nğŸ“Š æ¶ˆæ¯åˆ†ç±»:")
    print(f"   âœ… æ¸…æ´æ¶ˆæ¯: {len(clean_messages)}æ¡")
    print(f"   âŒ å™ªéŸ³æ¶ˆæ¯: {len(noise_messages)}æ¡")
    
    if noise_messages:
        print(f"\nâš ï¸  å‘ç°å™ªéŸ³æ¶ˆæ¯:")
        for i, msg in enumerate(noise_messages, 1):
            content = msg.get('content', '')
            print(f"   {i}. '{content[:100]}{'...' if len(content) > 100 else ''}'")
        print("\nâŒ æ¸…ç†ä¸å®Œæ•´ï¼")
    else:
        print(f"\nğŸ‰ æ¸…ç†æˆåŠŸï¼æ²¡æœ‰å‘ç°å™ªéŸ³æ¶ˆæ¯ã€‚")
    
    print("\nâœ… æµ‹è¯•å®Œæˆï¼")


if __name__ == "__main__":
    test_clean_output()