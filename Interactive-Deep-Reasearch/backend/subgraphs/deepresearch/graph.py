"""
åŸºäºLangGraphçš„æ™ºèƒ½ç ”ç©¶ç³»ç»Ÿ
æ­£ç¡®ä½¿ç”¨LangGraphæ¶æ„ï¼šStateGraph + èŠ‚ç‚¹ + æ¡ä»¶è·¯ç”± + æµå¼è¾“å‡º
"""

import json
import time
import asyncio
import os
from typing import Dict, Any, List, TypedDict, Annotated, Optional

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver

from context_builder import build_supervisor_context, determine_next_action_by_state
from prompts import get_supervisor_prompt, get_researcher_prompt, get_writer_prompt

# å¯¼å…¥æ–°çš„å·¥å…·ç³»ç»Ÿ
import sys
import os
# åŠ¨æ€è·å–é¡¹ç›®æ ¹ç›®å½•
# Get the absolute path of the current file's directory
current_dir = os.path.dirname(os.path.abspath(__file__))
# Construct the path to the project root (Interactive-Deep-Reasearch)
project_root = os.path.abspath(os.path.join(current_dir, '..', '..'))
# Add the project root to the system path
if project_root not in sys.path:
    sys.path.insert(0, project_root)
from tools import get_research_tools

# ============================================================================
# çŠ¶æ€å®šä¹‰ - LangGraphæ ¸å¿ƒ
# ============================================================================

class IntelligentResearchState(TypedDict):
    """æ™ºèƒ½ç ”ç©¶ç³»ç»ŸçŠ¶æ€"""
    messages: Annotated[List, add_messages]  # æ¶ˆæ¯å†å²
    user_input: str  # ç”¨æˆ·è¾“å…¥
    topic: str  # ç ”ç©¶ä¸»é¢˜
    mode: str  # ğŸ¯ æ‰§è¡Œæ¨¡å¼ï¼šcopilot/interactive
    sections: List[Dict[str, Any]]  # ç« èŠ‚åˆ—è¡¨
    current_section_index: int  # å½“å‰å¤„ç†çš„ç« èŠ‚ç´¢å¼•
    research_results: Dict[str, Any]  # ç ”ç©¶ç»“æœ
    writing_results: Dict[str, Any]  # å†™ä½œç»“æœ
    polishing_results: Dict[str, Any]  # æ¶¦è‰²ç»“æœ
    final_report: Dict[str, Any]  # æœ€ç»ˆæŠ¥å‘Š
    execution_path: List[str]  # æ‰§è¡Œè·¯å¾„
    iteration_count: int  # è¿­ä»£æ¬¡æ•°
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    next_action: str  # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    task_completed: bool  # ä»»åŠ¡å®Œæˆæ ‡å¿—
    error_log: List[str]  # é”™è¯¯æ—¥å¿—
    section_attempts: Dict[str, Dict[str, int]]  # æ¯ä¸ªç« èŠ‚çš„å°è¯•æ¬¡æ•°è®°å½• {"section_id": {"research": 2, "writing": 1}}

# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm() -> ChatOpenAI:
    """åˆ›å»ºLLMå®ä¾‹"""
    return  ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.7,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197",
    )

# ============================================================================
# Agentåˆ›å»º - ä¸“ä¸šåŒ–Agent
# ============================================================================

async def create_research_agents(state: IntelligentResearchState):
    """åˆ›å»ºä¸“ä¸šåŒ–çš„ç ”ç©¶Agent - ä½¿ç”¨æ–°çš„å·¥å…·ç³»ç»Ÿ"""
    llm = create_llm()

    # ğŸ¯ ä½¿ç”¨æ–°å·¥å…·ç³»ç»Ÿï¼šè‡ªåŠ¨æ£€æµ‹modeï¼Œè‡ªåŠ¨åŒ…è£…
    # å¼‚æ­¥è·å–å¹¶åŒ…è£…ç ”ç©¶å·¥å…·ï¼Œstateçš„ä¼ é€’æ˜¯å…³é”®
    research_tools = await get_research_tools(state["mode"])

    # ç ”ç©¶å‘˜Agent
    researcher_agent = create_react_agent(
        llm,
        tools=research_tools,  # ä½¿ç”¨åŒ…è£…åçš„å·¥å…·
        prompt=get_researcher_prompt()
    )

    # å†™ä½œå‘˜Agent - ä¹Ÿå¯ä»¥ä½¿ç”¨å·¥å…·è·å–æ›´å¤šæ•°æ®
    writer_agent = create_react_agent(
        llm,
        tools=research_tools,  # ä½¿ç”¨åŒ…è£…åçš„å·¥å…·
        prompt=get_writer_prompt()
    )

    return {
        "researcher": researcher_agent,
        "writer": writer_agent
    }

# ============================================================================
# èŠ‚ç‚¹å‡½æ•° - LangGraphæ ¸å¿ƒç»„ä»¶
# ============================================================================

async def supervisor_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    """æ™ºèƒ½SupervisorèŠ‚ç‚¹ - ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–å’Œè´¨é‡è¯„ä¼°"""

    llm = create_llm()

    # ä½¿ç”¨æ¨¡å—åŒ–çš„ä¸Šä¸‹æ–‡æ„å»º
    input_data = build_supervisor_context(state)

    # æ„å»ºSupervisorçš„æ™ºèƒ½å†³ç­–æç¤º
    supervisor_prompt = get_supervisor_prompt()

    formatted_messages = supervisor_prompt.format_messages(**input_data)
    # æµå¼è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–
    full_response = ""
    chunk_count = 0
    async for chunk in llm.astream(formatted_messages):
        if hasattr(chunk, 'content') and chunk.content:
            content = str(chunk.content)
            full_response += content
            chunk_count += 1

    # å†³ç­–ç»“æœè§£æä¸­
    import re
    try:
        json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', full_response, re.DOTALL)
        decision_json = None
        for json_str in json_matches:
            # æ¸…ç†JSONå­—ç¬¦ä¸²
            cleaned_json = json_str.strip()
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_json = re.sub(r'^```json\s*', '', cleaned_json)
            cleaned_json = re.sub(r'\s*```$', '', cleaned_json)

            decision_json = json.loads(cleaned_json)
            break

        if decision_json:
            # æ¸…ç†JSONé”®åï¼Œå¤„ç†å¯èƒ½åŒ…å«æ¢è¡Œç¬¦å’Œç©ºæ ¼çš„é”®
            cleaned_json = {}
            for key, value in decision_json.items():
                # æ¸…ç†é”®åï¼šç§»é™¤æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼
                clean_key = key.strip().replace('\n', '').replace('\t', '').replace(' ', '')
                cleaned_json[clean_key] = value

            # ä½¿ç”¨æ¸…ç†åçš„é”®æ¥è·å–å€¼
            next_action = cleaned_json.get("action", "integration")
            reasoning = cleaned_json.get("reasoning", "æ™ºèƒ½åˆ†æå†³ç­–")
            quality_feedback = cleaned_json.get("quality_feedback", "")
            confidence = cleaned_json.get("confidence", 0.7)
            target_section = cleaned_json.get("target_section", "")
        else:
            raise ValueError("æ— æ³•è§£æJSON")
    except Exception:
        # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘
        next_action, reasoning = determine_next_action_by_state(state)
        quality_feedback = "åŸºäºçŠ¶æ€é€»è¾‘çš„å†³ç­–"
        confidence = 0.6
        target_section = ""

    # è·å–å½“å‰è¿›åº¦ä¿¡æ¯
    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    research_results = state.get("research_results", {})
    writing_results = state.get("writing_results", {})

    # å…¨å±€å®Œæˆæ£€æŸ¥ - æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç« èŠ‚éƒ½æœ‰ç ”ç©¶å’Œå†™ä½œç»“æœ
    all_sections_completed = True
    for section in sections:
        section_id = section.get("id", "")
        has_research = section_id in research_results and research_results[section_id].get("content", "").strip() != ""
        has_writing = section_id in writing_results and writing_results[section_id].get("content", "").strip() != ""
        if not (has_research and has_writing):
            all_sections_completed = False
            break

    if all_sections_completed and next_action != "integration":
        next_action = "integration"
        reasoning = "æ‰€æœ‰ç« èŠ‚çš„ç ”ç©¶å’Œå†™ä½œéƒ½å·²å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆæ•´åˆ"

    # å¤„ç†ç« èŠ‚ç´¢å¼•æ›´æ–°å’Œç›®æ ‡ç« èŠ‚è®¾ç½®
    if next_action == "move_to_next_section":
        new_index = current_index + 1
        state["current_section_index"] = new_index
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç« èŠ‚èŒƒå›´
        if new_index >= len(sections):
            next_action = "integration"
            reasoning = "æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆæ•´åˆ"
        else:
            next_action = "research"  # ç§»åŠ¨åˆ°ä¸‹ä¸€ç« èŠ‚åå¼€å§‹ç ”ç©¶
            reasoning = f"å·²ä»ç¬¬{current_index + 1}ç« èŠ‚ç§»åŠ¨åˆ°ç¬¬{new_index + 1}ç« èŠ‚ï¼Œå¼€å§‹æ–°ç« èŠ‚ç ”ç©¶"
        current_index = new_index  # æ›´æ–°æœ¬åœ°å˜é‡ç”¨äºæ˜¾ç¤º
    elif target_section and target_section != "":
        # å¦‚æœSupervisoræŒ‡å®šäº†ç›®æ ‡ç« èŠ‚ï¼Œæ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
        target_index = None
        for i, section in enumerate(sections):
            if section.get("id", "") == target_section:
                target_index = i
                break

        if target_index is not None and target_index != current_index:
            state["current_section_index"] = target_index
            current_index = target_index
            reasoning = f"æ ¹æ®SupervisoræŒ‡ç¤ºï¼Œåˆ‡æ¢åˆ°ç›®æ ‡ç« èŠ‚: {sections[target_index].get('title', '')}"

    # æ›´æ–°çŠ¶æ€
    state["next_action"] = next_action
    state["supervisor_reasoning"] = reasoning
    state["quality_feedback"] = quality_feedback
    state["supervisor_confidence"] = confidence
    state["iteration_count"] = state.get("iteration_count", 0) + 1
    state["execution_path"] = state.get("execution_path", []) + ["intelligent_supervisor"]

    # ä¸å†æ·»åŠ è¯¦ç»†çš„supervisoræ¶ˆæ¯åˆ°çŠ¶æ€ä¸­
    return state

async def research_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    _ = config  # LangGraphä¼šä¼ å…¥configï¼Œä½†æ­¤èŠ‚ç‚¹æš‚æ—¶æœªä½¿ç”¨
    """ç ”ç©¶èŠ‚ç‚¹ - æ‰§è¡Œç« èŠ‚ç ”ç©¶"""
    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    if current_index >= len(sections):
        return state

    current_section = sections[current_index]
    section_id = current_section.get("id", "")
    title = current_section.get("title", "")
    description = current_section.get("description", "")

    # è®°å½•ç ”ç©¶å°è¯•æ¬¡æ•°
    section_attempts = state.get("section_attempts", {})
    if section_id not in section_attempts:
        section_attempts[section_id] = {"research": 0, "writing": 0}
    section_attempts[section_id]["research"] += 1
    state["section_attempts"] = section_attempts

    # current_attempt = section_attempts[section_id]["research"]  # æš‚æ—¶ä¸ä½¿ç”¨


    # åˆ›å»ºç ”ç©¶Agent - ä½¿ç”¨æ–°çš„å·¥å…·ç³»ç»Ÿ
    agents = await create_research_agents(state)
    researcher = agents["researcher"]

    # æ„å»ºç ”ç©¶ä»»åŠ¡
    research_task = f"""
    è¯·æ·±åº¦ç ”ç©¶ä»¥ä¸‹ç« èŠ‚ï¼š

    ç« èŠ‚æ ‡é¢˜ï¼š{title}
    ç« èŠ‚æè¿°ï¼š{description}

    ç ”ç©¶è¦æ±‚ï¼š
    1. ä½¿ç”¨åˆé€‚çš„å·¥å…·è·å–ç›¸å…³æ•°æ®
    2. å¦‚æœæ•°æ®ä¸è¶³ï¼Œä¸»åŠ¨ä½¿ç”¨å¤šä¸ªå·¥å…·è¡¥å……
    3. æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
    """

    # Agentæ‰§è¡Œç ”ç©¶
    agent_input = {"messages": [HumanMessage(content=research_task)]}

    # Agentçš„ainvokeè°ƒç”¨ç°åœ¨ç›´æ¥åœ¨èŠ‚ç‚¹çš„ä¸»try/exceptå—ä¸­è¿è¡Œã€‚
    # LangGraphå¼•æ“å°†å¤„ç†HumanInterruptï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨è¿™é‡Œæ•è·å®ƒã€‚
    result = await researcher.ainvoke(agent_input)
    full_response = ""

    # æå–å®Œæ•´çš„Agentå“åº”
    if isinstance(result, dict) and 'messages' in result:
        messages = result['messages']
        # å¯»æ‰¾æœ€åçš„AIæ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç»“æœ
        for msg in reversed(messages):
            if hasattr(msg, 'type') and msg.type == 'ai' and hasattr(msg, 'content'):
                content = str(msg.content).strip()
                if content and len(content) > 50:  # ç¡®ä¿æœ‰å®è´¨æ€§å†…å®¹
                    full_response = content
                    break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AIæ¶ˆæ¯ï¼Œæ”¶é›†æ‰€æœ‰æœ‰ç”¨æ¶ˆæ¯
        if not full_response:
            useful_content = []
            for msg in messages:
                if hasattr(msg, 'content') and msg.content:
                    content = str(msg.content).strip()
                    if content and len(content) > 20:
                        useful_content.append(content)
            full_response = '\n\n'.join(useful_content)

    # æœ€åçš„è´¨é‡æ£€æŸ¥å’Œå†…å®¹ç”Ÿæˆ
    if not full_response or len(full_response.strip()) < 50:
        # æä¾›åŸºäºæè¿°çš„ç ”ç©¶ç»“æœ
        full_response = f"ç« èŠ‚'{title}'çš„ç ”ç©¶åˆ†æï¼š\n\n{description}\n\nè¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç ”ç©¶é¢†åŸŸï¼Œéœ€è¦è¿›ä¸€æ­¥çš„æ·±å…¥è°ƒæŸ¥å’Œæ•°æ®æ”¶é›†æ¥æ”¯æ’‘æˆ‘ä»¬çš„åˆ†æç»“è®ºã€‚è™½ç„¶é‡åˆ°äº†ä¸€äº›æŠ€æœ¯æŒ‘æˆ˜ï¼Œä½†åŸºäºç°æœ‰æ¡†æ¶å¯ä»¥æä¾›åˆæ­¥åˆ†æç»“æœã€‚"

    # ç ”ç©¶å†…å®¹å®Œæˆ
    # word_count = len(full_response.split()) if full_response else 0  # æš‚æ—¶ä¸ä½¿ç”¨

    # ä¿å­˜ç ”ç©¶ç»“æœ
    research_results = state.get("research_results", {})
    research_results[section_id] = {
        "title": title,
        "content": full_response,
        "timestamp": time.time()
    }
    state["research_results"] = research_results
    state["execution_path"] = state.get("execution_path", []) + ["research"]

    # ç ”ç©¶æ­¥éª¤å®Œæˆ

    return state

    # Any real exceptions will be caught by the LangGraph engine.
    # We no longer need a broad try/except block here that would incorrectly catch the Interrupt signal.

async def writing_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    _ = config  # LangGraphä¼šä¼ å…¥configï¼Œä½†æ­¤èŠ‚ç‚¹æš‚æ—¶æœªä½¿ç”¨
    """å†™ä½œèŠ‚ç‚¹ - åŸºäºç ”ç©¶ç»“æœå†™ä½œ"""

    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    research_results = state.get("research_results", {})


    if current_index >= len(sections):
        return state

    current_section = sections[current_index]
    section_id = current_section.get("id", "")
    title = current_section.get("title", "")
    description = current_section.get("description", "")

    # è®°å½•å†™ä½œå°è¯•æ¬¡æ•°
    section_attempts = state.get("section_attempts", {})
    if section_id not in section_attempts:
        section_attempts[section_id] = {"research": 0, "writing": 0}
    section_attempts[section_id]["writing"] += 1
    state["section_attempts"] = section_attempts

    # current_attempt = section_attempts[section_id]["writing"]  # æš‚æ—¶ä¸ä½¿ç”¨

    # è·å–ç ”ç©¶æ•°æ®
    research_data = research_results.get(section_id, {})
    research_content = research_data.get("content", "")

    # å¼€å§‹å†™ä½œ

    # åˆ›å»ºå†™ä½œAgent - ä½¿ç”¨æ–°çš„å·¥å…·ç³»ç»Ÿ
    agents = await create_research_agents(state)
    writer_agent = agents["writer"]

    # è·å–å…¶ä»–ç« èŠ‚çš„ç ”ç©¶ç»“æœä½œä¸ºå‚è€ƒ
    all_research_data = ""
    for sec_id, research_data in research_results.items():
        if sec_id != section_id:  # æ’é™¤å½“å‰ç« èŠ‚
            all_research_data += f"\nå‚è€ƒç« èŠ‚ {research_data.get('title', '')}: {research_data.get('content', '')[:200]}...\n"

    # æ„å»ºå†™ä½œä»»åŠ¡
    writing_task = f"""
    è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œæ’°å†™é«˜è´¨é‡çš„ç« èŠ‚å†…å®¹ï¼š

    å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{title}
    å½“å‰ç« èŠ‚ç ”ç©¶æ•°æ®ï¼š
    {research_content}

    å…¶ä»–ç« èŠ‚ç ”ç©¶æ•°æ®ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
    {all_research_data}

    è¦æ±‚ï¼š
    1. ä¸“æ³¨äºå½“å‰ç« èŠ‚ä¸»é¢˜
    2. å¦‚æœç ”ç©¶æ•°æ®ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯
    3. ç¡®ä¿å†…å®¹å……å®ï¼Œæ•°æ®æ”¯æ’‘å……åˆ†
    4. ä¿æŒä¸å…¶ä»–ç« èŠ‚çš„é€»è¾‘è¿è´¯æ€§
    """

    # Agentå†™ä½œè¿›è¡Œä¸­

    # Agentæ‰§è¡Œå†™ä½œ
    agent_input = {"messages": [HumanMessage(content=writing_task)]}

    # Agentçš„ainvokeè°ƒç”¨ç°åœ¨ç›´æ¥åœ¨èŠ‚ç‚¹çš„ä¸»try/exceptå—ä¸­è¿è¡Œã€‚
    # LangGraphå¼•æ“å°†å¤„ç†HumanInterruptï¼Œæˆ‘ä»¬ä¸éœ€è¦åœ¨è¿™é‡Œæ•è·å®ƒã€‚
    result = await writer_agent.ainvoke(agent_input)
    full_response = ""

    # æå–Agentçš„æœ€ç»ˆå“åº”
    if isinstance(result, dict) and 'messages' in result:
        messages = result['messages']
        # å¯»æ‰¾æœ€åçš„AIæ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç»“æœ
        for msg in reversed(messages):
            if hasattr(msg, 'type') and msg.type == 'ai' and hasattr(msg, 'content'):
                content = str(msg.content).strip()
                if content and len(content) > 100:  # å†™ä½œå†…å®¹åº”è¯¥æ›´é•¿
                    full_response = content
                    break

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°AIæ¶ˆæ¯ï¼Œæ”¶é›†æ‰€æœ‰æœ‰ç”¨å†…å®¹
        if not full_response:
            useful_content = []
            for msg in messages:
                if hasattr(msg, 'content') and msg.content:
                    content = str(msg.content).strip()
                    if content and len(content) > 30:
                        useful_content.append(content)
            full_response = '\n\n'.join(useful_content)

    # å¦‚æœç›´æ¥è°ƒç”¨æ²¡æœ‰è·å¾—å¥½çš„ç»“æœï¼Œå°è¯•æµå¼è°ƒç”¨
    if not full_response or len(full_response) < 200:
        stream_response = ""
        ai_messages = []

        async for chunk in writer_agent.astream(agent_input, stream_mode=["messages"]):
            if isinstance(chunk, tuple) and len(chunk) >= 2:
                chunk_type, chunk_data = chunk
                if chunk_type == "messages":
                    if hasattr(chunk_data, 'type') and chunk_data.type == 'ai':
                        if hasattr(chunk_data, 'content') and chunk_data.content:
                            ai_messages.append(str(chunk_data.content))
                    elif hasattr(chunk_data, 'content'):
                        stream_response += str(chunk_data.content)

        # ä½¿ç”¨AIæ¶ˆæ¯æˆ–æµå¼å“åº”
        if ai_messages:
            full_response = ''.join(ai_messages)
        elif stream_response:
            full_response = stream_response

    # æœ€åçš„è´¨é‡æ£€æŸ¥å’Œå†…å®¹ç”Ÿæˆ
    if not full_response or len(full_response.strip()) < 200:
        # å¦‚æœè¿˜æ˜¯æ²¡æœ‰å¥½çš„ç»“æœï¼ŒåŸºäºç ”ç©¶æ•°æ®ç”Ÿæˆå†…å®¹
        if research_content and len(research_content) > 50:
            full_response = f"# {title}\n\nåŸºäºæ·±å…¥ç ”ç©¶ï¼Œæˆ‘ä»¬å‘ç°ä»¥ä¸‹å…³é”®insightsï¼š\n\n{research_content[:1000]}\n\n## æ·±åº¦åˆ†æ\n\né€šè¿‡å¯¹ç›¸å…³æ•°æ®çš„åˆ†æï¼Œæˆ‘ä»¬å¯ä»¥å¾—å‡ºé‡è¦ç»“è®ºã€‚è¿™ä¸€ç« èŠ‚çš„ç ”ç©¶ä¸ºæ•´ä½“æŠ¥å‘Šæä¾›äº†åšå®çš„åŸºç¡€ï¼Œä¸ºåç»­åˆ†æå¥ å®šäº†é‡è¦åŸºç¡€ã€‚\n\n## å…³é”®è¦ç‚¹\n\n1. æ ¸å¿ƒå‘ç°å’Œå…³é”®æ•°æ®ç‚¹\n2. é‡è¦è¶‹åŠ¿å’Œæ¨¡å¼è¯†åˆ«\n3. å¯¹æ•´ä½“ç ”ç©¶çš„æ„ä¹‰å’Œä»·å€¼\n\nè¿™äº›å‘ç°å°†ä¸ºæˆ‘ä»¬çš„ç»¼åˆåˆ†ææä¾›é‡è¦æ”¯æ’‘ã€‚"
        else:
            full_response = f"# {title}\n\n## æ¦‚è¿°\n\næœ¬ç« èŠ‚å›´ç»•'{title}'è¿™ä¸€æ ¸å¿ƒä¸»é¢˜å±•å¼€æ·±å…¥åˆ†æã€‚{description}\n\n## å…³é”®åˆ†æ\n\né€šè¿‡ç»¼åˆç ”ç©¶ï¼Œæˆ‘ä»¬è¯†åˆ«å‡ºä»¥ä¸‹å‡ ä¸ªé‡è¦ç»´åº¦ï¼š\n\n1. **èƒŒæ™¯ä¸ç°çŠ¶**ï¼šå½“å‰å‘å±•çŠ¶å†µå’Œä¸»è¦ç‰¹å¾\n2. **æ ¸å¿ƒè¦ç´ **ï¼šå½±å“å‘å±•çš„å…³é”®å› ç´ \n3. **è¶‹åŠ¿è¯†åˆ«**ï¼šæœªæ¥å‘å±•çš„å¯èƒ½æ–¹å‘\n4. **å®è·µæ„ä¹‰**ï¼šå¯¹å®é™…åº”ç”¨çš„æŒ‡å¯¼ä»·å€¼\n\n## æ·±å…¥æ´å¯Ÿ\n\nåŸºäºæˆ‘ä»¬çš„åˆ†ææ¡†æ¶ï¼Œè¿™ä¸€é¢†åŸŸå‘ˆç°å‡ºå¤æ‚è€Œå¤šæ ·çš„å‘å±•æ€åŠ¿ã€‚ç›¸å…³stakeholderséœ€è¦ä»å¤šä¸ªè§’åº¦æ¥ç†è§£å’ŒæŠŠæ¡å‘å±•æœºé‡ã€‚\n\n## å°ç»“\n\n{title}ä½œä¸ºé‡è¦ç ”ç©¶ä¸»é¢˜ï¼Œå…¶å‘å±•çŠ¶å†µå’Œæœªæ¥è¶‹åŠ¿å€¼å¾—æŒç»­å…³æ³¨ã€‚æœ¬ç« èŠ‚çš„åˆ†æä¸ºåç»­æ·±å…¥ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚"

    # å¦‚æœæ²¡æœ‰è·å–åˆ°å“åº”ï¼Œæä¾›é»˜è®¤ä¿¡æ¯
    if not full_response:
        full_response = "å†™ä½œå®Œæˆï¼Œä½†æœªè·å–åˆ°æœ€ç»ˆç»“æœ"

    # ä¿å­˜å†™ä½œç»“æœ
    word_count = len(full_response.split()) if full_response else 0
    writing_results = state.get("writing_results", {})
    writing_results[section_id] = {
        "title": title,
        "content": full_response,
        "word_count": word_count,
        "timestamp": time.time()
    }
    state["writing_results"] = writing_results
    state["execution_path"] = state.get("execution_path", []) + ["writing"]

    # å†™ä½œæ­¥éª¤å®Œæˆ

    return state

    # Any real exceptions will be caught by the LangGraph engine.
    # We no longer need a broad try/except block here that would incorrectly catch the Interrupt signal.

async def integration_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    _ = config  # LangGraphä¼šä¼ å…¥configï¼Œä½†æ­¤èŠ‚ç‚¹æš‚æ—¶æœªä½¿ç”¨
    """æ•´åˆèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    topic = state.get("topic", "")
    writing_results = state.get("writing_results", {})
    sections = state.get("sections", [])

    # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
    final_sections = []
    total_words = 0

    for section in sections:
        section_id = section.get("id", "")
        if section_id in writing_results:
            section_data = writing_results[section_id]
            final_sections.append(section_data)
            total_words += section_data.get("word_count", 0)

    final_report = {
        "title": f"{topic} - æ™ºèƒ½ç ”ç©¶æŠ¥å‘Š",
        "topic": topic,
        "sections": final_sections,
        "total_sections": len(final_sections),
        "total_words": total_words,
        "generation_method": "langgraph_intelligent_research",
        "execution_path": state.get("execution_path", []),
        "generation_timestamp": time.time()
    }

    state["final_report"] = final_report
    state["task_completed"] = True
    state["execution_path"] = state.get("execution_path", []) + ["integration"]

    # æŠ¥å‘Šæ•´åˆå®Œæˆ

    return state

# ============================================================================
# è·¯ç”±å‡½æ•° - LangGraphæ¡ä»¶è·¯ç”±
# ============================================================================

def route_after_intelligent_supervisor(state: IntelligentResearchState) -> str:
    """æ™ºèƒ½Supervisoråçš„è·¯ç”±å†³ç­–"""
    next_action = state.get("next_action", "integration")

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 10)

    if iteration_count >= max_iterations:
        return "integration"

    if next_action == "research":
        return "research"
    elif next_action == "writing":
        return "writing"
    elif next_action == "quality_check":
        return "intelligent_supervisor"  # å›åˆ°supervisorè¿›è¡Œæ›´è¯¦ç»†åˆ†æ
    else:
        return "integration"

def route_after_research(state: IntelligentResearchState) -> str:
    """ç ”ç©¶åçš„è·¯ç”±å†³ç­– - å›åˆ°æ™ºèƒ½supervisor"""
    _ = state  # å‚æ•°æš‚æ—¶ä¸ä½¿ç”¨ä½†ä¿ç•™æ¥å£
    return "intelligent_supervisor"

def route_after_writing(state: IntelligentResearchState) -> str:
    """å†™ä½œåçš„è·¯ç”±å†³ç­– - å›åˆ°æ™ºèƒ½supervisor"""
    _ = state  # å‚æ•°æš‚æ—¶ä¸ä½¿ç”¨ä½†ä¿ç•™æ¥å£
    return "intelligent_supervisor"

def should_end(state: IntelligentResearchState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»“æŸ"""
    if state.get("task_completed", False):
        return END
    else:
        return "intelligent_supervisor"

# ============================================================================
# å›¾æ„å»º - LangGraphæ ¸å¿ƒ
# ============================================================================

def create_intelligent_research_graph(checkpointer: Optional[InMemorySaver] = None):
    _ = checkpointer  # ä¿ç•™checkpointerå‚æ•°ä»¥å¤‡å°†æ¥ä½¿ç”¨
    """åˆ›å»ºæ™ºèƒ½ç ”ç©¶å·¥ä½œæµå›¾ - ä½¿ç”¨æ™ºèƒ½Supervisor"""
    workflow = StateGraph(IntelligentResearchState)

    # æ·»åŠ èŠ‚ç‚¹ - ä½¿ç”¨æ™ºèƒ½supervisor
    workflow.add_node("intelligent_supervisor", supervisor_node)
    workflow.add_node("research", research_node)
    workflow.add_node("writing", writing_node)
    workflow.add_node("integration", integration_node)

    # è®¾ç½®èµ·å§‹èŠ‚ç‚¹ - ä»æ™ºèƒ½supervisorå¼€å§‹
    workflow.add_edge(START, "intelligent_supervisor")

    # æ·»åŠ æ¡ä»¶è·¯ç”± - æ™ºèƒ½supervisorçš„å†³ç­–è·¯ç”±
    workflow.add_conditional_edges(
        "intelligent_supervisor",
        route_after_intelligent_supervisor,
        {
            "research": "research",
            "writing": "writing",
            "integration": "integration",
            "intelligent_supervisor": "intelligent_supervisor"  # æ”¯æŒè´¨é‡æ£€æŸ¥å¾ªç¯
        }
    )

    # ç ”ç©¶å’Œå†™ä½œå®Œæˆåéƒ½å›åˆ°æ™ºèƒ½supervisorè¿›è¡Œè´¨é‡è¯„ä¼°
    workflow.add_conditional_edges(
        "research",
        route_after_research,
        {
            "intelligent_supervisor": "intelligent_supervisor"
        }
    )

    workflow.add_conditional_edges(
        "writing",
        route_after_writing,
        {
            "intelligent_supervisor": "intelligent_supervisor"
        }
    )

    # æ•´åˆå®Œæˆåçš„ç»“æŸåˆ¤æ–­
    workflow.add_conditional_edges(
        "integration",
        should_end,
        {
            "intelligent_supervisor": "intelligent_supervisor",
            END: END
        }
    )

    return workflow
