"""
æ ¸å¿ƒå›¾æ¨¡å—
é«˜çº§äº¤äº’å¼æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ä¸»è¦å·¥ä½œæµå›¾
é›†æˆMulti-Agentåä½œã€Human-in-loopäº¤äº’å’Œæµå¼è¾“å‡º
"""

import json
import time
import asyncio
import uuid
import os
from typing import Dict, Any, List

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from langgraph.graph import StateGraph, END, START
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.types import interrupt
import logging

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from state import (
    DeepResearchState, ReportMode, TaskStatus, InteractionType,
    ReportOutline, ReportSection, ResearchResult,
    update_performance_metrics,
    update_task_status, add_user_interaction, add_node_output
)

# å¯¼å…¥å­å›¾æ¨¡å—
from subgraphs.deepresearch.graph import (
    create_intelligent_research_graph
)
# å¯¼å…¥æ ‡å‡†åŒ–æµå¼è¾“å‡ºç³»ç»Ÿ
from writer.core import create_stream_writer, create_workflow_processor

# å¯¼å…¥é€šç”¨ä¸­æ–­èŠ‚ç‚¹
from common import create_confirmation_node

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm() -> ChatOpenAI:
    """åˆ›å»ºLLMå®ä¾‹"""
    return ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.7,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197",
    )

# ç¼–è¯‘å­å›¾ï¼ˆå…¨å±€å˜é‡ï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼‰- ä½¿ç”¨updateå­å›¾
_intelligent_research_subgraph = None

def get_intelligent_research_subgraph():
    """è·å–æ™ºèƒ½ç ”ç©¶å­å›¾å®ä¾‹"""
    global _intelligent_research_subgraph
    if _intelligent_research_subgraph is None:
        workflow = create_intelligent_research_graph()
        _intelligent_research_subgraph = workflow.compile()
    return _intelligent_research_subgraph

def create_update_subgraph_state(state: DeepResearchState) -> Dict[str, Any]:
    """
    åˆ›å»ºupdateå­å›¾çš„åˆå§‹çŠ¶æ€
    å°†ä¸»å›¾çŠ¶æ€è½¬æ¢ä¸ºupdateå­å›¾æ‰€éœ€çš„çŠ¶æ€æ ¼å¼
    """
    outline = state.get("outline", {})
    sections = outline.get("sections", []) if outline else []

    # è½¬æ¢ç« èŠ‚æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½æœ‰id
    formatted_sections = []
    for i, section in enumerate(sections):
        formatted_section = {
            "id": section.get("id", f"section_{i}"),
            "title": section.get("title", f"ç« èŠ‚ {i+1}"),
            "description": section.get("description", ""),
            "key_points": section.get("key_points", []),
            "research_queries": section.get("research_queries", [])
        }
        formatted_sections.append(formatted_section)
    # åˆ›å»ºupdateå­å›¾çŠ¶æ€
    subgraph_state = {
        "messages": [],
        "user_input": f"è¯·ä¸ºä¸»é¢˜'{state.get('topic', '')}'ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Š",
        "topic": state.get("topic", ""),
        "mode": "interactive",
        "sections": formatted_sections,
        "current_section_index": 0,
        "research_results": {},
        "writing_results": {},
        "polishing_results": {},
        "final_report": {},
        "execution_path": [],
        "iteration_count": 0,
        "max_iterations": 10,
        "next_action": "research",
        "task_completed": False,
        "error_log": [],
        "section_attempts": {}
    }

    return subgraph_state

async def report_generation(state: DeepResearchState) -> DeepResearchState:
    """
    è°ƒç”¨æ™ºèƒ½ç ”ç©¶å­å›¾ - ç»Ÿä¸€æµå¼è¾“å‡º

    è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸»å›¾å’Œå­å›¾ä¹‹é—´çš„çŠ¶æ€è½¬æ¢ï¼Œæ™ºèƒ½è¯†åˆ«Agentå·¥ä½œæµç¨‹
    """
    # åˆ›å»ºå·¥ä½œæµç¨‹å¤„ç†å™¨ - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    processor = create_workflow_processor("intelligent_research", "æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")
    processor.writer.processing("å¼€å§‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")

    # è·å–å­å›¾å®ä¾‹
    subgraph = get_intelligent_research_subgraph()
    subgraph_input = create_update_subgraph_state(state)

    processor.writer.processing(
        "å‡†å¤‡ç ”ç©¶è®¡åˆ’",
        sections_count=len(subgraph_input.get("sections", []))
    )
    
    # ç”¨äºæ”¶é›†æœ€ç»ˆç»“æœçš„å˜é‡
    subgraph_output = {}
    updated_sections = []
    new_research_results = []
    
    # æµå¼è°ƒç”¨å­å›¾ï¼Œä½¿ç”¨å¢å¼ºçš„processorç»Ÿä¸€å¤„ç†åµŒå¥—æµå¼è¾“å‡º
    async for chunk in subgraph.astream(subgraph_input, stream_mode=["updates", "messages"], subgraphs=True):
        # ä½¿ç”¨å¢å¼ºçš„processorç»Ÿä¸€å¤„ç†æ‰€æœ‰ç±»å‹çš„chunk
        processor.process_chunk(chunk)
        # åŒæ—¶æ”¶é›†å®é™…æ•°æ®ç”¨äºçŠ¶æ€æ›´æ–°ï¼Œå¤„ç†åµŒå¥—ç»“æ„
        if isinstance(chunk, tuple):
            if len(chunk) >= 3:
                # åµŒå¥—å­å›¾æ ¼å¼: (('subgraph_id',), 'updates', data)
                _, chunk_type, chunk_data = chunk[0], chunk[1], chunk[2]
            elif len(chunk) >= 2:
                # æ™®é€šæ ¼å¼: ('updates', data)
                chunk_type, chunk_data = chunk[0], chunk[1]
            else:
                continue
            
            if chunk_type == "updates" and isinstance(chunk_data, dict):
                for node_output in chunk_data.values():
                    if node_output and isinstance(node_output, dict):
                        subgraph_output.update(node_output)
                        
                        # æ”¶é›†ç« èŠ‚æ›´æ–°
                        if "sections" in node_output:
                            sections_data = node_output["sections"]
                            if isinstance(sections_data, list):
                                for section_data in sections_data:
                                    updated_section = {
                                        "title": section_data.get("title", ""),
                                        "content": section_data.get("content", ""),
                                        "word_count": section_data.get("word_count", 0),
                                        "status": "completed"
                                    }
                                    if not any(s.get("title") == updated_section["title"] for s in updated_sections):
                                        updated_sections.append(updated_section)
                        
                        # æ”¶é›†ç ”ç©¶ç»“æœ
                        if "research_results" in node_output:
                            research_results = node_output["research_results"]
                            if isinstance(research_results, dict):
                                for research_data in research_results.values():
                                    research_result = ResearchResult(
                                        id=str(uuid.uuid4()),
                                        query=f"ç ”ç©¶ç« èŠ‚: {research_data.get('title', '')}",
                                        source_type="subgraph",
                                        title=research_data.get("title", ""),
                                        content=research_data.get("content", ""),
                                        url="",
                                        relevance_score=0.9,
                                        timestamp=research_data.get("timestamp", time.time()),
                                        section_id=research_data.get("id", "")
                                    )
                                    if not any(r.title == research_result.title for r in new_research_results):
                                        new_research_results.append(research_result)
    # å¤„ç†æœ€ç»ˆç»“æœ
    if subgraph_output.get("final_report"):

        final_report = subgraph_output["final_report"]
        final_sections_data = final_report.get("sections", [])

        # ç¡®ä¿æ‰€æœ‰ç« èŠ‚éƒ½è¢«å¤„ç†
        for section_data in final_sections_data:
            updated_section = {
                "title": section_data.get("title", ""),
                "content": section_data.get("content", ""),
                "word_count": section_data.get("word_count", 0),
                "status": "completed"
            }
            if not any(s.get("title") == updated_section["title"] for s in updated_sections):
                updated_sections.append(updated_section)

        # æ›´æ–°çŠ¶æ€
        updated_state = {
            **state,
            "sections": updated_sections,
            "research_results": state.get("research_results", []) + new_research_results,
            "content_creation_completed": True,
            "completed_sections_count": len(updated_sections),
            "performance_metrics": {
                **state.get("performance_metrics", {}),
                "sections_completed": len(updated_sections),
                "total_sections": len(updated_sections),
                "total_words": final_report.get("total_words", 0)
            }
        }

        processor.writer.final_result(
            f"æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            {
                "sections_count": len(updated_sections),
                "total_words": final_report.get("total_words", 0),
                "research_findings": len(new_research_results)
            }
        )
        return updated_state
    else:
        # è¿”å›éƒ¨åˆ†ç»“æœ
        updated_state = {
            **state,
            "sections": updated_sections,
            "research_results": state.get("research_results", []) + new_research_results,
            "content_creation_completed": len(updated_sections) > 0,
            "completed_sections_count": len(updated_sections),
            "performance_metrics": {
                **state.get("performance_metrics", {}),
                "sections_completed": len(updated_sections),
                "total_sections": len(updated_sections),
                "total_words": sum(section.get("word_count", 0) for section in updated_sections)
            }
        }
        
        processor.writer.processing(
            "éƒ¨åˆ†å†…å®¹ç”Ÿæˆå®Œæˆ",
            sections_count=len(updated_sections),
            is_partial=True
        )
        return updated_state

def convert_research_data_to_results(research_data: List[Dict[str, Any]]) -> List[ResearchResult]:
    """å°†å­å›¾çš„ç ”ç©¶æ•°æ®è½¬æ¢ä¸ºä¸»å›¾çš„ ResearchResult æ ¼å¼"""
    results = []

    for item in research_data:
        try:
            result = ResearchResult(
                id=item.get("id", str(uuid.uuid4())),
                query=item.get("query", ""),
                source_type="web",
                title=item.get("title", ""),
                content=item.get("content", ""),
                url=item.get("url", ""),
                relevance_score=item.get("relevance_score", 0.8),
                timestamp=item.get("timestamp", time.time()),
                section_id=item.get("section_id", "")
            )
            results.append(result)
        except Exception as e:
            logger.warning(f"è½¬æ¢ç ”ç©¶æ•°æ®å¤±è´¥: {e}")
            continue

    return results

async def intelligent_section_processing_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """
    æ™ºèƒ½ç« èŠ‚å¤„ç†èŠ‚ç‚¹ - ä½¿ç”¨updateå­å›¾è¿›è¡Œæ•´ä½“ç ”ç©¶å’Œå†™ä½œ

    è¿™ä¸ªèŠ‚ç‚¹çš„ä½œç”¨ï¼š
    1. è·å–å¤§çº²ä¸­çš„æ‰€æœ‰ç« èŠ‚
    2. è°ƒç”¨updateå­å›¾è¿›è¡Œæ™ºèƒ½ç ”ç©¶å’Œå†™ä½œ
    3. å­å›¾å†…éƒ¨å¤„ç†ï¼šæ™ºèƒ½Supervisor â†’ ç ”ç©¶ â†’ å†™ä½œ â†’ æ•´åˆ
    4. è¿”å›å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Š
    """
    # ä½¿ç”¨æ‰å¹³åŒ–Writer - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    writer = create_stream_writer("intelligent_section_processing", "æ™ºèƒ½ç« èŠ‚å¤„ç†")
    writer.processing("å¼€å§‹æ™ºèƒ½ç ”ç©¶å¤„ç†ï¼ˆä½¿ç”¨updateå­å›¾ï¼‰")

    try:
        outline = state.get("outline", {})
        sections = outline.get("sections", []) if outline else []

        if not sections:
            writer.error("æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚ä¿¡æ¯", "NoSectionsError")
            return state

        writer.processing(
            f"å‡†å¤‡ä½¿ç”¨æ™ºèƒ½Supervisorå¤„ç† {len(sections)} ä¸ªç« èŠ‚",
            total_sections=len(sections)
        )

        # è°ƒç”¨updateå­å›¾è¿›è¡Œæ•´ä½“å¤„ç†
        writer.processing("å¯åŠ¨æ™ºèƒ½ç ”ç©¶å­å›¾")

        # ç›´æ¥è°ƒç”¨å­å›¾ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼ï¼‰
        updated_state = await call_intelligent_research_subgraph(state)

        # æ£€æŸ¥å¤„ç†ç»“æœ
        if updated_state.get("content_creation_completed"):
            completed_sections = updated_state.get("sections", [])
            total_words = updated_state.get("performance_metrics", {}).get("total_words", 0)

            writer.processing(
                "æ™ºèƒ½ç ”ç©¶å®Œæˆ",
                completed_sections=len(completed_sections),
                total_sections=len(sections),
                total_words=total_words
            )

            logger.info(f"æ™ºèƒ½ç ”ç©¶å®Œæˆ: {len(completed_sections)} ä¸ªç« èŠ‚, æ€»å­—æ•°: {total_words}")
            return updated_state
        else:
            writer.processing("å­å›¾å¤„ç†æœªå®Œæˆï¼Œè¿”å›åŸçŠ¶æ€")
            logger.warning("å­å›¾å¤„ç†æœªå®Œæˆ")
            return state

    except Exception as e:
        logger.error(f"æ™ºèƒ½ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")
        writer.error(f"ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}", "IntelligentSectionProcessingError")

        state["error_log"] = state.get("error_log", []) + [f"æ™ºèƒ½ç« èŠ‚å¤„ç†é”™è¯¯: {str(e)}"]
        return state

# ============================================================================
# å¤§çº²ç”ŸæˆèŠ‚ç‚¹
# ============================================================================

async def outline_generation(state: DeepResearchState, config=None) -> DeepResearchState:
    """å¤§çº²ç”ŸæˆèŠ‚ç‚¹ - æ”¯æŒæµå¼è¾“å‡ºæ±‡æ€»"""
    # ä½¿ç”¨æ‰å¹³åŒ–å¤„ç†å™¨ - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    processor = create_workflow_processor("outline_generation", "å¤§çº²ç”Ÿæˆå™¨")
    processor.writer.processing("å¼€å§‹ç”Ÿæˆæ·±åº¦ç ”ç©¶å¤§çº²")

    try:
        start_time = time.time()
        llm = create_llm()
        parser = JsonOutputParser(pydantic_object=ReportOutline)
        
        # æ„å»ºé«˜çº§å¤§çº²ç”Ÿæˆæç¤º
        outline_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šå¤§çº²è®¾è®¡ä¸“å®¶ã€‚è¯·ç”Ÿæˆè¯¦ç»†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚
            
            è¦æ±‚ï¼š
            1. å¤§çº²ç¬¦åˆ{report_type}æŠ¥å‘Šçš„æ ‡å‡†ç»“æ„
            2. é’ˆå¯¹{target_audience}è¯»è€…ç¾¤ä½“è®¾è®¡
            3. ç ”ç©¶æ·±åº¦ä¸º{depth_level}çº§åˆ«
            4. ç›®æ ‡å­—æ•°çº¦{target_length}å­—
            5. æ¯ä¸ªç« èŠ‚åŒ…å«ç ”ç©¶æŸ¥è¯¢å…³é”®è¯
            6. ç« èŠ‚ä¼˜å…ˆçº§åˆç†åˆ†é…
            
            {format_instructions}"""),
            ("human", """
            è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸“ä¸šçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š
            
            ç ”ç©¶ä¸»é¢˜ï¼š{topic}
            æŠ¥å‘Šç±»å‹ï¼š{report_type}
            ç›®æ ‡è¯»è€…ï¼š{target_audience}
            æ·±åº¦çº§åˆ«ï¼š{depth_level}
            
            è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€ä¾¿äºæ·±åº¦ç ”ç©¶ã€‚
            """)
        ])
        
        input_data = {
            "topic": state["topic"],
            "report_type": state["report_type"],
            "target_audience": state["target_audience"],
            "depth_level": state["depth_level"],
            "target_length": state["target_length"],
            "format_instructions": parser.get_format_instructions()
        }
        
        processor.writer.processing("æ­£åœ¨ç”Ÿæˆä¸“ä¸šå¤§çº²...")

        # åˆ›å»ºLLMé“¾å¹¶æµå¼æ‰§è¡Œ
        llm_chain = outline_prompt | llm | parser

        outline_data = None
        chunk_count = 0
        current_outline_display = ""
        
        async for chunk in llm_chain.astream(input_data, config=config):
            outline_data = chunk
            chunk_count += 1

            # å®æ—¶æ˜¾ç¤ºå¤§çº²å†…å®¹ï¼ˆæ¯5ä¸ªchunkæ›´æ–°ä¸€æ¬¡ä»¥å‡å°‘é¢‘ç‡ï¼‰
            if chunk_count % 5 == 0:
                # æ„å»ºå½“å‰å¤§çº²çš„æ˜¾ç¤ºæ–‡æœ¬
                if hasattr(chunk, 'title'):
                    current_outline_display = f"ğŸ¯ æ ‡é¢˜ï¼š{chunk.title}\n"
                if hasattr(chunk, 'sections') and chunk.sections:
                    current_outline_display += f"ğŸ“š ç« èŠ‚({len(chunk.sections)}ä¸ª):\n"
                    for i, section in enumerate(chunk.sections[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç« èŠ‚
                        if hasattr(section, 'title'):
                            current_outline_display += f"  {i}. {section.title}\n"
                            if hasattr(section, 'description'):
                                current_outline_display += f"     {section.description}\n"
                    if len(chunk.sections) > 3:
                        current_outline_display += f"  ... è¿˜æœ‰{len(chunk.sections)-3}ä¸ªç« èŠ‚"
                
                processor.writer.content(
                    str(chunk),
                    current_outline=current_outline_display,
                    chunk_count=chunk_count
                )

                # å¦‚æœå¤§çº²åŸºæœ¬å®Œæ•´ï¼Œæå‰æ˜¾ç¤º
                if hasattr(chunk, 'title') and hasattr(chunk, 'sections') and len(chunk.sections) >= 3:
                    processor.writer.processing(
                        "å¤§çº²ç»“æ„å·²ç”Ÿæˆï¼Œæ­£åœ¨å®Œå–„ç»†èŠ‚...",
                        partial_outline=chunk,
                        streaming_content=current_outline_display
                    )
              
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if hasattr(outline_data, 'dict'):
            outline_dict = outline_data.dict()
        else:
            outline_dict = dict(outline_data) if outline_data else {}
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "outline_generator", execution_time)
        update_task_status(state, "outline_generation", TaskStatus.COMPLETED)
        
        state["outline"] = outline_dict
        state["current_step"] = "outline_generated"
        state["execution_path"] = state["execution_path"] + ["outline_generation"]
        
        add_node_output(
            state, 
            "outline_generation", 
            outline_data,  # åªå­˜å‚¨åŸå§‹ç´¯è®¡çš„content
            execution_time=execution_time,
            word_count=len(outline_data),
            status="completed"
        )
        
        processor.writer.processing(
            "æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆ",
            sections_count=len(outline_dict.get("sections", [])),
            execution_time=execution_time,
            outline_data=outline_dict,
            display_text=outline_data
        )
        
        from writer.config import get_writer_config
        config = get_writer_config()
        if not config.is_node_streaming_enabled("outline_generation"):
            from langgraph.config import get_stream_writer
            writer = get_stream_writer()
            if writer:
                aggregated_message = {
                    "message_type": "node_output_complete",
                    "content": outline_data,  # ğŸ”¥ åªå‘é€æœ€åŸå§‹çš„ç´¯è®¡content
                    "node": "outline_generation",
                    "timestamp": time.time(),
                    "duration": execution_time,
                    "word_count": len(outline_data)
                }
                writer(aggregated_message)
        return state
        
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        processor.writer.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}", "OutlineGenerationError")
        
        # é”™è¯¯æƒ…å†µä¹Ÿè®°å½•åˆ° node_outputs
        add_node_output(
            state, 
            "outline_generation", 
            f"é”™è¯¯ï¼š{str(e)}",
            status="failed",
            error=str(e)
        )
        
        state["error_log"] = state["error_log"] + [f"å¤§çº²ç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "outline_generation_failed"
        update_task_status(state, "outline_generation", TaskStatus.FAILED)
        return state

# ============================================================================
# äº¤äº’ç¡®è®¤èŠ‚ç‚¹ - ä½¿ç”¨é€šç”¨ä¸­æ–­èŠ‚ç‚¹
# ============================================================================

def create_interaction_node_legacy(interaction_type: InteractionType):
    """åˆ›å»ºäº¤äº’ç¡®è®¤èŠ‚ç‚¹çš„å·¥å‚å‡½æ•° - ä½¿ç”¨ç»Ÿä¸€çš„ä¸­æ–­æœºåˆ¶"""

    async def interaction_node(state: DeepResearchState, config=None) -> DeepResearchState:
        """é€šç”¨äº¤äº’ç¡®è®¤èŠ‚ç‚¹ - ç»Ÿä¸€ä¸­æ–­å¤„ç†æ ¼å¼"""
        # ä½¿ç”¨æ‰å¹³åŒ–å¤„ç†å™¨
        processor = create_workflow_processor(f"interaction_{interaction_type.value}", f"{interaction_type.value}_äº¤äº’")
        processor.writer.processing(f"å¼€å§‹{interaction_type.value}ç¡®è®¤")

        interaction_config = get_interaction_config(interaction_type)
        mode = state["mode"]

        # Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡
        if mode == ReportMode.COPILOT:
            processor.writer.processing("Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡")

            state["approval_status"][interaction_type.value] = True
            state["user_feedback"][interaction_type.value] = {"approved": True, "auto": True}

            processor.writer.processing(f"Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡", auto_approved=True)

            state["messages"] = state["messages"] + [
                AIMessage(content=f"ğŸ¤– Copilotæ¨¡å¼ï¼š{interaction_config['copilot_message']}")
            ]

            return state

        # Interactiveæ¨¡å¼ï¼šä½¿ç”¨ç»Ÿä¸€çš„ä¸­æ–­æœºåˆ¶
        processor.writer.processing("å‡†å¤‡ç”¨æˆ·ç¡®è®¤...")

        # æ„å»ºä¸­æ–­è¯·æ±‚ï¼Œæ ¼å¼ä¸å·¥å…·åŒ…è£…å™¨ä¸€è‡´
        message_content = format_interaction_message(state, interaction_type, interaction_config)

        # åˆ›å»ºæ ‡å‡†åŒ–çš„ä¸­æ–­è¯·æ±‚
        request: HumanInterrupt = {
            "action_request": {
                "action": f"confirm_{interaction_type.value}",
                "args": {
                    "interaction_type": interaction_type.value,
                    "title": interaction_config["title"],
                    "message": message_content,
                    "options": interaction_config["options"]
                }
            },
            "config": {
                "allow_accept": True,
                "allow_edit": False,  # ç¡®è®¤èŠ‚ç‚¹é€šå¸¸ä¸å…è®¸ç¼–è¾‘
                "allow_respond": True
            },
            "description": f"è¯·ç¡®è®¤{interaction_config['title']}ï¼š\n\n{message_content}\n\nå¯é€‰æ“ä½œï¼š\n- è¾“å…¥ 'yes' ç¡®è®¤é€šè¿‡\n- è¾“å…¥ 'no' æ‹’ç»\n- è¾“å…¥ 'response' æä¾›è‡ªå®šä¹‰åé¦ˆ",
        }

        processor.writer.processing("ç­‰å¾…ç”¨æˆ·ç¡®è®¤...")

        # è°ƒç”¨ç»Ÿä¸€çš„ä¸­æ–­å‡½æ•°
        response = types.interrupt(request)

        processor.writer.processing("å¤„ç†ç”¨æˆ·å“åº”...")

        # æ ‡å‡†åŒ–å“åº”å¤„ç†
        if response["type"] == "accept":
            # ç”¨æˆ·ç¡®è®¤é€šè¿‡
            approved = True
            user_feedback = {"approved": True, "type": "accept"}
            result_message = f"âœ… {interaction_config['title']}ï¼šç¡®è®¤é€šè¿‡"

        elif response["type"] == "reject":
            # ç”¨æˆ·æ‹’ç»
            approved = False
            user_feedback = {"approved": False, "type": "reject"}
            result_message = f"âŒ {interaction_config['title']}ï¼šè¢«æ‹’ç»"

        elif response["type"] == "response":
            # ç”¨æˆ·æä¾›è‡ªå®šä¹‰åé¦ˆ
            user_feedback_content = response.get("args", "")
            approved = True  # é»˜è®¤è®¤ä¸ºæä¾›åé¦ˆå°±æ˜¯é€šè¿‡
            user_feedback = {"approved": True, "type": "response", "content": user_feedback_content}
            result_message = f"ğŸ’¬ {interaction_config['title']}ï¼šç”¨æˆ·åé¦ˆ - {user_feedback_content}"

        else:
            # æœªçŸ¥å“åº”ç±»å‹ï¼Œé»˜è®¤æ‹’ç»
            approved = False
            user_feedback = {"approved": False, "type": "unknown", "raw_response": response}
            result_message = f"â“ {interaction_config['title']}ï¼šæœªçŸ¥å“åº”ï¼Œé»˜è®¤æ‹’ç»"

        # æ›´æ–°çŠ¶æ€
        state["approval_status"][interaction_type.value] = approved
        state["user_feedback"][interaction_type.value] = user_feedback

        # è®°å½•äº¤äº’å†å²
        add_user_interaction(state, interaction_type.value, user_feedback)

        # æ·»åŠ ç¡®è®¤æ¶ˆæ¯
        state["messages"] = state["messages"] + [AIMessage(content=result_message)]

        processor.writer.processing(
            result_message,
            approved=approved,
            user_feedback=user_feedback,
            interaction_type=interaction_type.value
        )

        return state

    return interaction_node

def get_interaction_config(interaction_type: InteractionType) -> Dict[str, Any]:
    """è·å–äº¤äº’é…ç½®"""
    configs = {
        InteractionType.OUTLINE_CONFIRMATION: {
            "title": "å¤§çº²ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤æŠ¥å‘Šå¤§çº²ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶",
            "options": ["ç¡®è®¤ç»§ç»­", "ä¿®æ”¹å¤§çº²", "é‡æ–°ç”Ÿæˆ"],
            "default": "ç¡®è®¤ç»§ç»­"
        },
        InteractionType.RESEARCH_PERMISSION: {
            "title": "ç ”ç©¶æƒé™ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨å…è®¸è¿›è¡Œæ·±åº¦ç ”ç©¶å’Œæ•°æ®æ”¶é›†",
            "options": ["å…è®¸ç ”ç©¶", "è·³è¿‡ç ”ç©¶", "é™åˆ¶èŒƒå›´"],
            "default": "å…è®¸ç ”ç©¶"
        },
        InteractionType.ANALYSIS_APPROVAL: {
            "title": "åˆ†æç»“æœå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤åˆ†æç»“æœï¼Œç»§ç»­å†…å®¹ç”Ÿæˆ",
            "options": ["ç¡®è®¤åˆ†æ", "é‡æ–°åˆ†æ", "è°ƒæ•´æ–¹å‘"],
            "default": "ç¡®è®¤åˆ†æ"
        },
        InteractionType.CONTENT_REVIEW: {
            "title": "å†…å®¹å®¡æŸ¥",
            "copilot_message": "è‡ªåŠ¨é€šè¿‡å†…å®¹å®¡æŸ¥ï¼Œå‡†å¤‡æœ€ç»ˆæŠ¥å‘Š",
            "options": ["é€šè¿‡å®¡æŸ¥", "ä¿®æ”¹å†…å®¹", "é‡å†™ç« èŠ‚"],
            "default": "é€šè¿‡å®¡æŸ¥"
        },
        InteractionType.FINAL_APPROVAL: {
            "title": "æœ€ç»ˆå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨å®Œæˆæœ€ç»ˆå®¡æ‰¹ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "options": ["æœ€ç»ˆç¡®è®¤", "å†æ¬¡ä¿®æ”¹", "ç”Ÿæˆæ–°ç‰ˆæœ¬"],
            "default": "æœ€ç»ˆç¡®è®¤"
        }
    }
    return configs.get(interaction_type, {})

def format_interaction_message(state: DeepResearchState, interaction_type: InteractionType, config: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–äº¤äº’æ¶ˆæ¯"""
    if interaction_type == InteractionType.OUTLINE_CONFIRMATION:
        outline = state.get("outline", {})
        sections_text = "\n".join([
            f"  {i+1}. {section.get('title', 'æœªçŸ¥ç« èŠ‚')}\n     {section.get('description', 'æ— æè¿°')}"
            for i, section in enumerate(outline.get("sections", []))
        ])
        return f"""
            è¯·ç¡®è®¤ä»¥ä¸‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š

            ğŸ“‹ æ ‡é¢˜ï¼š{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}

            ğŸ“ æ‘˜è¦ï¼š{outline.get('executive_summary', 'æ— æ‘˜è¦')}

            ğŸ“š ç« èŠ‚ç»“æ„ï¼š
            {sections_text}

            ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline.get('methodology', 'æœªæŒ‡å®š')}
            ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline.get('estimated_length', 0):,}å­—
            ğŸ‘¥ ç›®æ ‡è¯»è€…ï¼š{outline.get('target_audience', 'æœªçŸ¥')}
        """
    elif interaction_type == InteractionType.RESEARCH_PERMISSION:
        return f"""
            æ˜¯å¦å…è®¸å¯¹ä¸»é¢˜ã€Œ{state['topic']}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Ÿ

            ç ”ç©¶å°†åŒ…æ‹¬ï¼š
            - å¤šæºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†
            - ç›¸å…³æ¡ˆä¾‹å’Œæ•°æ®åˆ†æ
            - è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
            - ä¸“ä¸šæ´å¯Ÿç”Ÿæˆ

            é¢„è®¡ç ”ç©¶æ—¶é—´ï¼š5-10åˆ†é’Ÿ
        """
    else:
        return f"è¯·ç¡®è®¤{config['title']}ç›¸å…³è®¾ç½®"

# ============================================================================
# ä½¿ç”¨é€šç”¨ä¸­æ–­èŠ‚ç‚¹åˆ›å»ºå…·ä½“çš„äº¤äº’èŠ‚ç‚¹
# ============================================================================

def get_outline_data(state: DeepResearchState) -> Dict[str, Any]:
    """è·å–å¤§çº²ç¡®è®¤æ‰€éœ€çš„æ•°æ®"""
    outline = state.get("outline", {})
    sections_text = "\n".join([
        f"  {i+1}. {section.get('title', 'æœªçŸ¥ç« èŠ‚')}\n     {section.get('description', 'æ— æè¿°')}"
        for i, section in enumerate(outline.get("sections", []))
    ])

    return {
        "title": outline.get('title', 'æœªçŸ¥æ ‡é¢˜'),
        "executive_summary": outline.get('executive_summary', 'æ— æ‘˜è¦'),
        "sections_text": sections_text,
        "methodology": outline.get('methodology', 'æœªæŒ‡å®š'),
        "estimated_length": outline.get('estimated_length', 0),
        "target_audience": outline.get('target_audience', 'æœªçŸ¥')
    }

def process_outline_confirmation(state: DeepResearchState, response_data: Dict[str, Any]) -> DeepResearchState:
    """å¤„ç†å¤§çº²ç¡®è®¤å“åº”"""
    # æ›´æ–°ç¡®è®¤çŠ¶æ€
    state["approval_status"]["outline_confirmation"] = response_data.get("approved", False)
    state["user_feedback"]["outline_confirmation"] = response_data

    # è®°å½•äº¤äº’å†å²
    add_user_interaction(state, "outline_confirmation", response_data)

    return state

# åˆ›å»ºå¤§çº²ç¡®è®¤èŠ‚ç‚¹
_outline_confirmation_base_node = create_confirmation_node(
    node_name="outline_confirmation",
    title="å¤§çº²ç¡®è®¤",
    message_template="""è¯·ç¡®è®¤ä»¥ä¸‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š

    æ ‡é¢˜ï¼š{title}

    æ‘˜è¦ï¼š{executive_summary}

    ç« èŠ‚ç»“æ„ï¼š
    {sections_text}

    ç ”ç©¶æ–¹æ³•ï¼š{methodology}
    é¢„ä¼°å­—æ•°ï¼š{estimated_length:,}å­—
    ç›®æ ‡è¯»è€…ï¼š{target_audience}""",
    get_data_func=get_outline_data
)

async def outline_confirmation(state: DeepResearchState, config=None) -> DeepResearchState:
    """å¸¦æœ‰è‡ªå®šä¹‰å¤„ç†çš„å¤§çº²ç¡®è®¤èŠ‚ç‚¹"""
    # è°ƒç”¨åŸºç¡€çš„ç¡®è®¤èŠ‚ç‚¹
    result_state = await _outline_confirmation_base_node(state, config)

    # åº”ç”¨è‡ªå®šä¹‰å¤„ç†
    if "confirmations" in result_state and "outline_confirmation" in result_state["confirmations"]:
        confirmation_data = result_state["confirmations"]["outline_confirmation"]
        result_state = process_outline_confirmation(result_state, confirmation_data)

    return result_state

# ============================================================================
# è·¯ç”±å‡½æ•°
# ============================================================================

def route_after_outline_confirmation(state: DeepResearchState) -> str:
    """å¤§çº²ç¡®è®¤åçš„è·¯ç”± - é€‚é…é€šç”¨ä¸­æ–­èŠ‚ç‚¹"""
    # æ£€æŸ¥é€šç”¨ä¸­æ–­èŠ‚ç‚¹çš„ç¡®è®¤çŠ¶æ€
    confirmations = state.get("confirmations", {})
    outline_confirmation = confirmations.get("outline_confirmation", {})

    # å¦‚æœç”¨æˆ·æ‹’ç»æˆ–æ²¡æœ‰ç¡®è®¤ï¼Œé‡æ–°ç”Ÿæˆå¤§çº²
    if not outline_confirmation.get("approved", True):
        return "outline_generation"

    # ç¡®è®¤é€šè¿‡ï¼Œè¿›å…¥å†…å®¹åˆ›å»º
    return "report_generation"

# ============================================================================
# å›¾æ„å»ºå‡½æ•°
# ============================================================================

def create_deep_research_graph():
    """åˆ›å»ºæ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå›¾ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾"""
    workflow = StateGraph(DeepResearchState)

    # æ·»åŠ ç®€åŒ–çš„æ ¸å¿ƒèŠ‚ç‚¹ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
    workflow.add_node("outline_generation", outline_generation)
    workflow.add_node("outline_confirmation", outline_confirmation)
    workflow.add_node("report_generation", report_generation)
    # è®¾ç½®ç®€åŒ–çš„æµç¨‹ï¼šå¤§çº²ç”Ÿæˆ â†’ å¤§çº²ç¡®è®¤ â†’ å†…å®¹åˆ›å»º
    workflow.add_edge(START, "outline_generation")
    workflow.add_edge("outline_generation", "outline_confirmation")

    # å¤§çº²ç¡®è®¤åçš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "outline_confirmation",
        route_after_outline_confirmation,
        {
            "outline_generation": "outline_generation",
            "report_generation": "report_generation"
        }
    )
    
    workflow.add_edge("report_generation", END)
    
    return workflow
