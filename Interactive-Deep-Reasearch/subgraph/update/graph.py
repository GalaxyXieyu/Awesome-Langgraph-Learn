"""
åŸºäºLangGraphçš„æ™ºèƒ½ç ”ç©¶ç³»ç»Ÿ
æ­£ç¡®ä½¿ç”¨LangGraphæ¶æ„ï¼šStateGraph + èŠ‚ç‚¹ + æ¡ä»¶è·¯ç”± + æµå¼è¾“å‡º
"""

import json
import time
import asyncio
from typing import Dict, Any, List, TypedDict, Annotated, Optional
from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langgraph.prebuilt import create_react_agent
from langchain_openai import ChatOpenAI
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer
import logging

# å¯¼å…¥æ¨¡å—åŒ–ç»„ä»¶
from tools import ALL_RESEARCH_TOOLS
from context_builder import build_supervisor_context, determine_next_action_by_state
from prompts import get_supervisor_prompt, get_researcher_prompt, get_writer_prompt

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# çŠ¶æ€å®šä¹‰ - LangGraphæ ¸å¿ƒ
# ============================================================================

class IntelligentResearchState(TypedDict):
    """æ™ºèƒ½ç ”ç©¶ç³»ç»ŸçŠ¶æ€"""
    messages: Annotated[List, add_messages]  # æ¶ˆæ¯å†å²
    user_input: str  # ç”¨æˆ·è¾“å…¥
    topic: str  # ç ”ç©¶ä¸»é¢˜
    sections: List[Dict[str, Any]]  # ç« èŠ‚åˆ—è¡¨
    current_section_index: int  # å½“å‰å¤„ç†çš„ç« èŠ‚ç´¢å¼•
    research_results: Dict[str, Any]  # ç ”ç©¶ç»“æœ
    writing_results: Dict[str, Any]  # å†™ä½œç»“æœ
    polishing_results: Dict[str, Any]  # æ¶¦è‰²ç»“æœ
    final_report: Dict[str, Any]  # æœ€ç»ˆæŠ¥å‘Š
    execution_path: List[str]  # æ‰§è¡Œè·¯å¾„
    iteration_count: int  # è¿­ä»£æ¬¡æ•°
    max_iterations: int  # æœ€å¤§è¿­ä»£æ¬¡æ•°
    next_action: str  # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
    task_completed: bool  # ä»»åŠ¡å®Œæˆæ ‡å¿—
    error_log: List[str]  # é”™è¯¯æ—¥å¿—
    section_attempts: Dict[str, Dict[str, int]]  # æ¯ä¸ªç« èŠ‚çš„å°è¯•æ¬¡æ•°è®°å½• {"section_id": {"research": 2, "writing": 1}}



# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm():
    """åˆ›å»ºLLMå®ä¾‹"""
    return ChatOpenAI(
        model="qwen2.5-72b-instruct-awq",
        temperature=0.7,
        base_url="https://llm.3qiao.vip:23436/v1",
        api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197",
    )

# ============================================================================
# Agentåˆ›å»º - ä¸“ä¸šåŒ–Agent
# ============================================================================

def create_research_agents():
    """åˆ›å»ºä¸“ä¸šåŒ–çš„ç ”ç©¶Agent"""
    llm = create_llm()
    
    # ç ”ç©¶å‘˜Agent
    researcher_agent = create_react_agent(
        llm,
        tools=ALL_RESEARCH_TOOLS,
        prompt=get_researcher_prompt()
    )
    
    # å†™ä½œå‘˜Agent - ä¹Ÿå¯ä»¥ä½¿ç”¨å·¥å…·è·å–æ›´å¤šæ•°æ®
    writer_agent = create_react_agent(
        llm,
        tools=ALL_RESEARCH_TOOLS,  # å†™ä½œå‘˜ä¹Ÿå¯ä»¥è°ƒç”¨å·¥å…·è¡¥å……æ•°æ®
        prompt=get_writer_prompt()
    )
    
    return {
        "researcher": researcher_agent,
        "writer": writer_agent
    }

# ============================================================================
# èŠ‚ç‚¹å‡½æ•° - LangGraphæ ¸å¿ƒç»„ä»¶
# ============================================================================

async def supervisor_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    """æ™ºèƒ½SupervisorèŠ‚ç‚¹ - ä½¿ç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–å’Œè´¨é‡è¯„ä¼°"""

    writer = get_stream_writer()

    writer({"step": "supervisor", "status": "å¼€å§‹æ™ºèƒ½è°ƒåº¦åˆ†æ", "progress": 0})

    llm = create_llm()

    # ä½¿ç”¨æ¨¡å—åŒ–çš„ä¸Šä¸‹æ–‡æ„å»º
    input_data = build_supervisor_context(state)

    # æ„å»ºSupervisorçš„æ™ºèƒ½å†³ç­–æç¤º
    supervisor_prompt = get_supervisor_prompt()

    writer({"step": "supervisor", "status": "æ­£åœ¨è¿›è¡Œæ™ºèƒ½åˆ†æ...", "progress": 30})

    formatted_messages = supervisor_prompt.format_messages(**input_data)
    # æµå¼è°ƒç”¨LLMè¿›è¡Œæ™ºèƒ½å†³ç­–
    full_response = ""
    chunk_count = 0
    async for chunk in llm.astream(formatted_messages, config=config):
        if hasattr(chunk, 'content') and chunk.content:
            content = str(chunk.content)
            full_response += content
            chunk_count += 1
    writer({"step": "supervisor", "status": "è§£æå†³ç­–ç»“æœ...", "content": full_response, "progress": 85})
    import re
    try:
        json_matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', full_response, re.DOTALL)
        decision_json = None
        for json_str in json_matches:
            # æ¸…ç†JSONå­—ç¬¦ä¸²
            cleaned_json = json_str.strip()
            # ç§»é™¤å¯èƒ½çš„markdownä»£ç å—æ ‡è®°
            cleaned_json = re.sub(r'^```json\s*', '', cleaned_json)
            cleaned_json = re.sub(r'\s*```$', '', cleaned_json)

            decision_json = json.loads(cleaned_json)
            break

        if decision_json:
            # æ¸…ç†JSONé”®åï¼Œå¤„ç†å¯èƒ½åŒ…å«æ¢è¡Œç¬¦å’Œç©ºæ ¼çš„é”®
            cleaned_json = {}
            for key, value in decision_json.items():
                # æ¸…ç†é”®åï¼šç§»é™¤æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦å’Œå¤šä½™ç©ºæ ¼
                clean_key = key.strip().replace('\n', '').replace('\t', '').replace(' ', '')
                cleaned_json[clean_key] = value

            # ä½¿ç”¨æ¸…ç†åçš„é”®æ¥è·å–å€¼
            next_action = cleaned_json.get("action", "integration")
            reasoning = cleaned_json.get("reasoning", "æ™ºèƒ½åˆ†æå†³ç­–")
            quality_feedback = cleaned_json.get("quality_feedback", "")
            confidence = cleaned_json.get("confidence", 0.7)
            target_section = cleaned_json.get("target_section", "")
        else:
            raise ValueError("æ— æ³•è§£æJSON")
    except Exception as parse_error:
        # JSONè§£æå¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨é€»è¾‘
        logger.error(f"JSONè§£æå¤±è´¥: {str(parse_error)}")
        next_action, reasoning = determine_next_action_by_state(state)
        quality_feedback = "åŸºäºçŠ¶æ€é€»è¾‘çš„å†³ç­–"
        confidence = 0.6
        target_section = ""

    # è·å–å½“å‰è¿›åº¦ä¿¡æ¯
    sections = state.get("sections", [])
    current_index = state.get("current_section_index", 0)
    research_results = state.get("research_results", {})
    writing_results = state.get("writing_results", {})

    # å…¨å±€å®Œæˆæ£€æŸ¥ - æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç« èŠ‚éƒ½æœ‰ç ”ç©¶å’Œå†™ä½œç»“æœ
    all_sections_completed = True
    for section in sections:
        section_id = section.get("id", "")
        has_research = section_id in research_results and research_results[section_id].get("content", "").strip() != ""
        has_writing = section_id in writing_results and writing_results[section_id].get("content", "").strip() != ""
        if not (has_research and has_writing):
            all_sections_completed = False
            break

    if all_sections_completed and next_action != "integration":
        logger.info("ğŸ‰ æ£€æµ‹åˆ°æ‰€æœ‰ç« èŠ‚éƒ½å·²å®Œæˆï¼Œå¼ºåˆ¶è¿›å…¥integration")
        next_action = "integration"
        reasoning = "æ‰€æœ‰ç« èŠ‚çš„ç ”ç©¶å’Œå†™ä½œéƒ½å·²å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆæ•´åˆ"

    # å¤„ç†ç« èŠ‚ç´¢å¼•æ›´æ–°å’Œç›®æ ‡ç« èŠ‚è®¾ç½®
    if next_action == "move_to_next_section":
        new_index = current_index + 1
        state["current_section_index"] = new_index
        # æ£€æŸ¥æ˜¯å¦è¶…å‡ºç« èŠ‚èŒƒå›´
        if new_index >= len(sections):
            logger.info("ğŸ‰ ç« èŠ‚ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œæ‰€æœ‰ç« èŠ‚å·²å®Œæˆï¼Œè¿›å…¥integration")
            next_action = "integration"
            reasoning = "æ‰€æœ‰ç« èŠ‚å¤„ç†å®Œæˆï¼Œå¼€å§‹æœ€ç»ˆæ•´åˆ"
        else:
            next_action = "research"  # ç§»åŠ¨åˆ°ä¸‹ä¸€ç« èŠ‚åå¼€å§‹ç ”ç©¶
            reasoning = f"å·²ä»ç¬¬{current_index + 1}ç« èŠ‚ç§»åŠ¨åˆ°ç¬¬{new_index + 1}ç« èŠ‚ï¼Œå¼€å§‹æ–°ç« èŠ‚ç ”ç©¶"
        current_index = new_index  # æ›´æ–°æœ¬åœ°å˜é‡ç”¨äºæ˜¾ç¤º
    elif target_section and target_section != "":
        # å¦‚æœSupervisoræŒ‡å®šäº†ç›®æ ‡ç« èŠ‚ï¼Œæ‰¾åˆ°å¯¹åº”çš„ç´¢å¼•
        target_index = None
        for i, section in enumerate(sections):
            if section.get("id", "") == target_section:
                target_index = i
                break

        if target_index is not None and target_index != current_index:
            logger.info(f"ğŸ¯ SupervisoræŒ‡å®šç›®æ ‡ç« èŠ‚: {target_section}, æ›´æ–°ç´¢å¼• {current_index} â†’ {target_index}")
            state["current_section_index"] = target_index
            current_index = target_index
            reasoning = f"æ ¹æ®SupervisoræŒ‡ç¤ºï¼Œåˆ‡æ¢åˆ°ç›®æ ‡ç« èŠ‚: {sections[target_index].get('title', '')}"

    # æ›´æ–°çŠ¶æ€
    state["next_action"] = next_action
    state["supervisor_reasoning"] = reasoning
    state["quality_feedback"] = quality_feedback
    state["supervisor_confidence"] = confidence
    state["iteration_count"] = state.get("iteration_count", 0) + 1
    state["execution_path"] = state.get("execution_path", []) + ["intelligent_supervisor"]

    # æ·»åŠ Supervisorçš„æ™ºèƒ½åˆ†ææ¶ˆæ¯
    supervisor_message = f"""
    ğŸ§  æ™ºèƒ½è°ƒåº¦åˆ†æå®Œæˆï¼š
    - å†³ç­–ï¼š{next_action}
    - ç†ç”±ï¼š{reasoning}
    - è´¨é‡åé¦ˆï¼š{quality_feedback}
    - ç½®ä¿¡åº¦ï¼š{confidence:.1%}
    - å½“å‰è¿›åº¦ï¼š{current_index}/{len(sections)}ç« èŠ‚
    """

    state["messages"] = state.get("messages", []) + [
        AIMessage(content=supervisor_message)
    ]
    return state

async def research_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    """ç ”ç©¶èŠ‚ç‚¹ - æ‰§è¡Œç« èŠ‚ç ”ç©¶"""
    writer = get_stream_writer()
    try:
        sections = state.get("sections", [])
        current_index = state.get("current_section_index", 0)

        logger.info(f"ğŸ” ResearchèŠ‚ç‚¹ - å½“å‰ç« èŠ‚ç´¢å¼•: {current_index}, æ€»ç« èŠ‚æ•°: {len(sections)}")

        if current_index >= len(sections):
            logger.info("ğŸ“ æ‰€æœ‰ç« èŠ‚å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡researchèŠ‚ç‚¹")
            return state

        current_section = sections[current_index]
        section_id = current_section.get("id", "")
        title = current_section.get("title", "")
        description = current_section.get("description", "")

        logger.info(f"ğŸ“– å¼€å§‹å¤„ç†ç« èŠ‚: {title} (ID: {section_id})")
        
        # è®°å½•ç ”ç©¶å°è¯•æ¬¡æ•°
        section_attempts = state.get("section_attempts", {})
        if section_id not in section_attempts:
            section_attempts[section_id] = {"research": 0, "writing": 0}
        section_attempts[section_id]["research"] += 1
        state["section_attempts"] = section_attempts

        current_attempt = section_attempts[section_id]["research"]
        writer({"step": "research", "status": f"å¼€å§‹ç ”ç©¶: {title} (ç¬¬{current_attempt}æ¬¡å°è¯•)", "progress": 0})

        # åˆ›å»ºç ”ç©¶Agent
        agents = create_research_agents()
        researcher = agents["researcher"]
        
        # æ„å»ºç ”ç©¶ä»»åŠ¡
        research_task = f"""
        è¯·æ·±åº¦ç ”ç©¶ä»¥ä¸‹ç« èŠ‚ï¼š
        
        ç« èŠ‚æ ‡é¢˜ï¼š{title}
        ç« èŠ‚æè¿°ï¼š{description}
        
        ç ”ç©¶è¦æ±‚ï¼š
        1. ä½¿ç”¨åˆé€‚çš„å·¥å…·è·å–ç›¸å…³æ•°æ®
        2. å¦‚æœæ•°æ®ä¸è¶³ï¼Œä¸»åŠ¨ä½¿ç”¨å¤šä¸ªå·¥å…·è¡¥å……
        3. æä¾›è¯¦ç»†çš„åˆ†ææŠ¥å‘Š
        """
        
        writer({"step": "research", "status": f"Agentå¼€å§‹ç ”ç©¶: {title}", "progress": 30})
        
        # Agentæ‰§è¡Œç ”ç©¶
        agent_input = {"messages": [HumanMessage(content=research_task)]}
        
        # æµå¼æ‰§è¡Œç ”ç©¶ - ä½¿ç”¨updatesæ¨¡å¼è·å–ç»“æ„åŒ–ç»“æœ
        full_response = ""
        final_agent_message = None

        try:
            async for chunk in researcher.astream(agent_input, stream_mode="updates"):
                # å¤„ç†agentçš„æ¶ˆæ¯
                if 'agent' in chunk and 'messages' in chunk['agent']:
                    messages = chunk['agent']['messages']
                    for message in messages:
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content)
                            if content.strip():
                                # æ˜¾ç¤ºæµå¼å†…å®¹ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                                writer({
                                    "step": "research",
                                    "status": f"Agentç ”ç©¶ä¸­: {title}",
                                    "progress": 60,
                                    "streaming_content": content[:200] + "..." if len(content) > 200 else content
                                })
                                # ä¿å­˜æœ€åä¸€æ¡Agentæ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç»“æœ
                                final_agent_message = message

                # å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆå¯é€‰æ˜¾ç¤ºï¼‰
                elif 'tools' in chunk:
                    writer({
                        "step": "research",
                        "status": f"å·¥å…·è°ƒç”¨ä¸­: {title}",
                        "progress": 50
                    })

            # ä½¿ç”¨æœ€ç»ˆçš„Agentæ¶ˆæ¯ä½œä¸ºç ”ç©¶ç»“æœ
            if final_agent_message and hasattr(final_agent_message, 'content'):
                full_response = str(final_agent_message.content)
            else:
                full_response = "ç ”ç©¶å®Œæˆï¼Œä½†æœªè·å–åˆ°æœ€ç»ˆç»“æœ"
        
        except Exception as e:
            logger.error(f"Agentæµå¼æ‰§è¡Œå¤±è´¥: {e}")
            # å¦‚æœæµå¼æ‰§è¡Œå¤±è´¥ï¼Œå°è¯•æ™®é€šæ‰§è¡Œ
            try:
                result = await researcher.ainvoke(agent_input)
                if hasattr(result, 'content'):
                    full_response = str(result.content)
                elif isinstance(result, dict) and 'messages' in result:
                    messages = result['messages']
                    for msg in messages:
                        if hasattr(msg, 'content') and msg.content:
                            full_response += str(msg.content)
            except Exception as e2:
                logger.error(f"Agentæ™®é€šæ‰§è¡Œä¹Ÿå¤±è´¥: {e2}")
                full_response = f"ç ”ç©¶æ‰§è¡Œå¤±è´¥: {str(e2)}"
        
        writer({"step": "research", "status": f"ç ”ç©¶å®Œæˆ: {title}", "content": full_response, "progress": 100})
        # ä¿å­˜ç ”ç©¶ç»“æœ
        research_results = state.get("research_results", {})
        research_results[section_id] = {
            "title": title,
            "content": full_response,
            "timestamp": time.time()
        }
        state["research_results"] = research_results
        state["execution_path"] = state.get("execution_path", []) + ["research"]
        
        writer({
            "step": "research",
            "status": f"ç ”ç©¶å®Œæˆ: {title}",
            "progress": 100,
            "result_length": len(full_response)
        })
        
        logger.info(f"ç ”ç©¶å®Œæˆ: {title}")
        return state
        
    except Exception as e:
        logger.error(f"ç ”ç©¶å¤±è´¥: {e}")
        state["error_log"] = state.get("error_log", []) + [f"ç ”ç©¶é”™è¯¯: {e}"]
        return state

async def writing_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    """å†™ä½œèŠ‚ç‚¹ - åŸºäºç ”ç©¶ç»“æœå†™ä½œ"""
    try:
        writer = get_stream_writer()
    except Exception:
        writer = lambda _: None
    
    try:
        sections = state.get("sections", [])
        current_index = state.get("current_section_index", 0)
        research_results = state.get("research_results", {})

        logger.info(f"âœï¸ WritingèŠ‚ç‚¹ - å½“å‰ç« èŠ‚ç´¢å¼•: {current_index}, æ€»ç« èŠ‚æ•°: {len(sections)}")

        if current_index >= len(sections):
            logger.info("ğŸ“ æ‰€æœ‰ç« èŠ‚å·²å¤„ç†å®Œæˆï¼Œè·³è¿‡writingèŠ‚ç‚¹")
            return state

        current_section = sections[current_index]
        section_id = current_section.get("id", "")
        title = current_section.get("title", "")

        logger.info(f"ğŸ“ å¼€å§‹å†™ä½œç« èŠ‚: {title} (ID: {section_id})")
        
        # è®°å½•å†™ä½œå°è¯•æ¬¡æ•°
        section_attempts = state.get("section_attempts", {})
        if section_id not in section_attempts:
            section_attempts[section_id] = {"research": 0, "writing": 0}
        section_attempts[section_id]["writing"] += 1
        state["section_attempts"] = section_attempts

        current_attempt = section_attempts[section_id]["writing"]

        # è·å–ç ”ç©¶æ•°æ®
        research_data = research_results.get(section_id, {})
        research_content = research_data.get("content", "")

        writer({"step": "writing", "status": f"å¼€å§‹å†™ä½œ: {title} (ç¬¬{current_attempt}æ¬¡å°è¯•)", "progress": 0})

        # åˆ›å»ºå†™ä½œAgent
        agents = create_research_agents()
        writer_agent = agents["writer"]
        
        # è·å–å…¶ä»–ç« èŠ‚çš„ç ”ç©¶ç»“æœä½œä¸ºå‚è€ƒ
        all_research_data = ""
        for sec_id, research_data in research_results.items():
            if sec_id != section_id:  # æ’é™¤å½“å‰ç« èŠ‚
                all_research_data += f"\nå‚è€ƒç« èŠ‚ {research_data.get('title', '')}: {research_data.get('content', '')[:200]}...\n"

        # æ„å»ºå†™ä½œä»»åŠ¡
        writing_task = f"""
        è¯·åŸºäºä»¥ä¸‹ä¿¡æ¯ï¼Œæ’°å†™é«˜è´¨é‡çš„ç« èŠ‚å†…å®¹ï¼š

        å½“å‰ç« èŠ‚æ ‡é¢˜ï¼š{title}
        å½“å‰ç« èŠ‚ç ”ç©¶æ•°æ®ï¼š
        {research_content}

        å…¶ä»–ç« èŠ‚ç ”ç©¶æ•°æ®ï¼ˆä¾›å‚è€ƒï¼‰ï¼š
        {all_research_data}

        è¦æ±‚ï¼š
        1. ä¸“æ³¨äºå½“å‰ç« èŠ‚ä¸»é¢˜
        2. å¦‚æœç ”ç©¶æ•°æ®ä¸è¶³ï¼Œå¯ä»¥ä½¿ç”¨å·¥å…·è·å–æ›´å¤šä¿¡æ¯
        3. ç¡®ä¿å†…å®¹å……å®ï¼Œæ•°æ®æ”¯æ’‘å……åˆ†
        4. ä¿æŒä¸å…¶ä»–ç« èŠ‚çš„é€»è¾‘è¿è´¯æ€§
        """
        
        writer({"step": "writing", "status": f"Agentå¼€å§‹å†™ä½œ: {title}", "progress": 30})
        
        # Agentæ‰§è¡Œå†™ä½œ
        agent_input = {"messages": [HumanMessage(content=writing_task)]}
        
        # æµå¼æ‰§è¡Œå†™ä½œ - ä½¿ç”¨updatesæ¨¡å¼è·å–ç»“æ„åŒ–ç»“æœ
        full_response = ""
        final_agent_message = None

        try:
            async for chunk in writer_agent.astream(agent_input, stream_mode="updates"):
                # å¤„ç†agentçš„æ¶ˆæ¯
                if 'agent' in chunk and 'messages' in chunk['agent']:
                    messages = chunk['agent']['messages']
                    for message in messages:
                        if hasattr(message, 'content') and message.content:
                            content = str(message.content)
                            if content.strip():
                                # æ˜¾ç¤ºæµå¼å†…å®¹ï¼ˆæˆªæ–­æ˜¾ç¤ºï¼‰
                                writer({
                                    "step": "writing",
                                    "status": f"Agentå†™ä½œä¸­: {title}",
                                    "progress": 60,
                                    "streaming_content": content[:200] + "..." if len(content) > 200 else content
                                })
                                # ä¿å­˜æœ€åä¸€æ¡Agentæ¶ˆæ¯ä½œä¸ºæœ€ç»ˆç»“æœ
                                final_agent_message = message

                # å¤„ç†å·¥å…·è°ƒç”¨ç»“æœï¼ˆå¯é€‰æ˜¾ç¤ºï¼‰
                elif 'tools' in chunk:
                    writer({
                        "step": "writing",
                        "status": f"å·¥å…·è°ƒç”¨ä¸­: {title}",
                        "progress": 50
                    })

            # ä½¿ç”¨æœ€ç»ˆçš„Agentæ¶ˆæ¯ä½œä¸ºå†™ä½œç»“æœ
            if final_agent_message and hasattr(final_agent_message, 'content'):
                full_response = str(final_agent_message.content)
            else:
                full_response = "å†™ä½œå®Œæˆï¼Œä½†æœªè·å–åˆ°æœ€ç»ˆç»“æœ"
        
        except Exception as e:
            logger.error(f"å†™ä½œAgentæµå¼æ‰§è¡Œå¤±è´¥: {e}")
            # å¦‚æœæµå¼æ‰§è¡Œå¤±è´¥ï¼Œå°è¯•æ™®é€šæ‰§è¡Œ
            try:
                result = await writer_agent.ainvoke(agent_input)
                if hasattr(result, 'content'):
                    full_response = str(result.content)
                elif isinstance(result, dict) and 'messages' in result:
                    messages = result['messages']
                    for msg in messages:
                        if hasattr(msg, 'content') and msg.content:
                            full_response += str(msg.content)
            except Exception as e2:
                logger.error(f"å†™ä½œAgentæ™®é€šæ‰§è¡Œä¹Ÿå¤±è´¥: {e2}")
                full_response = f"å†™ä½œæ‰§è¡Œå¤±è´¥: {str(e2)}"
        
        # ä¿å­˜å†™ä½œç»“æœ
        writing_results = state.get("writing_results", {})
        writing_results[section_id] = {
            "title": title,
            "content": full_response,
            "word_count": len(full_response.split()),
            "timestamp": time.time()
        }
        state["writing_results"] = writing_results
        state["execution_path"] = state.get("execution_path", []) + ["writing"]
        
        writer({
            "step": "writing",
            "status": f"å†™ä½œå®Œæˆ: {title}",
            "progress": 100,
            "word_count": len(full_response.split())
        })
        
        logger.info(f"å†™ä½œå®Œæˆ: {title}")
        return state
        
    except Exception as e:
        logger.error(f"å†™ä½œå¤±è´¥: {e}")
        state["error_log"] = state.get("error_log", []) + [f"å†™ä½œé”™è¯¯: {e}"]
        return state

async def integration_node(state: IntelligentResearchState, config=None) -> IntelligentResearchState:
    """æ•´åˆèŠ‚ç‚¹ - ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    try:
        writer = get_stream_writer()
    except Exception:
        writer = lambda _: None

    try:
        writer({"step": "integration", "status": "å¼€å§‹æ•´åˆæœ€ç»ˆæŠ¥å‘Š", "progress": 0})

        topic = state.get("topic", "")
        writing_results = state.get("writing_results", {})
        sections = state.get("sections", [])

        # æ„å»ºæœ€ç»ˆæŠ¥å‘Š
        final_sections = []
        total_words = 0

        for section in sections:
            section_id = section.get("id", "")
            if section_id in writing_results:
                section_data = writing_results[section_id]
                final_sections.append(section_data)
                total_words += section_data.get("word_count", 0)

        final_report = {
            "title": f"{topic} - æ™ºèƒ½ç ”ç©¶æŠ¥å‘Š",
            "topic": topic,
            "sections": final_sections,
            "total_sections": len(final_sections),
            "total_words": total_words,
            "generation_method": "langgraph_intelligent_research",
            "execution_path": state.get("execution_path", []),
            "generation_timestamp": time.time()
        }

        state["final_report"] = final_report
        state["task_completed"] = True
        state["execution_path"] = state.get("execution_path", []) + ["integration"]

        writer({
            "step": "integration",
            "status": "æŠ¥å‘Šæ•´åˆå®Œæˆ",
            "progress": 100,
            "total_sections": len(final_sections),
            "total_words": total_words
        })

        logger.info(f"æŠ¥å‘Šæ•´åˆå®Œæˆ: {len(final_sections)}ä¸ªç« èŠ‚, {total_words}å­—")
        return state

    except Exception as e:
        logger.error(f"æ•´åˆå¤±è´¥: {e}")
        state["error_log"] = state.get("error_log", []) + [f"æ•´åˆé”™è¯¯: {e}"]
        state["task_completed"] = True  # å³ä½¿å¤±è´¥ä¹Ÿæ ‡è®°å®Œæˆ
        return state

# ============================================================================
# è·¯ç”±å‡½æ•° - LangGraphæ¡ä»¶è·¯ç”±
# ============================================================================

def route_after_intelligent_supervisor(state: IntelligentResearchState) -> str:
    """æ™ºèƒ½Supervisoråçš„è·¯ç”±å†³ç­–"""
    next_action = state.get("next_action", "integration")

    # æ£€æŸ¥æ˜¯å¦è¶…è¿‡æœ€å¤§è¿­ä»£æ¬¡æ•°
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 10)

    if iteration_count >= max_iterations:
        logger.warning(f"è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•° {max_iterations}ï¼Œå¼ºåˆ¶è¿›å…¥æ•´åˆé˜¶æ®µ")
        return "integration"

    if next_action == "research":
        return "research"
    elif next_action == "writing":
        return "writing"
    elif next_action == "quality_check":
        return "intelligent_supervisor"  # å›åˆ°supervisorè¿›è¡Œæ›´è¯¦ç»†åˆ†æ
    else:
        return "integration"

def route_after_research(state: IntelligentResearchState) -> str:
    """ç ”ç©¶åçš„è·¯ç”±å†³ç­– - å›åˆ°æ™ºèƒ½supervisor"""
    return "intelligent_supervisor"

def route_after_writing(state: IntelligentResearchState) -> str:
    """å†™ä½œåçš„è·¯ç”±å†³ç­– - å›åˆ°æ™ºèƒ½supervisor"""
    return "intelligent_supervisor"

def should_end(state: IntelligentResearchState) -> str:
    """åˆ¤æ–­æ˜¯å¦ç»“æŸ"""
    if state.get("task_completed", False):
        return END
    else:
        return "intelligent_supervisor"

# ============================================================================
# å›¾æ„å»º - LangGraphæ ¸å¿ƒ
# ============================================================================

def create_intelligent_research_graph(checkpointer: Optional[InMemorySaver] = None):
    """åˆ›å»ºæ™ºèƒ½ç ”ç©¶å·¥ä½œæµå›¾ - ä½¿ç”¨æ™ºèƒ½Supervisor"""
    workflow = StateGraph(IntelligentResearchState)

    # æ·»åŠ èŠ‚ç‚¹ - ä½¿ç”¨æ™ºèƒ½supervisor
    workflow.add_node("intelligent_supervisor", supervisor_node)
    workflow.add_node("research", research_node)
    workflow.add_node("writing", writing_node)
    workflow.add_node("integration", integration_node)

    # è®¾ç½®èµ·å§‹èŠ‚ç‚¹ - ä»æ™ºèƒ½supervisorå¼€å§‹
    workflow.add_edge(START, "intelligent_supervisor")

    # æ·»åŠ æ¡ä»¶è·¯ç”± - æ™ºèƒ½supervisorçš„å†³ç­–è·¯ç”±
    workflow.add_conditional_edges(
        "intelligent_supervisor",
        route_after_intelligent_supervisor,
        {
            "research": "research",
            "writing": "writing",
            "integration": "integration",
            "intelligent_supervisor": "intelligent_supervisor"  # æ”¯æŒè´¨é‡æ£€æŸ¥å¾ªç¯
        }
    )

    # ç ”ç©¶å’Œå†™ä½œå®Œæˆåéƒ½å›åˆ°æ™ºèƒ½supervisorè¿›è¡Œè´¨é‡è¯„ä¼°
    workflow.add_conditional_edges(
        "research",
        route_after_research,
        {
            "intelligent_supervisor": "intelligent_supervisor"
        }
    )

    workflow.add_conditional_edges(
        "writing",
        route_after_writing,
        {
            "intelligent_supervisor": "intelligent_supervisor"
        }
    )

    # æ•´åˆå®Œæˆåçš„ç»“æŸåˆ¤æ–­
    workflow.add_conditional_edges(
        "integration",
        should_end,
        {
            "intelligent_supervisor": "intelligent_supervisor",
            END: END
        }
    )
    return workflow
