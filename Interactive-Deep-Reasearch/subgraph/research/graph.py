"""
æ™ºèƒ½ä¸Šä¸‹æ–‡æ„ŸçŸ¥ç« èŠ‚ç ”ç©¶ç³»ç»Ÿ
åŸºäºè´¨é‡é©±åŠ¨çš„è¿­ä»£ä¼˜åŒ–æ¶æ„ï¼šä¸Šä¸‹æ–‡æ„ŸçŸ¥ + è´¨é‡è¯„ä¼° + è‡ªé€‚åº”ä¼˜åŒ–
"""

import json
import time
import asyncio
import uuid
from typing import Dict, Any, List, Optional, TypedDict, Annotated
from datetime import datetime

from langgraph.graph import StateGraph, END, START
from langgraph.graph.message import add_messages
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langgraph.checkpoint.memory import InMemorySaver
from langgraph.config import get_stream_writer
from langchain_core.tools import tool
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def safe_get_stream_writer():
    """å®‰å…¨è·å–æµå†™å…¥å™¨ï¼Œé¿å…ä¸Šä¸‹æ–‡é”™è¯¯"""
    try:
        return get_stream_writer()
    except Exception:
        # å¦‚æœæ²¡æœ‰æµä¸Šä¸‹æ–‡ï¼Œè¿”å›ä¸€ä¸ªç©ºçš„å†™å…¥å™¨
        return lambda x: None

# ============================================================================
# æ™ºèƒ½çŠ¶æ€å®šä¹‰
# ============================================================================

class IntelligentSectionState(TypedDict):
    """æ™ºèƒ½ç« èŠ‚ç ”ç©¶çŠ¶æ€"""
    
    # ========== è¾“å…¥å‚æ•° ==========
    topic: str                                    # ç ”ç©¶ä¸»é¢˜
    section: Dict[str, Any]                      # å½“å‰ç« èŠ‚ä¿¡æ¯
    
    # ========== ä¸Šä¸‹æ–‡ä¿¡æ¯ ==========
    previous_sections_summary: List[str]         # å‰ç½®ç« èŠ‚æ‘˜è¦
    upcoming_sections_outline: List[str]         # åç»­ç« èŠ‚å¤§çº²
    report_main_thread: str                      # æŠ¥å‘Šä¸»çº¿é€»è¾‘
    writing_style: str                           # ç»Ÿä¸€å†™ä½œé£æ ¼
    logical_connections: Dict[str, Any]          # é€»è¾‘å…³è”ç‚¹
    
    # ========== ç ”ç©¶é˜¶æ®µ ==========
    initial_research_results: List[Dict[str, Any]]  # åˆæ­¥ç ”ç©¶ç»“æœ
    supplementary_research_results: List[Dict[str, Any]]  # è¡¥å……ç ”ç©¶ç»“æœ
    
    # ========== è´¨é‡è¯„ä¼° ==========
    quality_assessment: Optional[Dict[str, Any]]  # è´¨é‡è¯„ä¼°ç»“æœ
    content_gaps: List[str]                       # å†…å®¹ç¼ºå£
    improvement_suggestions: List[str]            # æ”¹è¿›å»ºè®®
    
    # ========== å†…å®¹ç”Ÿæˆ ==========
    draft_content: Optional[str]                  # åˆç¨¿å†…å®¹
    enhanced_content: Optional[str]               # å¢å¼ºå†…å®¹
    polished_content: Optional[str]               # æ¶¦è‰²å†…å®¹
    final_content: Optional[str]                  # æœ€ç»ˆå†…å®¹
    
    # ========== è¿­ä»£æ§åˆ¶ ==========
    iteration_count: int                          # è¿­ä»£æ¬¡æ•°
    max_iterations: int                           # æœ€å¤§è¿­ä»£æ¬¡æ•°
    quality_threshold: float                      # è´¨é‡é˜ˆå€¼
    
    # ========== æ‰§è¡ŒçŠ¶æ€ ==========
    current_step: str                             # å½“å‰æ‰§è¡Œæ­¥éª¤
    execution_path: List[str]                     # æ‰§è¡Œè·¯å¾„è®°å½•
    start_time: float                             # å¼€å§‹æ—¶é—´
    
    # ========== æœ€ç»ˆè¾“å‡º ==========
    final_result: Optional[Dict[str, Any]]        # æœ€ç»ˆç»“æœ
    execution_summary: Optional[Dict[str, Any]]   # æ‰§è¡Œæ‘˜è¦
    
    # ========== é”™è¯¯å¤„ç† ==========
    error_log: List[str]                          # é”™è¯¯æ—¥å¿—
    
    # ========== æ¶ˆæ¯å†å² ==========
    messages: Annotated[List, add_messages]       # æ¶ˆæ¯å†å²

# ============================================================================
# æ™ºèƒ½Agentå®šä¹‰
# ============================================================================

class ContextAwareAgent:
    """ä¸Šä¸‹æ–‡æ„ŸçŸ¥Agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.3,
            streaming=True,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"
        )
    
    async def analyze_context(self, state: IntelligentSectionState) -> Dict[str, Any]:
        """åˆ†æä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç ”ç©¶ç­–ç•¥"""
        
        context_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„ç ”ç©¶ç­–ç•¥åˆ†æå¸ˆï¼Œè¯·åˆ†æä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸ºç« èŠ‚"{state['section'].get('title')}"åˆ¶å®šæ™ºèƒ½ç ”ç©¶ç­–ç•¥ã€‚
        
        å½“å‰ç« èŠ‚ä¿¡æ¯ï¼š
        - æ ‡é¢˜ï¼š{state['section'].get('title')}
        - å…³é”®ç‚¹ï¼š{', '.join(state['section'].get('key_points', []))}
        
        ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼š
        - å‰ç½®ç« èŠ‚æ‘˜è¦ï¼š{'; '.join(state.get('previous_sections_summary', []))}
        - åç»­ç« èŠ‚å¤§çº²ï¼š{'; '.join(state.get('upcoming_sections_outline', []))}
        - æŠ¥å‘Šä¸»çº¿ï¼š{state.get('report_main_thread', '')}
        - å†™ä½œé£æ ¼ï¼š{state.get('writing_style', 'professional')}
        
        è¯·æä¾›ï¼š
        1. ä¸Šä¸‹æ–‡å…³è”åˆ†æ
        2. é€»è¾‘è¡”æ¥è¦æ±‚
        3. å†…å®¹é‡ç‚¹å»ºè®®
        4. ç ”ç©¶æŸ¥è¯¢ç­–ç•¥
        
        ä»¥JSONæ ¼å¼è¿”å›ï¼š
        {{
            "context_analysis": "ä¸Šä¸‹æ–‡åˆ†æ",
            "logical_requirements": ["é€»è¾‘è¦æ±‚1", "é€»è¾‘è¦æ±‚2"],
            "content_focus": ["é‡ç‚¹1", "é‡ç‚¹2"],
            "research_queries": ["æŸ¥è¯¢1", "æŸ¥è¯¢2", "æŸ¥è¯¢3"],
            "transition_needs": "è¿‡æ¸¡éœ€æ±‚"
        }}
        """
        
        messages = [HumanMessage(content=context_prompt)]
        response = ""
        
        async for chunk in self.llm.astream(messages):
            if chunk.content:
                response += chunk.content
        
        try:
            # æå–JSON
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            else:
                return {
                    "context_analysis": "ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ",
                    "logical_requirements": ["ä¿æŒé€»è¾‘è¿è´¯"],
                    "content_focus": ["æ ¸å¿ƒå†…å®¹"],
                    "research_queries": [f"{state['topic']} {state['section'].get('title')}"],
                    "transition_needs": "è‡ªç„¶è¿‡æ¸¡"
                }
        except:
            return {
                "context_analysis": "ä¸Šä¸‹æ–‡åˆ†æå®Œæˆ",
                "logical_requirements": ["ä¿æŒé€»è¾‘è¿è´¯"],
                "content_focus": ["æ ¸å¿ƒå†…å®¹"],
                "research_queries": [f"{state['topic']} {state['section'].get('title')}"],
                "transition_needs": "è‡ªç„¶è¿‡æ¸¡"
            }

class QualityAssessmentAgent:
    """è´¨é‡è¯„ä¼°Agent"""
    
    def __init__(self):
        self.llm = ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.1,
            streaming=True,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"
        )
    
    async def assess_content_quality(
        self, 
        content: str, 
        context: Dict[str, Any],
        research_data: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """è¯„ä¼°å†…å®¹è´¨é‡å¹¶æä¾›æ”¹è¿›å»ºè®®"""
        
        assessment_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„å†…å®¹è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œè¯·å…¨é¢è¯„ä¼°ä»¥ä¸‹ç« èŠ‚å†…å®¹çš„è´¨é‡ã€‚
        
        ç« èŠ‚å†…å®¹ï¼š
        {content[:2000]}...
        
        è¯„ä¼°ç»´åº¦ï¼š
        1. å†…å®¹å®Œæ•´æ€§ (0-1)ï¼šæ˜¯å¦è¦†ç›–äº†æ‰€æœ‰å…³é”®ç‚¹
        2. é€»è¾‘è¿è´¯æ€§ (0-1)ï¼šå†…å®¹é€»è¾‘æ˜¯å¦æ¸…æ™°
        3. ä¸Šä¸‹æ–‡ä¸€è‡´æ€§ (0-1)ï¼šä¸å‰åç« èŠ‚çš„è¡”æ¥
        4. æ•°æ®æ”¯æ’‘åº¦ (0-1)ï¼šæ˜¯å¦æœ‰å……åˆ†çš„æ•°æ®æ”¯æ’‘
        5. ä¸“ä¸šæ·±åº¦ (0-1)ï¼šä¸“ä¸šæ€§å’Œæ·±åº¦å¦‚ä½•
        
        ä¸Šä¸‹æ–‡è¦æ±‚ï¼š
        - å‰ç½®ç« èŠ‚ï¼š{context.get('previous_context', '')}
        - é€»è¾‘è¦æ±‚ï¼š{context.get('logical_requirements', [])}
        - å†…å®¹é‡ç‚¹ï¼š{context.get('content_focus', [])}
        
        è¯·ä»¥JSONæ ¼å¼è¿”å›è¯„ä¼°ç»“æœï¼š
        {{
            "completeness_score": 0.85,
            "logical_coherence": 0.90,
            "context_alignment": 0.75,
            "data_sufficiency": 0.80,
            "professional_depth": 0.85,
            "overall_quality": 0.83,
            "content_gaps": ["ç¼ºå£1", "ç¼ºå£2"],
            "improvement_suggestions": ["å»ºè®®1", "å»ºè®®2"],
            "needs_supplementary_research": true,
            "supplementary_topics": ["è¡¥å……ä¸»é¢˜1", "è¡¥å……ä¸»é¢˜2"]
        }}
        """
        
        messages = [HumanMessage(content=assessment_prompt)]
        response = ""
        
        async for chunk in self.llm.astream(messages):
            if chunk.content:
                response += chunk.content
        
        try:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µå­˜åœ¨
                default_result = {
                    "completeness_score": 0.7,
                    "logical_coherence": 0.7,
                    "context_alignment": 0.7,
                    "data_sufficiency": 0.7,
                    "professional_depth": 0.7,
                    "overall_quality": 0.7,
                    "content_gaps": [],
                    "improvement_suggestions": [],
                    "needs_supplementary_research": False,
                    "supplementary_topics": []
                }
                default_result.update(result)
                return default_result
            else:
                return {
                    "completeness_score": 0.6,
                    "logical_coherence": 0.6,
                    "context_alignment": 0.6,
                    "data_sufficiency": 0.6,
                    "professional_depth": 0.6,
                    "overall_quality": 0.6,
                    "content_gaps": ["éœ€è¦æ›´å¤šåˆ†æ"],
                    "improvement_suggestions": ["å¢åŠ å…·ä½“æ¡ˆä¾‹"],
                    "needs_supplementary_research": True,
                    "supplementary_topics": [f"{context.get('topic', '')} æ·±åº¦åˆ†æ"]
                }
        except Exception as e:
            logger.error(f"è´¨é‡è¯„ä¼°è§£æå¤±è´¥: {str(e)}")
            return {
                "completeness_score": 0.5,
                "logical_coherence": 0.5,
                "context_alignment": 0.5,
                "data_sufficiency": 0.5,
                "professional_depth": 0.5,
                "overall_quality": 0.5,
                "content_gaps": ["è¯„ä¼°å¤±è´¥"],
                "improvement_suggestions": ["éœ€è¦é‡æ–°è¯„ä¼°"],
                "needs_supplementary_research": True,
                "supplementary_topics": ["è¡¥å……ç ”ç©¶"]
            }

class AdaptiveResearchAgent:
    """è‡ªé€‚åº”ç ”ç©¶Agent"""
    
    def generate_supplementary_queries(
        self, 
        gaps: List[str], 
        suggestions: List[str],
        topics: List[str],
        context: Dict[str, Any]
    ) -> List[str]:
        """åŸºäºè´¨é‡è¯„ä¼°ç”Ÿæˆè¡¥å……æŸ¥è¯¢"""
        
        supplementary_queries = []
        topic = context.get('topic', '')
        section_title = context.get('section_title', '')
        
        # åŸºäºå†…å®¹ç¼ºå£ç”ŸæˆæŸ¥è¯¢
        for gap in gaps:
            if "æ¡ˆä¾‹" in gap or "å®ä¾‹" in gap:
                supplementary_queries.append(f"{topic} {section_title} æˆåŠŸæ¡ˆä¾‹ è¯¦ç»†åˆ†æ")
            elif "æ•°æ®" in gap or "ç»Ÿè®¡" in gap:
                supplementary_queries.append(f"{topic} {section_title} æœ€æ–°æ•°æ® ç»Ÿè®¡æŠ¥å‘Š")
            elif "è¶‹åŠ¿" in gap or "å‘å±•" in gap:
                supplementary_queries.append(f"{topic} {section_title} å‘å±•è¶‹åŠ¿ æœªæ¥é¢„æµ‹")
            elif "æŠ€æœ¯" in gap:
                supplementary_queries.append(f"{topic} {section_title} æŠ€æœ¯ç»†èŠ‚ å®ç°æ–¹æ¡ˆ")
        
        # åŸºäºæ”¹è¿›å»ºè®®ç”ŸæˆæŸ¥è¯¢
        for suggestion in suggestions:
            if "æ·±å…¥" in suggestion:
                supplementary_queries.append(f"{topic} {section_title} æ·±åº¦åˆ†æ ä¸“ä¸šè§£è¯»")
            elif "å¯¹æ¯”" in suggestion:
                supplementary_queries.append(f"{topic} {section_title} å¯¹æ¯”åˆ†æ ç«äº‰æ ¼å±€")
        
        # åŸºäºè¡¥å……ä¸»é¢˜ç”ŸæˆæŸ¥è¯¢
        for topic_item in topics:
            supplementary_queries.append(f"{topic} {topic_item} è¯¦ç»†ç ”ç©¶")
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_queries = list(set(supplementary_queries))
        return unique_queries[:5]  # æœ€å¤š5ä¸ªè¡¥å……æŸ¥è¯¢

# ============================================================================
# æ ¸å¿ƒèŠ‚ç‚¹å®ç°
# ============================================================================

async def context_aware_planning_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ§  ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ’èŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "context_aware_planning",
        "status": "ğŸ§  å¼€å§‹ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ’",
        "progress": 0,
        "section_title": state["section"].get("title", "æœªçŸ¥ç« èŠ‚")
    })
    
    try:
        # åˆ›å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥Agent
        context_agent = ContextAwareAgent()
        
        # åˆ†æä¸Šä¸‹æ–‡å¹¶ç”Ÿæˆç­–ç•¥
        context_analysis = await context_agent.analyze_context(state)
        
        # æ›´æ–°çŠ¶æ€
        state["logical_connections"] = {
            "context_analysis": context_analysis.get("context_analysis", ""),
            "logical_requirements": context_analysis.get("logical_requirements", []),
            "content_focus": context_analysis.get("content_focus", []),
            "transition_needs": context_analysis.get("transition_needs", "")
        }
        
        # ç”Ÿæˆåˆå§‹ç ”ç©¶æŸ¥è¯¢
        research_queries = context_analysis.get("research_queries", [])
        
        state["current_step"] = "context_planning_completed"
        state["execution_path"] = state.get("execution_path", []) + ["context_aware_planning"]
        state["start_time"] = time.time()
        
        writer({
            "step": "context_aware_planning",
            "status": f"âœ… ä¸Šä¸‹æ–‡è§„åˆ’å®Œæˆï¼šç”Ÿæˆ{len(research_queries)}ä¸ªæ™ºèƒ½æŸ¥è¯¢",
            "progress": 100,
            "research_queries": research_queries,
            "logical_requirements": context_analysis.get("logical_requirements", []),
            "content": {
                "type": "context_planning",
                "data": {
                    "research_queries": research_queries,
                    "context_analysis": context_analysis
                }
            }
        })
        
        # å°†æŸ¥è¯¢å­˜å‚¨åˆ°çŠ¶æ€ä¸­ä¾›åç»­ä½¿ç”¨
        state["research_queries"] = research_queries
        
        logger.info(f"ä¸Šä¸‹æ–‡æ„ŸçŸ¥è§„åˆ’å®Œæˆ: {state['section'].get('title')}")
        return state

    except Exception as e:
        logger.error(f"ä¸Šä¸‹æ–‡è§„åˆ’å¤±è´¥: {str(e)}")
        writer({
            "step": "context_aware_planning",
            "status": f"âŒ è§„åˆ’å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"ä¸Šä¸‹æ–‡è§„åˆ’é”™è¯¯: {str(e)}"]
        state["current_step"] = "planning_failed"
        return state

async def initial_research_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ” åˆæ­¥ç ”ç©¶èŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "initial_research",
        "status": "ğŸ” å¼€å§‹åˆæ­¥å¹¶è¡Œç ”ç©¶",
        "progress": 0
    })

    try:
        from langchain_tavily import TavilySearch

        research_queries = state.get("research_queries", [])
        if not research_queries:
            research_queries = [f"{state['topic']} {state['section'].get('title')}"]

        writer({
            "step": "initial_research",
            "status": f"âš¡ å¹¶è¡Œæ‰§è¡Œ{len(research_queries)}ä¸ªåˆæ­¥æŸ¥è¯¢",
            "progress": 20
        })

        # å¹¶è¡Œæœç´¢æ‰§è¡Œ
        async def search_query(query: str) -> List[Dict[str, Any]]:
            try:
                from langchain_tavily import TavilySearch
                import os

                # è®¾ç½®APIå¯†é’¥
                os.environ["TAVILY_API_KEY"] = "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"

                search_tool = TavilySearch(
                    max_results=3,
                    search_depth="basic"
                )

                search_response = search_tool.invoke(query)

                # å¤„ç†APIé”™è¯¯æˆ–é™åˆ¶
                if isinstance(search_response, dict) and "error" in search_response:
                    logger.warning(f"æœç´¢APIé”™è¯¯: {search_response['error']}")
                    return [{
                        "id": str(uuid.uuid4()),
                        "title": f"æœç´¢ç»“æœ: {query}",
                        "url": "",
                        "content": f"å…³äº{query}çš„æ¨¡æ‹Ÿå†…å®¹ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äºè¯¥ä¸»é¢˜çš„è¯¦ç»†åˆ†æï¼ŒåŒ…å«äº†ç›¸å…³çš„æŠ€æœ¯å‘å±•ã€å¸‚åœºè¶‹åŠ¿å’Œåº”ç”¨æ¡ˆä¾‹ã€‚",
                        "query": query,
                        "relevance_score": 0.8,
                        "timestamp": time.time()
                    }]

                # ä½¿ç”¨æ­£ç¡®çš„å¤„ç†æ–¹å¼
                if not search_response or "results" not in search_response:
                    return [{
                        "id": str(uuid.uuid4()),
                        "title": f"æœç´¢ç»“æœ: {query}",
                        "url": "",
                        "content": f"å…³äº{query}çš„æ¨¡æ‹Ÿå†…å®¹ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äºè¯¥ä¸»é¢˜çš„è¯¦ç»†åˆ†æï¼ŒåŒ…å«äº†ç›¸å…³çš„æŠ€æœ¯å‘å±•ã€å¸‚åœºè¶‹åŠ¿å’Œåº”ç”¨æ¡ˆä¾‹ã€‚",
                        "query": query,
                        "relevance_score": 0.8,
                        "timestamp": time.time()
                    }]

                results = search_response["results"]
                if not results:
                    return [{
                        "id": str(uuid.uuid4()),
                        "title": f"æœç´¢ç»“æœ: {query}",
                        "url": "",
                        "content": f"å…³äº{query}çš„æ¨¡æ‹Ÿå†…å®¹ï¼šè¿™æ˜¯ä¸€ä¸ªå…³äºè¯¥ä¸»é¢˜çš„è¯¦ç»†åˆ†æï¼ŒåŒ…å«äº†ç›¸å…³çš„æŠ€æœ¯å‘å±•ã€å¸‚åœºè¶‹åŠ¿å’Œåº”ç”¨æ¡ˆä¾‹ã€‚",
                        "query": query,
                        "relevance_score": 0.8,
                        "timestamp": time.time()
                    }]

                formatted_results = []
                for i, result in enumerate(results[:3]):
                    formatted_result = {
                        "id": str(uuid.uuid4()),
                        "title": result.get("title", f"ç»“æœ {i+1}"),
                        "url": result.get("url", ""),
                        "content": result.get("content", "")[:800],
                        "query": query,
                        "relevance_score": 0.9 - i * 0.1,
                        "timestamp": time.time()
                    }
                    formatted_results.append(formatted_result)

                return formatted_results

            except Exception as e:
                logger.error(f"æœç´¢å¤±è´¥: {query} - {str(e)}")
                return [{
                    "id": str(uuid.uuid4()),
                    "title": f"æœç´¢ç»“æœ: {query}",
                    "content": f"å…³äº{query}çš„æ¨¡æ‹Ÿå†…å®¹...",
                    "query": query,
                    "error": str(e)
                }]

        # åˆ›å»ºå¹¶è¡Œä»»åŠ¡
        search_tasks = [search_query(query) for query in research_queries]
        search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

        # å¤„ç†æœç´¢ç»“æœ
        all_results = []
        for i, results in enumerate(search_results_list):
            if isinstance(results, list):
                all_results.extend(results)
                progress = 20 + ((i + 1) / len(research_queries)) * 60
                writer({
                    "step": "initial_research",
                    "status": f"âœ… æŸ¥è¯¢å®Œæˆ: {research_queries[i]} ({len(results)}ä¸ªç»“æœ)",
                    "progress": int(progress)
                })

        # å»é‡å’Œè´¨é‡ç­›é€‰
        unique_results = []
        seen_urls = set()
        for result in all_results:
            url = result.get("url", "")
            if url and url not in seen_urls:
                seen_urls.add(url)
                unique_results.append(result)
            elif not url:  # æ¨¡æ‹Ÿç»“æœ
                unique_results.append(result)

        state["initial_research_results"] = unique_results
        state["current_step"] = "initial_research_completed"
        state["execution_path"] = state.get("execution_path", []) + ["initial_research"]

        writer({
            "step": "initial_research",
            "status": "ğŸ‰ åˆæ­¥ç ”ç©¶å®Œæˆï¼",
            "progress": 100,
            "total_results": len(unique_results),
            "content": {
                "type": "initial_research_results",
                "data": {
                    "results_count": len(unique_results),
                    "queries_executed": len(research_queries)
                }
            }
        })

        logger.info(f"åˆæ­¥ç ”ç©¶å®Œæˆ: {len(unique_results)}ä¸ªç»“æœ")
        return state

    except Exception as e:
        logger.error(f"åˆæ­¥ç ”ç©¶å¤±è´¥: {str(e)}")
        writer({
            "step": "initial_research",
            "status": f"âŒ ç ”ç©¶å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"åˆæ­¥ç ”ç©¶é”™è¯¯: {str(e)}"]
        state["current_step"] = "initial_research_failed"
        return state

async def draft_content_generation_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ“ åˆç¨¿å†…å®¹ç”ŸæˆèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "draft_generation",
        "status": "ğŸ“ å¼€å§‹ç”Ÿæˆåˆç¨¿å†…å®¹",
        "progress": 0
    })

    try:
        research_results = state.get("initial_research_results", [])
        section = state["section"]
        logical_connections = state.get("logical_connections", {})

        if not research_results:
            writer({
                "step": "draft_generation",
                "status": "æ²¡æœ‰å¯ç”¨çš„ç ”ç©¶ç»“æœ",
                "progress": 100
            })
            return state

        # åˆ›å»ºå†™ä½œLLM
        llm = ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.7,
            streaming=True,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"
        )

        # æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„å†™ä½œæç¤º
        writing_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„æŠ€æœ¯å†™ä½œä¸“å®¶ï¼Œè¯·åŸºäºç ”ç©¶ç»“æœå’Œä¸Šä¸‹æ–‡è¦æ±‚ä¸º"{section.get('title')}"ç« èŠ‚æ’°å†™åˆç¨¿å†…å®¹ã€‚

        ç« èŠ‚ä¿¡æ¯ï¼š
        - æ ‡é¢˜ï¼š{section.get('title')}
        - å…³é”®ç‚¹ï¼š{', '.join(section.get('key_points', []))}
        - ä¸»é¢˜ï¼š{state['topic']}

        ä¸Šä¸‹æ–‡è¦æ±‚ï¼š
        - å‰ç½®ç« èŠ‚æ‘˜è¦ï¼š{'; '.join(state.get('previous_sections_summary', []))}
        - é€»è¾‘è¦æ±‚ï¼š{', '.join(logical_connections.get('logical_requirements', []))}
        - å†…å®¹é‡ç‚¹ï¼š{', '.join(logical_connections.get('content_focus', []))}
        - è¿‡æ¸¡éœ€æ±‚ï¼š{logical_connections.get('transition_needs', '')}

        ç ”ç©¶æ•°æ®æ‘˜è¦ï¼š
        {json.dumps([{
            'title': r.get('title', ''),
            'content': r.get('content', '')[:300]
        } for r in research_results[:8]], ensure_ascii=False, indent=2)}

        å†™ä½œè¦æ±‚ï¼š
        1. ä¸å‰åç« èŠ‚é€»è¾‘è¡”æ¥è‡ªç„¶
        2. ä½“ç°ä¸Šä¸‹æ–‡çš„è¿è´¯æ€§
        3. ç»“æ„æ¸…æ™°ï¼Œå±‚æ¬¡åˆ†æ˜
        4. å†…å®¹ä¸“ä¸šï¼Œæ•°æ®æ”¯æ’‘
        5. å­—æ•°æ§åˆ¶åœ¨800-1200å­—

        è¯·æ’°å†™å®Œæ•´çš„ç« èŠ‚åˆç¨¿å†…å®¹ã€‚
        """

        writer({
            "step": "draft_generation",
            "status": "ğŸ¤– AIæ­£åœ¨ç”Ÿæˆä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„åˆç¨¿...",
            "progress": 30
        })

        # æµå¼ç”Ÿæˆåˆç¨¿
        messages = [HumanMessage(content=writing_prompt)]
        draft_content = ""
        chunk_count = 0

        async for chunk in llm.astream(messages):
            if chunk.content:
                draft_content += chunk.content
                chunk_count += 1

                if chunk_count % 15 == 0:
                    progress = min(90, 30 + (chunk_count // 15) * 10)
                    writer({
                        "step": "draft_generation",
                        "status": f"ğŸ“ æ­£åœ¨ç”Ÿæˆ... ({len(draft_content)}å­—ç¬¦)",
                        "progress": progress
                    })

        state["draft_content"] = draft_content
        state["current_step"] = "draft_completed"
        state["execution_path"] = state.get("execution_path", []) + ["draft_generation"]

        writer({
            "step": "draft_generation",
            "status": "âœ… åˆç¨¿ç”Ÿæˆå®Œæˆï¼",
            "progress": 100,
            "content_length": len(draft_content),
            "word_count": len(draft_content.split()),
            "content": {
                "type": "draft_content",
                "data": {
                    "content_preview": draft_content[:200] + "..." if len(draft_content) > 200 else draft_content,
                    "word_count": len(draft_content.split()),
                    "character_count": len(draft_content)
                }
            }
        })

        logger.info(f"åˆç¨¿ç”Ÿæˆå®Œæˆ: {len(draft_content)}å­—ç¬¦")
        return state

    except Exception as e:
        logger.error(f"åˆç¨¿ç”Ÿæˆå¤±è´¥: {str(e)}")
        writer({
            "step": "draft_generation",
            "status": f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"åˆç¨¿ç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "draft_failed"
        return state

async def quality_assessment_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ“Š æ™ºèƒ½è´¨é‡è¯„ä¼°èŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "quality_assessment",
        "status": "ğŸ“Š å¼€å§‹æ™ºèƒ½è´¨é‡è¯„ä¼°",
        "progress": 0
    })

    try:
        draft_content = state.get("draft_content") or state.get("enhanced_content", "")
        if not draft_content:
            writer({
                "step": "quality_assessment",
                "status": "æ²¡æœ‰å¯è¯„ä¼°çš„å†…å®¹",
                "progress": 100
            })
            return state

        # åˆ›å»ºè´¨é‡è¯„ä¼°Agent
        quality_agent = QualityAssessmentAgent()

        # å‡†å¤‡è¯„ä¼°ä¸Šä¸‹æ–‡
        assessment_context = {
            "topic": state.get("topic", ""),
            "section_title": state["section"].get("title", ""),
            "previous_context": "; ".join(state.get("previous_sections_summary", [])),
            "logical_requirements": state.get("logical_connections", {}).get("logical_requirements", []),
            "content_focus": state.get("logical_connections", {}).get("content_focus", [])
        }

        writer({
            "step": "quality_assessment",
            "status": "ğŸ¤– AIæ­£åœ¨è¿›è¡Œå¤šç»´åº¦è´¨é‡è¯„ä¼°...",
            "progress": 30
        })

        # æ‰§è¡Œè´¨é‡è¯„ä¼°
        research_data = state.get("initial_research_results", []) + state.get("supplementary_research_results", [])
        quality_assessment = await quality_agent.assess_content_quality(
            draft_content,
            assessment_context,
            research_data
        )

        # æ›´æ–°çŠ¶æ€
        state["quality_assessment"] = quality_assessment
        state["content_gaps"] = quality_assessment.get("content_gaps", [])
        state["improvement_suggestions"] = quality_assessment.get("improvement_suggestions", [])

        # åˆ¤æ–­æ˜¯å¦éœ€è¦è¡¥å……ç ”ç©¶
        overall_quality = quality_assessment.get("overall_quality", 0)
        quality_threshold = state.get("quality_threshold", 0.75)
        needs_supplementary = quality_assessment.get("needs_supplementary_research", False)

        if overall_quality < quality_threshold or needs_supplementary:
            state["current_step"] = "needs_improvement"
        else:
            state["current_step"] = "quality_approved"

        state["execution_path"] = state.get("execution_path", []) + ["quality_assessment"]

        writer({
            "step": "quality_assessment",
            "status": f"âœ… è´¨é‡è¯„ä¼°å®Œæˆï¼ç»¼åˆè¯„åˆ†: {overall_quality:.3f}",
            "progress": 100,
            "overall_quality": overall_quality,
            "quality_threshold": quality_threshold,
            "needs_improvement": overall_quality < quality_threshold or needs_supplementary,
            "content_gaps_count": len(quality_assessment.get("content_gaps", [])),
            "content": {
                "type": "quality_assessment",
                "data": {
                    "overall_quality": overall_quality,
                    "detailed_scores": {
                        "completeness": quality_assessment.get("completeness_score", 0),
                        "coherence": quality_assessment.get("logical_coherence", 0),
                        "context_alignment": quality_assessment.get("context_alignment", 0),
                        "data_sufficiency": quality_assessment.get("data_sufficiency", 0),
                        "professional_depth": quality_assessment.get("professional_depth", 0)
                    },
                    "content_gaps": quality_assessment.get("content_gaps", []),
                    "improvement_suggestions": quality_assessment.get("improvement_suggestions", [])
                }
            }
        })

        logger.info(f"è´¨é‡è¯„ä¼°å®Œæˆ: ç»¼åˆè¯„åˆ†{overall_quality:.3f}, éœ€è¦æ”¹è¿›: {overall_quality < quality_threshold}")
        return state

    except Exception as e:
        logger.error(f"è´¨é‡è¯„ä¼°å¤±è´¥: {str(e)}")
        writer({
            "step": "quality_assessment",
            "status": f"âŒ è¯„ä¼°å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"è´¨é‡è¯„ä¼°é”™è¯¯: {str(e)}"]
        state["current_step"] = "assessment_failed"
        return state

def quality_decision_node(state: IntelligentSectionState) -> str:
    """è´¨é‡å†³ç­–èŠ‚ç‚¹ - å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨"""
    current_step = state.get("current_step", "")
    iteration_count = state.get("iteration_count", 0)
    max_iterations = state.get("max_iterations", 3)

    if current_step == "quality_approved":
        return "content_polishing"
    elif current_step == "needs_improvement" and iteration_count < max_iterations:
        return "supplementary_research"
    else:
        # è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œç›´æ¥è¿›å…¥æ¶¦è‰²
        return "content_polishing"

async def supplementary_research_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ¯ è¡¥å……ç ”ç©¶èŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "supplementary_research",
        "status": "ğŸ¯ å¼€å§‹è¡¥å……ç ”ç©¶",
        "progress": 0
    })

    try:
        quality_assessment = state.get("quality_assessment", {})
        content_gaps = quality_assessment.get("content_gaps", [])
        improvement_suggestions = quality_assessment.get("improvement_suggestions", [])
        supplementary_topics = quality_assessment.get("supplementary_topics", [])

        if not (content_gaps or improvement_suggestions or supplementary_topics):
            writer({
                "step": "supplementary_research",
                "status": "æ²¡æœ‰éœ€è¦è¡¥å……çš„ç ”ç©¶å†…å®¹",
                "progress": 100
            })
            return state

        # åˆ›å»ºè‡ªé€‚åº”ç ”ç©¶Agent
        research_agent = AdaptiveResearchAgent()

        # ç”Ÿæˆè¡¥å……æŸ¥è¯¢
        supplementary_queries = research_agent.generate_supplementary_queries(
            content_gaps,
            improvement_suggestions,
            supplementary_topics,
            {
                "topic": state.get("topic", ""),
                "section_title": state["section"].get("title", "")
            }
        )

        writer({
            "step": "supplementary_research",
            "status": f"ğŸ” æ‰§è¡Œ{len(supplementary_queries)}ä¸ªè¡¥å……æŸ¥è¯¢",
            "progress": 20
        })

        # æ‰§è¡Œè¡¥å……æœç´¢
        async def supplementary_search(query: str) -> List[Dict[str, Any]]:
            try:
                from langchain_tavily import TavilySearch
                import os

                # è®¾ç½®APIå¯†é’¥
                os.environ["TAVILY_API_KEY"] = "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"

                search_tool = TavilySearch(
                    max_results=2,
                    search_depth="advanced"  # ä½¿ç”¨é«˜çº§æœç´¢è·å¾—æ›´å¥½ç»“æœ
                )

                search_response = search_tool.invoke(query)

                # ä½¿ç”¨æ­£ç¡®çš„å¤„ç†æ–¹å¼
                if not search_response or "results" not in search_response:
                    return [{
                        "id": str(uuid.uuid4()),
                        "title": f"è¡¥å……æœç´¢ç»“æœ: {query}",
                        "url": "",
                        "content": f"å…³äº{query}çš„è¡¥å……å†…å®¹...",
                        "query": query,
                        "search_type": "supplementary",
                        "relevance_score": 0.85,
                        "timestamp": time.time()
                    }]

                results = search_response["results"]
                if not results:
                    return [{
                        "id": str(uuid.uuid4()),
                        "title": f"è¡¥å……æœç´¢ç»“æœ: {query}",
                        "url": "",
                        "content": f"å…³äº{query}çš„è¡¥å……å†…å®¹...",
                        "query": query,
                        "search_type": "supplementary",
                        "relevance_score": 0.85,
                        "timestamp": time.time()
                    }]

                formatted_results = []
                for i, result in enumerate(results[:2]):
                    formatted_result = {
                        "id": str(uuid.uuid4()),
                        "title": result.get("title", f"è¡¥å……ç»“æœ {i+1}"),
                        "url": result.get("url", ""),
                        "content": result.get("content", "")[:600],
                        "query": query,
                        "search_type": "supplementary",
                        "relevance_score": 0.95 - i * 0.05,
                        "timestamp": time.time()
                    }
                    formatted_results.append(formatted_result)

                return formatted_results

            except Exception as e:
                logger.error(f"è¡¥å……æœç´¢å¤±è´¥: {query} - {str(e)}")
                return [{
                    "id": str(uuid.uuid4()),
                    "title": f"è¡¥å……æœç´¢ç»“æœ: {query}",
                    "content": f"å…³äº{query}çš„è¡¥å……å†…å®¹...",
                    "query": query,
                    "search_type": "supplementary",
                    "error": str(e)
                }]

        # å¹¶è¡Œæ‰§è¡Œè¡¥å……æœç´¢
        search_tasks = [supplementary_search(query) for query in supplementary_queries]
        search_results_list = await asyncio.gather(*search_tasks, return_exceptions=True)

        # å¤„ç†è¡¥å……æœç´¢ç»“æœ
        supplementary_results = []
        for i, results in enumerate(search_results_list):
            if isinstance(results, list):
                supplementary_results.extend(results)
                progress = 20 + ((i + 1) / len(supplementary_queries)) * 60
                writer({
                    "step": "supplementary_research",
                    "status": f"âœ… è¡¥å……æŸ¥è¯¢å®Œæˆ: {supplementary_queries[i]}",
                    "progress": int(progress)
                })

        state["supplementary_research_results"] = supplementary_results
        state["iteration_count"] = state.get("iteration_count", 0) + 1
        state["current_step"] = "supplementary_completed"
        state["execution_path"] = state.get("execution_path", []) + ["supplementary_research"]

        writer({
            "step": "supplementary_research",
            "status": "ğŸ‰ è¡¥å……ç ”ç©¶å®Œæˆï¼",
            "progress": 100,
            "supplementary_results": len(supplementary_results),
            "iteration_count": state["iteration_count"],
            "content": {
                "type": "supplementary_research",
                "data": {
                    "results_count": len(supplementary_results),
                    "queries_executed": len(supplementary_queries),
                    "iteration": state["iteration_count"]
                }
            }
        })

        logger.info(f"è¡¥å……ç ”ç©¶å®Œæˆ: {len(supplementary_results)}ä¸ªè¡¥å……ç»“æœ")
        return state

    except Exception as e:
        logger.error(f"è¡¥å……ç ”ç©¶å¤±è´¥: {str(e)}")
        writer({
            "step": "supplementary_research",
            "status": f"âŒ è¡¥å……ç ”ç©¶å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"è¡¥å……ç ”ç©¶é”™è¯¯: {str(e)}"]
        state["current_step"] = "supplementary_failed"
        return state

async def content_enhancement_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ“ å†…å®¹å¢å¼ºèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "content_enhancement",
        "status": "ğŸ“ å¼€å§‹å†…å®¹å¢å¼º",
        "progress": 0
    })

    try:
        draft_content = state.get("draft_content", "")
        supplementary_results = state.get("supplementary_research_results", [])
        quality_assessment = state.get("quality_assessment", {})

        if not draft_content:
            writer({
                "step": "content_enhancement",
                "status": "æ²¡æœ‰å¯å¢å¼ºçš„å†…å®¹",
                "progress": 100
            })
            return state

        # åˆ›å»ºå¢å¼ºLLM
        llm = ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.6,
            streaming=True,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"
        )

        # æ„å»ºå¢å¼ºæç¤º
        enhancement_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„å†…å®¹å¢å¼ºä¸“å®¶ï¼Œè¯·åŸºäºè´¨é‡è¯„ä¼°åé¦ˆå’Œè¡¥å……ç ”ç©¶ç»“æœï¼Œå¯¹ä»¥ä¸‹ç« èŠ‚å†…å®¹è¿›è¡Œæ™ºèƒ½å¢å¼ºã€‚

        åŸå§‹å†…å®¹ï¼š
        {draft_content}

        è´¨é‡è¯„ä¼°åé¦ˆï¼š
        - å†…å®¹ç¼ºå£ï¼š{', '.join(quality_assessment.get('content_gaps', []))}
        - æ”¹è¿›å»ºè®®ï¼š{', '.join(quality_assessment.get('improvement_suggestions', []))}
        - å½“å‰è´¨é‡è¯„åˆ†ï¼š{quality_assessment.get('overall_quality', 0):.2f}

        è¡¥å……ç ”ç©¶æ•°æ®ï¼š
        {json.dumps([{
            'title': r.get('title', ''),
            'content': r.get('content', '')[:300]
        } for r in supplementary_results[:5]], ensure_ascii=False, indent=2)}

        å¢å¼ºè¦æ±‚ï¼š
        1. é’ˆå¯¹æ€§å¡«è¡¥å†…å®¹ç¼ºå£
        2. æ•´åˆè¡¥å……ç ”ç©¶æ•°æ®
        3. æå‡ä¸“ä¸šæ·±åº¦å’Œæ•°æ®æ”¯æ’‘
        4. ä¿æŒé€»è¾‘è¿è´¯æ€§
        5. ä¼˜åŒ–ç»“æ„å’Œè¡¨è¾¾

        è¯·è¾“å‡ºå¢å¼ºåçš„å®Œæ•´ç« èŠ‚å†…å®¹ã€‚
        """

        writer({
            "step": "content_enhancement",
            "status": "ğŸ¤– AIæ­£åœ¨è¿›è¡Œæ™ºèƒ½å†…å®¹å¢å¼º...",
            "progress": 30
        })

        # æµå¼ç”Ÿæˆå¢å¼ºå†…å®¹
        messages = [HumanMessage(content=enhancement_prompt)]
        enhanced_content = ""
        chunk_count = 0

        async for chunk in llm.astream(messages):
            if chunk.content:
                enhanced_content += chunk.content
                chunk_count += 1

                if chunk_count % 20 == 0:
                    progress = min(90, 30 + (chunk_count // 20) * 10)
                    writer({
                        "step": "content_enhancement",
                        "status": f"ğŸ“ æ­£åœ¨å¢å¼º... ({len(enhanced_content)}å­—ç¬¦)",
                        "progress": progress
                    })

        state["enhanced_content"] = enhanced_content
        state["current_step"] = "enhancement_completed"
        state["execution_path"] = state.get("execution_path", []) + ["content_enhancement"]

        writer({
            "step": "content_enhancement",
            "status": "âœ… å†…å®¹å¢å¼ºå®Œæˆï¼",
            "progress": 100,
            "enhanced_length": len(enhanced_content),
            "improvement_ratio": len(enhanced_content) / max(len(draft_content), 1),
            "content": {
                "type": "enhanced_content",
                "data": {
                    "content_preview": enhanced_content[:200] + "..." if len(enhanced_content) > 200 else enhanced_content,
                    "word_count": len(enhanced_content.split()),
                    "character_count": len(enhanced_content),
                    "improvement_ratio": len(enhanced_content) / max(len(draft_content), 1)
                }
            }
        })

        logger.info(f"å†…å®¹å¢å¼ºå®Œæˆ: {len(enhanced_content)}å­—ç¬¦")
        return state

    except Exception as e:
        logger.error(f"å†…å®¹å¢å¼ºå¤±è´¥: {str(e)}")
        writer({
            "step": "content_enhancement",
            "status": f"âŒ å¢å¼ºå¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"å†…å®¹å¢å¼ºé”™è¯¯: {str(e)}"]
        state["current_step"] = "enhancement_failed"
        return state

async def content_polishing_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """âœ¨ æ™ºèƒ½æ¶¦è‰²èŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "content_polishing",
        "status": "âœ¨ å¼€å§‹æ™ºèƒ½æ¶¦è‰²",
        "progress": 0
    })

    try:
        # é€‰æ‹©æœ€ä½³å†…å®¹ç‰ˆæœ¬è¿›è¡Œæ¶¦è‰²
        content_to_polish = (
            state.get("enhanced_content") or
            state.get("draft_content", "")
        )

        if not content_to_polish:
            writer({
                "step": "content_polishing",
                "status": "æ²¡æœ‰å¯æ¶¦è‰²çš„å†…å®¹",
                "progress": 100
            })
            return state

        # åˆ›å»ºæ¶¦è‰²LLM
        llm = ChatOpenAI(
            model="qwen2.5-72b-instruct-awq",
            temperature=0.4,
            streaming=True,
            base_url="https://llm.3qiao.vip:23436/v1",
            api_key="sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"
        )

        # æ„å»ºæ¶¦è‰²æç¤º
        logical_connections = state.get("logical_connections", {})
        writing_style = state.get("writing_style", "professional")

        polishing_prompt = f"""
        ä½œä¸ºä¸“ä¸šçš„æ–‡æœ¬æ¶¦è‰²ä¸“å®¶ï¼Œè¯·å¯¹ä»¥ä¸‹ç« èŠ‚å†…å®¹è¿›è¡Œç²¾ç»†æ¶¦è‰²ï¼Œç¡®ä¿è¾¾åˆ°å‡ºç‰ˆçº§åˆ«çš„è´¨é‡ã€‚

        å¾…æ¶¦è‰²å†…å®¹ï¼š
        {content_to_polish}

        æ¶¦è‰²è¦æ±‚ï¼š
        1. è¯­è¨€è¡¨è¾¾ï¼šä¼˜åŒ–è¯­è¨€æµç•…æ€§å’Œä¸“ä¸šæ€§
        2. ç»“æ„ä¼˜åŒ–ï¼šè°ƒæ•´æ®µè½ç»“æ„ï¼Œå¢å¼ºé€»è¾‘æ€§
        3. é£æ ¼ç»Ÿä¸€ï¼šä¿æŒ{writing_style}çš„å†™ä½œé£æ ¼
        4. ä¸Šä¸‹æ–‡è¡”æ¥ï¼šç¡®ä¿ä¸å‰åç« èŠ‚çš„è‡ªç„¶è¿‡æ¸¡
        5. ä¸“ä¸šæå‡ï¼šå¢å¼ºä¸“ä¸šæœ¯è¯­çš„å‡†ç¡®æ€§

        ä¸Šä¸‹æ–‡è¦æ±‚ï¼š
        - é€»è¾‘è¦æ±‚ï¼š{', '.join(logical_connections.get('logical_requirements', []))}
        - è¿‡æ¸¡éœ€æ±‚ï¼š{logical_connections.get('transition_needs', '')}

        è¯·è¾“å‡ºæ¶¦è‰²åçš„æœ€ç»ˆç« èŠ‚å†…å®¹ï¼Œç¡®ä¿è´¨é‡è¾¾åˆ°ä¸“ä¸šæŠ¥å‘Šæ ‡å‡†ã€‚
        """

        writer({
            "step": "content_polishing",
            "status": "ğŸ¤– AIæ­£åœ¨è¿›è¡Œä¸“ä¸šæ¶¦è‰²...",
            "progress": 30
        })

        # æµå¼ç”Ÿæˆæ¶¦è‰²å†…å®¹
        messages = [HumanMessage(content=polishing_prompt)]
        polished_content = ""
        chunk_count = 0

        async for chunk in llm.astream(messages):
            if chunk.content:
                polished_content += chunk.content
                chunk_count += 1

                if chunk_count % 15 == 0:
                    progress = min(90, 30 + (chunk_count // 15) * 10)
                    writer({
                        "step": "content_polishing",
                        "status": f"âœ¨ æ­£åœ¨æ¶¦è‰²... ({len(polished_content)}å­—ç¬¦)",
                        "progress": progress
                    })

        state["polished_content"] = polished_content
        state["final_content"] = polished_content  # è®¾ç½®ä¸ºæœ€ç»ˆå†…å®¹
        state["current_step"] = "polishing_completed"
        state["execution_path"] = state.get("execution_path", []) + ["content_polishing"]

        writer({
            "step": "content_polishing",
            "status": "ğŸ‰ æ™ºèƒ½æ¶¦è‰²å®Œæˆï¼",
            "progress": 100,
            "final_length": len(polished_content),
            "final_word_count": len(polished_content.split()),
            "content": {
                "type": "polished_content",
                "data": {
                    "content_preview": polished_content[:300] + "..." if len(polished_content) > 300 else polished_content,
                    "word_count": len(polished_content.split()),
                    "character_count": len(polished_content),
                    "quality_level": "professional"
                }
            }
        })

        logger.info(f"æ™ºèƒ½æ¶¦è‰²å®Œæˆ: {len(polished_content)}å­—ç¬¦")
        return state

    except Exception as e:
        logger.error(f"æ™ºèƒ½æ¶¦è‰²å¤±è´¥: {str(e)}")
        writer({
            "step": "content_polishing",
            "status": f"âŒ æ¶¦è‰²å¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"æ¶¦è‰²é”™è¯¯: {str(e)}"]
        state["current_step"] = "polishing_failed"
        return state

async def final_integration_node(state: IntelligentSectionState, config=None) -> IntelligentSectionState:
    """ğŸ¯ æœ€ç»ˆæ•´åˆèŠ‚ç‚¹"""
    writer = safe_get_stream_writer()
    writer({
        "step": "final_integration",
        "status": "ğŸ¯ å¼€å§‹æœ€ç»ˆæ•´åˆ",
        "progress": 0
    })

    try:
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = time.time() - state.get("start_time", time.time())

        # æ„å»ºæœ€ç»ˆç»“æœ
        final_result = {
            "section_id": state["section"].get("id"),
            "section_title": state["section"].get("title"),
            "topic": state["topic"],
            "final_content": state.get("final_content", ""),

            "quality_metrics": {
                "final_quality_score": state.get("quality_assessment", {}).get("overall_quality", 0),
                "iteration_count": state.get("iteration_count", 0),
                "research_results_count": len(state.get("initial_research_results", [])),
                "supplementary_results_count": len(state.get("supplementary_research_results", [])),
                "content_evolution": {
                    "draft_length": len(state.get("draft_content", "")),
                    "enhanced_length": len(state.get("enhanced_content", "")),
                    "final_length": len(state.get("final_content", ""))
                }
            },

            "context_integration": {
                "logical_connections": state.get("logical_connections", {}),
                "context_awareness": True,
                "previous_sections_considered": len(state.get("previous_sections_summary", [])),
                "upcoming_sections_considered": len(state.get("upcoming_sections_outline", []))
            },

            "research_data": {
                "initial_research": state.get("initial_research_results", []),
                "supplementary_research": state.get("supplementary_research_results", []),
                "quality_assessment": state.get("quality_assessment", {}),
                "content_gaps_addressed": state.get("content_gaps", []),
                "improvements_made": state.get("improvement_suggestions", [])
            },

            "execution_metadata": {
                "execution_time": execution_time,
                "execution_path": state.get("execution_path", []),
                "iteration_count": state.get("iteration_count", 0),
                "max_iterations": state.get("max_iterations", 3),
                "quality_threshold": state.get("quality_threshold", 0.75),
                "timestamp": datetime.now().isoformat(),
                "system_version": "intelligent_v1.0"
            }
        }

        # ç”Ÿæˆæ‰§è¡Œæ‘˜è¦
        execution_summary = {
            "status": "completed",
            "execution_time": execution_time,
            "steps_completed": len(state.get("execution_path", [])),
            "iterations_performed": state.get("iteration_count", 0),
            "final_quality_score": state.get("quality_assessment", {}).get("overall_quality", 0),
            "error_count": len(state.get("error_log", [])),
            "intelligence_features": {
                "context_aware": True,
                "quality_driven": True,
                "iterative_improvement": state.get("iteration_count", 0) > 0,
                "adaptive_research": len(state.get("supplementary_research_results", [])) > 0
            }
        }

        # æ›´æ–°çŠ¶æ€
        state["final_result"] = final_result
        state["execution_summary"] = execution_summary
        state["current_step"] = "completed"
        state["execution_path"] = state.get("execution_path", []) + ["final_integration"]

        writer({
            "step": "final_integration",
            "status": "ğŸ‰ æ™ºèƒ½ç« èŠ‚ç ”ç©¶å®Œæˆï¼",
            "progress": 100,
            "execution_time": execution_time,
            "final_quality": state.get("quality_assessment", {}).get("overall_quality", 0),
            "iterations": state.get("iteration_count", 0),
            "content": {
                "type": "final_result",
                "data": final_result
            }
        })

        logger.info(f"æ™ºèƒ½ç« èŠ‚ç ”ç©¶å®Œæˆ: {state['section'].get('title')}, æ‰§è¡Œæ—¶é—´{execution_time:.2f}ç§’, è¿­ä»£{state.get('iteration_count', 0)}æ¬¡")
        return state

    except Exception as e:
        logger.error(f"æœ€ç»ˆæ•´åˆå¤±è´¥: {str(e)}")
        writer({
            "step": "final_integration",
            "status": f"âŒ æ•´åˆå¤±è´¥: {str(e)}",
            "progress": -1
        })

        state["error_log"] = state.get("error_log", []) + [f"æœ€ç»ˆæ•´åˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "integration_failed"
        return state

# ============================================================================
# Graphæ„å»º
# ============================================================================

def create_intelligent_section_research_graph() -> StateGraph:
    """åˆ›å»ºæ™ºèƒ½ç« èŠ‚ç ”ç©¶Graph"""

    # åˆ›å»ºçŠ¶æ€å›¾
    workflow = StateGraph(IntelligentSectionState)

    # æ·»åŠ èŠ‚ç‚¹
    workflow.add_node("context_aware_planning", context_aware_planning_node)
    workflow.add_node("initial_research", initial_research_node)
    workflow.add_node("draft_generation", draft_content_generation_node)
    workflow.add_node("quality_assessment", quality_assessment_node)
    workflow.add_node("supplementary_research", supplementary_research_node)
    workflow.add_node("content_enhancement", content_enhancement_node)
    workflow.add_node("content_polishing", content_polishing_node)
    workflow.add_node("final_integration", final_integration_node)

    # å®šä¹‰æµç¨‹
    workflow.add_edge(START, "context_aware_planning")
    workflow.add_edge("context_aware_planning", "initial_research")
    workflow.add_edge("initial_research", "draft_generation")
    workflow.add_edge("draft_generation", "quality_assessment")

    # è´¨é‡å†³ç­–åˆ†æ”¯
    workflow.add_conditional_edges(
        "quality_assessment",
        quality_decision_node,
        {
            "supplementary_research": "supplementary_research",
            "content_polishing": "content_polishing"
        }
    )

    # è¡¥å……ç ”ç©¶åçš„æµç¨‹
    workflow.add_edge("supplementary_research", "content_enhancement")
    workflow.add_edge("content_enhancement", "quality_assessment")  # é‡æ–°è¯„ä¼°è´¨é‡

    # æ¶¦è‰²åçš„æœ€ç»ˆæ•´åˆ
    workflow.add_edge("content_polishing", "final_integration")
    workflow.add_edge("final_integration", END)

    return workflow
