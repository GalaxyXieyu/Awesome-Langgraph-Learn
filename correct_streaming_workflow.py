#!/usr/bin/env python3
"""
æ­£ç¡®çš„æµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•è„šæœ¬
æŒ‰ç…§å®˜æ–¹æ¨¡å¼å®ç°ï¼šæ£€æµ‹ä¸­æ–­ â†’ ç”¨æˆ·è¾“å…¥ â†’ æ¢å¤æ‰§è¡Œ â†’ æµå¼è¾“å‡º
"""

import asyncio
import time
from typing import Dict, Any, cast
from graph import create_writing_assistant_graph, initialize_writing_state
from tools import load_knowledge_bases
from langgraph.types import Command
from langgraph.checkpoint.memory import MemorySaver  # ç¡®ä¿checkpointerå¯ç”¨


def print_streaming_content(chunk: Any):
    """å¤„ç†æµå¼chunk - ç»è¿‡éªŒè¯çš„æ–¹æ³•"""
    if isinstance(chunk, tuple) and len(chunk) > 0:
        message_chunk = chunk[0]
        if hasattr(message_chunk, 'content'):
            content = message_chunk.content
            if content and isinstance(content, str):
                print(content, end="", flush=True)
                return True
    return False


async def correct_streaming_workflow():
    """
    æ­£ç¡®çš„æµå¼+ä¸­æ–­å¤„ç†å·¥ä½œæµç¨‹
    """
    print("ğŸ¯ æ­£ç¡®çš„LangGraphæµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•")
    print("ğŸ”§ åŸºäºå®˜æ–¹æ¨¡å¼ï¼šä¸­æ–­æ£€æµ‹ â†’ ç”¨æˆ·è¾“å…¥ â†’ æ¢å¤æ‰§è¡Œ")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    try:
        load_knowledge_bases()
        print("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ çŸ¥è¯†åº“åˆå§‹åŒ–è­¦å‘Š: {e}")
    
    graph = create_writing_assistant_graph()
    print("âœ… å†™ä½œåŠ©æ‰‹å›¾åˆ›å»ºæˆåŠŸ")
    
    # ğŸ”¥ éªŒè¯checkpointeré…ç½®
    if hasattr(graph, 'checkpointer') and graph.checkpointer:
        print("âœ… Checkpointeré…ç½®æ­£ç¡®")
    else:
        print("âš ï¸ è­¦å‘Šï¼šCheckpointerå¯èƒ½æœªæ­£ç¡®é…ç½®")
    
    # å®Œæ•´å·¥ä½œæµç¨‹é…ç½®
    initial_state = initialize_writing_state(
        topic="Pythonå¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ",
        user_id="correct_workflow_test",
        max_words=600,
        style="technical",
        language="zh",
        enable_search=True  # ğŸ”¥ å¯ç”¨å®Œæ•´åŠŸèƒ½
    )
    
    config: Dict[str, Any] = {"configurable": {"thread_id": "correct_workflow_001"}}
    
    print(f"ğŸ“ ä¸»é¢˜: {initial_state['topic']}")
    print(f"ğŸ¯ å­—æ•°: {initial_state['max_words']}")
    print(f"ğŸ” æœç´¢: å¯ç”¨")
    print(f"ğŸ§  RAG: å¯ç”¨")
    print("=" * 60)
    
    start_time = time.time()
    total_tokens = 0
    step_count = 0
    
    try:
        current_input = initial_state
        
        while step_count < 20:  # å®‰å…¨é™åˆ¶
            step_count += 1
            print(f"\nğŸ“ æ­¥éª¤ {step_count}: å¯åŠ¨å·¥ä½œæµç¨‹")
            print("-" * 40)
            
            # ğŸ”¥ æ­¥éª¤1ï¼šå¯åŠ¨å·¥ä½œæµç¨‹å¹¶æ£€æµ‹ä¸­æ–­ï¼ˆæŒ‰ç…§å®˜æ–¹æ¨¡å¼ï¼‰
            interrupted = False
            interrupt_info = None
            
            # é¦–å…ˆå°è¯•æ­£å¸¸æ‰§è¡Œï¼Œæ£€æµ‹æ˜¯å¦æœ‰ä¸­æ–­
            try:
                final_result = await graph.ainvoke(current_input, cast(Any, config))
                
                # æ£€æŸ¥ç»“æœä¸­æ˜¯å¦æœ‰ä¸­æ–­æ ‡è®°
                if "__interrupt__" in str(final_result) or (isinstance(final_result, dict) and "__interrupt__" in final_result):
                    interrupted = True
                    interrupt_info = final_result.get("__interrupt__") if isinstance(final_result, dict) else "unknown interrupt"
                    print(f"â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­")
                else:
                    # æ²¡æœ‰ä¸­æ–­ï¼Œå·¥ä½œæµç¨‹å®Œæˆï¼Œç°åœ¨è·å–æµå¼è¾“å‡º
                    print(f"ğŸŒŠ å·¥ä½œæµç¨‹å®Œæˆï¼Œè·å–æµå¼è¾“å‡º...")
                    async for chunk in graph.astream(current_input, cast(Any, config), stream_mode="messages"):
                        if print_streaming_content(chunk):
                            total_tokens += 1
                            
                            if total_tokens % 30 == 0:
                                elapsed = time.time() - start_time
                                print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
                    break
                    
            except Exception as e:
                if "interrupt" in str(e).lower():
                    interrupted = True
                    interrupt_info = str(e)
                    print(f"â¸ï¸  é€šè¿‡å¼‚å¸¸æ£€æµ‹åˆ°ä¸­æ–­: {str(e)[:100]}")
                else:
                    raise e
            
            if not interrupted:
                # æ²¡æœ‰ä¸­æ–­ï¼Œå·¥ä½œæµç¨‹å®Œæˆ
                print(f"\nğŸ‰ å·¥ä½œæµç¨‹å®Œæˆï¼")
                break
            
            # ğŸ”¥ æ­¥éª¤2ï¼šå¤„ç†ä¸­æ–­ï¼Œè·å–ç”¨æˆ·è¾“å…¥
            print(f"\nğŸ¤– å¤„ç†ä¸­æ–­...")
            
            # æ ¹æ®ä¸­æ–­ç±»å‹è‡ªåŠ¨å“åº”ï¼ˆåœ¨å®é™…åº”ç”¨ä¸­è¿™é‡Œåº”è¯¥æ˜¯ç”¨æˆ·è¾“å…¥ï¼‰
            if isinstance(interrupt_info, list) and len(interrupt_info) > 0:
                interrupt_data = interrupt_info[0] if isinstance(interrupt_info[0], dict) else {}
            elif isinstance(interrupt_info, dict):
                interrupt_data = interrupt_info
            else:
                interrupt_data = {}
            
            interrupt_type = interrupt_data.get("type", "unknown")
            message = interrupt_data.get("message", "éœ€è¦ç”¨æˆ·ç¡®è®¤")
            
            print(f"   ç±»å‹: {interrupt_type}")
            print(f"   æ¶ˆæ¯: {message}")
            
            # è‡ªåŠ¨å“åº”é€»è¾‘
            if "outline" in interrupt_type:
                user_response = "yes"
                print("   ğŸ“‹ è‡ªåŠ¨ç¡®è®¤å¤§çº²")
            elif "knowledge" in interrupt_type or "rag" in interrupt_type:
                user_response = "skip"
                print("   ğŸ§  è‡ªåŠ¨è·³è¿‡RAGå¢å¼º")
            elif "search" in interrupt_type:
                user_response = "yes"
                print("   ğŸ” è‡ªåŠ¨åŒæ„æœç´¢")
            else:
                user_response = "yes"
                print("   â¡ï¸  è‡ªåŠ¨ç»§ç»­")
            
            print(f"   å“åº”: {user_response}")
            
            # ğŸ”¥ æ­¥éª¤3ï¼šä½¿ç”¨Command(resume=...)æ¢å¤æ‰§è¡Œå¹¶ç»§ç»­æµå¼è¾“å‡º
            print(f"\nğŸŒŠ æ¢å¤æ‰§è¡Œå¹¶æ•è·æµå¼è¾“å‡º...")
            print("-" * 30)
            
            step_tokens = 0
            current_input = Command(resume=user_response)
            
            # æ¢å¤æ‰§è¡Œå¹¶è·å–æµå¼è¾“å‡º
            async for chunk in graph.astream(current_input, cast(Any, config), stream_mode="messages"):
                if print_streaming_content(chunk):
                    step_tokens += 1
                    total_tokens += 1
                    
                    if total_tokens % 30 == 0:
                        elapsed = time.time() - start_time
                        print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
            
            print(f"\nâœ… æ­¥éª¤å®Œæˆ ({step_tokens} tokens)")
            
            # ç»§ç»­æ£€æµ‹æ˜¯å¦è¿˜æœ‰æ›´å¤šä¸­æ–­
            continue
        
        total_time = time.time() - start_time
        
        print(f"\n" + "=" * 60)
        print(f"ğŸ‰ æ­£ç¡®çš„æµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•å®Œæˆï¼")
        print(f"ğŸ“Š ç»Ÿè®¡ç»“æœ:")
        print(f"   æ€»æ­¥éª¤æ•°: {step_count}")
        print(f"   æ€»Tokenæ•°: {total_tokens}")
        print(f"   æ€»è€—æ—¶: {total_time:.1f}ç§’")
        
        if total_tokens > 100:
            tokens_per_second = total_tokens / total_time
            print(f"   æµå¼é€Ÿåº¦: {tokens_per_second:.1f} tokens/ç§’")
            print("âœ… å®Œæ•´å·¥ä½œæµç¨‹+æµå¼è¾“å‡ºæµ‹è¯•æˆåŠŸï¼")
            
            # è·å–æœ€ç»ˆç»“æœ
            try:
                final_result = await graph.ainvoke(initial_state, cast(Any, config))
                if final_result.get("article"):
                    article_length = len(final_result["article"])
                    word_count = final_result.get("word_count", 0)
                    print(f"ğŸ“„ æœ€ç»ˆæ–‡ç« : {article_length}å­—ç¬¦ï¼Œ{word_count}å­—")
            except Exception as e:
                print(f"âš ï¸ è·å–æœ€ç»ˆç»“æœå¤±è´¥: {e}")
            
        else:
            print("âŒ æµå¼è¾“å‡ºtokenæ•°é‡åå°‘ï¼Œå¯èƒ½å­˜åœ¨é—®é¢˜")
        
        return {"success": True, "tokens": total_tokens, "time": total_time, "steps": step_count}
        
    except Exception as e:
        print(f"âŒ æ­£ç¡®æµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return {"success": False, "error": str(e)}


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸŒŠ æ­£ç¡®çš„LangGraphæµå¼+ä¸­æ–­å¤„ç†å®Œæ•´æµ‹è¯•")
    print("ğŸ¯ ç›®æ ‡ï¼šéªŒè¯å®˜æ–¹æ¨¡å¼çš„ä¸­æ–­å¤„ç†+å®Œæ•´æµå¼è¾“å‡º")
    
    result = await correct_streaming_workflow()
    
    print(f"\nğŸ’¡ æµ‹è¯•ç»“è®º:")
    print("=" * 60)
    
    if result and result.get("success"):
        tokens = result.get("tokens", 0)
        time_taken = result.get("time", 0)
        steps = result.get("steps", 0)
        
        print("ğŸ‰ æ­£ç¡®çš„æµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•æˆåŠŸ!")
        print(f"ğŸ“Š æ€§èƒ½: {tokens} tokens in {time_taken:.1f}s ({steps} steps)")
        
        if tokens > 100:
            print("ğŸŒŠ ç¡®è®¤: å®Œæ•´å·¥ä½œæµç¨‹æ”¯æŒçœŸæ­£çš„æµå¼+ä¸­æ–­å¤„ç†")
            print("ğŸ’¡ æˆåŠŸå®ç°: å¤§çº²ç”Ÿæˆâ†’ç¡®è®¤â†’RAGâ†’æœç´¢â†’æ–‡ç« ç”Ÿæˆ(æµå¼)")
            print("ğŸ¯ å…³é”®æŠ€æœ¯: interrupt() + Command(resume=...) + stream_mode='messages'")
        else:
            print("âš ï¸ æµå¼æ•ˆæœéœ€è¦è¿›ä¸€æ­¥åˆ†æ")
    else:
        error = result.get('error', 'æœªçŸ¥é”™è¯¯') if result else 'æµ‹è¯•å¤±è´¥'
        print(f"âŒ æµ‹è¯•å¤±è´¥: {error}")
    
    print("\nğŸ¯ æ­£ç¡®æµå¼+ä¸­æ–­å¤„ç†æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())