"""
æ ¸å¿ƒå›¾æ¨¡å—
é«˜çº§äº¤äº’å¼æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆç³»ç»Ÿçš„ä¸»è¦å·¥ä½œæµå›¾
é›†æˆMulti-Agentåä½œã€Human-in-loopäº¤äº’å’Œæµå¼è¾“å‡º
"""

import json
import time
import asyncio
import uuid
import os
from typing import Dict, Any, List

# åŠ è½½ç¯å¢ƒå˜é‡
from dotenv import load_dotenv
load_dotenv()

from langgraph.graph import StateGraph, END, START
from langchain_openai import ChatOpenAI
from langchain_core.messages import AIMessage
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser
from langgraph.types import interrupt
import logging

# å¯¼å…¥æœ¬åœ°æ¨¡å—
from state import (
    DeepResearchState, ReportMode, TaskStatus, InteractionType,
    ReportOutline, ReportSection, ResearchResult,
    update_performance_metrics, 
    update_task_status, add_user_interaction
)

# å¯¼å…¥å­å›¾æ¨¡å—
from subgraphs.intelligent_research.graph import (
    create_intelligent_research_graph
)
# å¯¼å…¥æ ‡å‡†åŒ–æµå¼è¾“å‡ºç³»ç»Ÿ
from writer.core import create_stream_writer, create_workflow_processor

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============================================================================
# LLMé…ç½®
# ============================================================================

def create_llm() -> ChatOpenAI:
    """åˆ›å»ºLLMå®ä¾‹"""
    return ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "qwen2.5-72b-instruct-awq"),
        temperature=float(os.getenv("OPENAI_TEMPERATURE", "0.7")),
        base_url=os.getenv("OPENAI_BASE_URL", "https://llm.3qiao.vip:23436/v1"),
        api_key=os.getenv("OPENAI_API_KEY","sk-0rnrrSH0OsiaWCiv6b37C1E4E60c4b9394325001Ec19A197"),
    )

# ç¼–è¯‘å­å›¾ï¼ˆå…¨å±€å˜é‡ï¼Œé¿å…é‡å¤ç¼–è¯‘ï¼‰- ä½¿ç”¨updateå­å›¾
_intelligent_research_subgraph = None

def get_intelligent_research_subgraph():
    """è·å–æ™ºèƒ½ç ”ç©¶å­å›¾å®ä¾‹"""
    global _intelligent_research_subgraph
    if _intelligent_research_subgraph is None:
        workflow = create_intelligent_research_graph()
        _intelligent_research_subgraph = workflow.compile()
    return _intelligent_research_subgraph

def create_update_subgraph_state(state: DeepResearchState) -> Dict[str, Any]:
    """
    åˆ›å»ºupdateå­å›¾çš„åˆå§‹çŠ¶æ€
    å°†ä¸»å›¾çŠ¶æ€è½¬æ¢ä¸ºupdateå­å›¾æ‰€éœ€çš„çŠ¶æ€æ ¼å¼
    """
    outline = state.get("outline", {})
    sections = outline.get("sections", []) if outline else []

    # è½¬æ¢ç« èŠ‚æ ¼å¼ï¼Œç¡®ä¿æ¯ä¸ªç« èŠ‚éƒ½æœ‰id
    formatted_sections = []
    for i, section in enumerate(sections):
        formatted_section = {
            "id": section.get("id", f"section_{i}"),
            "title": section.get("title", f"ç« èŠ‚ {i+1}"),
            "description": section.get("description", ""),
            "key_points": section.get("key_points", []),
            "research_queries": section.get("research_queries", [])
        }
        formatted_sections.append(formatted_section)

    # åˆ›å»ºupdateå­å›¾çŠ¶æ€
    subgraph_state = {
        "messages": [],
        "user_input": f"è¯·ä¸ºä¸»é¢˜'{state.get('topic', '')}'ç”Ÿæˆæ·±åº¦ç ”ç©¶æŠ¥å‘Š",
        "topic": state.get("topic", ""),
        "sections": formatted_sections,
        "current_section_index": 0,
        "research_results": {},
        "writing_results": {},
        "polishing_results": {},
        "final_report": {},
        "execution_path": [],
        "iteration_count": 0,
        "max_iterations": 10,
        "next_action": "research",
        "task_completed": False,
        "error_log": [],
        "section_attempts": {}
    }

    return subgraph_state

async def call_intelligent_research_subgraph(state: DeepResearchState) -> DeepResearchState:
    """
    è°ƒç”¨æ™ºèƒ½ç ”ç©¶å­å›¾ - ç»Ÿä¸€æµå¼è¾“å‡º

    è¿™ä¸ªå‡½æ•°å®ç°äº†ä¸»å›¾å’Œå­å›¾ä¹‹é—´çš„çŠ¶æ€è½¬æ¢ï¼Œæ™ºèƒ½è¯†åˆ«Agentå·¥ä½œæµç¨‹
    """
    # åˆ›å»ºå·¥ä½œæµç¨‹å¤„ç†å™¨ - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    processor = create_workflow_processor("intelligent_research", "æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")
    
    try:
        processor.writer.step_start("å¼€å§‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆ")
        
        # è·å–å­å›¾å®ä¾‹
        subgraph = get_intelligent_research_subgraph()
        subgraph_input = create_update_subgraph_state(state)
        
        processor.writer.step_progress(
            "å‡†å¤‡ç ”ç©¶è®¡åˆ’", 
            10, 
            sections_count=len(subgraph_input.get("sections", []))
        )
        
        # ç”¨äºæ”¶é›†æœ€ç»ˆç»“æœçš„å˜é‡
        subgraph_output = {}
        updated_sections = []
        new_research_results = []
        
        # æµå¼è°ƒç”¨å­å›¾ï¼Œä½¿ç”¨å¢å¼ºçš„processorç»Ÿä¸€å¤„ç†åµŒå¥—æµå¼è¾“å‡º
        async for chunk in subgraph.astream(subgraph_input, stream_mode=["updates", "messages"], subgraphs=True):
            # ä½¿ç”¨å¢å¼ºçš„processorç»Ÿä¸€å¤„ç†æ‰€æœ‰ç±»å‹çš„chunk
            print("subgraph_input")
            print(chunk)
            print("="*20)
            processor.process_chunk(chunk)
            
            # åŒæ—¶æ”¶é›†å®é™…æ•°æ®ç”¨äºçŠ¶æ€æ›´æ–°ï¼Œå¤„ç†åµŒå¥—ç»“æ„
            if isinstance(chunk, tuple):
                if len(chunk) >= 3:
                    # åµŒå¥—å­å›¾æ ¼å¼: (('subgraph_id',), 'updates', data)
                    _, chunk_type, chunk_data = chunk[0], chunk[1], chunk[2]
                elif len(chunk) >= 2:
                    # æ™®é€šæ ¼å¼: ('updates', data)
                    chunk_type, chunk_data = chunk[0], chunk[1]
                else:
                    continue
                
                if chunk_type == "updates" and isinstance(chunk_data, dict):
                    for node_output in chunk_data.values():
                        if node_output and isinstance(node_output, dict):
                            subgraph_output.update(node_output)
                            
                            # æ”¶é›†ç« èŠ‚æ›´æ–°
                            if "sections" in node_output:
                                sections_data = node_output["sections"]
                                if isinstance(sections_data, list):
                                    for section_data in sections_data:
                                        updated_section = {
                                            "title": section_data.get("title", ""),
                                            "content": section_data.get("content", ""),
                                            "word_count": section_data.get("word_count", 0),
                                            "status": "completed"
                                        }
                                        if not any(s.get("title") == updated_section["title"] for s in updated_sections):
                                            updated_sections.append(updated_section)
                            
                            # æ”¶é›†ç ”ç©¶ç»“æœ
                            if "research_results" in node_output:
                                research_results = node_output["research_results"]
                                if isinstance(research_results, dict):
                                    for research_data in research_results.values():
                                        research_result = ResearchResult(
                                            id=str(uuid.uuid4()),
                                            query=f"ç ”ç©¶ç« èŠ‚: {research_data.get('title', '')}",
                                            source_type="subgraph",
                                            title=research_data.get("title", ""),
                                            content=research_data.get("content", ""),
                                            url="",
                                            relevance_score=0.9,
                                            timestamp=research_data.get("timestamp", time.time()),
                                            section_id=research_data.get("id", "")
                                        )
                                        if not any(r.title == research_result.title for r in new_research_results):
                                            new_research_results.append(research_result)
        # å¤„ç†æœ€ç»ˆç»“æœ
        if subgraph_output.get("final_report"):

            final_report = subgraph_output["final_report"]
            final_sections_data = final_report.get("sections", [])

            # ç¡®ä¿æ‰€æœ‰ç« èŠ‚éƒ½è¢«å¤„ç†
            for section_data in final_sections_data:
                updated_section = {
                    "title": section_data.get("title", ""),
                    "content": section_data.get("content", ""),
                    "word_count": section_data.get("word_count", 0),
                    "status": "completed"
                }
                if not any(s.get("title") == updated_section["title"] for s in updated_sections):
                    updated_sections.append(updated_section)

            # æ›´æ–°çŠ¶æ€
            updated_state = {
                **state,
                "sections": updated_sections,
                "research_results": state.get("research_results", []) + new_research_results,
                "content_creation_completed": True,
                "completed_sections_count": len(updated_sections),
                "performance_metrics": {
                    **state.get("performance_metrics", {}),
                    "sections_completed": len(updated_sections),
                    "total_sections": len(updated_sections),
                    "total_words": final_report.get("total_words", 0)
                }
            }

            processor.writer.final_result(
                f"æ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå®Œæˆ",
                {
                    "sections_count": len(updated_sections),
                    "total_words": final_report.get("total_words", 0),
                    "research_findings": len(new_research_results)
                }
            )
            return updated_state
        else:
            # è¿”å›éƒ¨åˆ†ç»“æœ
            updated_state = {
                **state,
                "sections": updated_sections,
                "research_results": state.get("research_results", []) + new_research_results,
                "content_creation_completed": len(updated_sections) > 0,
                "completed_sections_count": len(updated_sections),
                "performance_metrics": {
                    **state.get("performance_metrics", {}),
                    "sections_completed": len(updated_sections),
                    "total_sections": len(updated_sections),
                    "total_words": sum(section.get("word_count", 0) for section in updated_sections)
                }
            }
            
            processor.writer.step_complete(
                "éƒ¨åˆ†å†…å®¹ç”Ÿæˆå®Œæˆ",
                sections_count=len(updated_sections),
                is_partial=True
            )
            return updated_state

    except Exception as e:
        processor.writer.error(f"ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}", "ResearchGenerationError")
        return state

def convert_research_data_to_results(research_data: List[Dict[str, Any]]) -> List[ResearchResult]:
    """å°†å­å›¾çš„ç ”ç©¶æ•°æ®è½¬æ¢ä¸ºä¸»å›¾çš„ ResearchResult æ ¼å¼"""
    results = []

    for item in research_data:
        try:
            result = ResearchResult(
                id=item.get("id", str(uuid.uuid4())),
                query=item.get("query", ""),
                source_type="web",
                title=item.get("title", ""),
                content=item.get("content", ""),
                url=item.get("url", ""),
                relevance_score=item.get("relevance_score", 0.8),
                timestamp=item.get("timestamp", time.time()),
                section_id=item.get("section_id", "")
            )
            results.append(result)
        except Exception as e:
            logger.warning(f"è½¬æ¢ç ”ç©¶æ•°æ®å¤±è´¥: {e}")
            continue

    return results

async def intelligent_section_processing_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """
    æ™ºèƒ½ç« èŠ‚å¤„ç†èŠ‚ç‚¹ - ä½¿ç”¨updateå­å›¾è¿›è¡Œæ•´ä½“ç ”ç©¶å’Œå†™ä½œ

    è¿™ä¸ªèŠ‚ç‚¹çš„ä½œç”¨ï¼š
    1. è·å–å¤§çº²ä¸­çš„æ‰€æœ‰ç« èŠ‚
    2. è°ƒç”¨updateå­å›¾è¿›è¡Œæ™ºèƒ½ç ”ç©¶å’Œå†™ä½œ
    3. å­å›¾å†…éƒ¨å¤„ç†ï¼šæ™ºèƒ½Supervisor â†’ ç ”ç©¶ â†’ å†™ä½œ â†’ æ•´åˆ
    4. è¿”å›å®Œæ•´çš„ç ”ç©¶æŠ¥å‘Š
    """
    # ä½¿ç”¨æ‰å¹³åŒ–Writer - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    writer = create_stream_writer("intelligent_section_processing", "æ™ºèƒ½ç« èŠ‚å¤„ç†")
    writer.step_start("å¼€å§‹æ™ºèƒ½ç ”ç©¶å¤„ç†ï¼ˆä½¿ç”¨updateå­å›¾ï¼‰")

    try:
        outline = state.get("outline", {})
        sections = outline.get("sections", []) if outline else []

        if not sections:
            writer.error("æ²¡æœ‰å¯ç”¨çš„ç« èŠ‚ä¿¡æ¯", "NoSectionsError")
            return state

        writer.step_progress(
            f"å‡†å¤‡ä½¿ç”¨æ™ºèƒ½Supervisorå¤„ç† {len(sections)} ä¸ªç« èŠ‚",
            10,
            total_sections=len(sections)
        )

        # è°ƒç”¨updateå­å›¾è¿›è¡Œæ•´ä½“å¤„ç†
        writer.step_progress("å¯åŠ¨æ™ºèƒ½ç ”ç©¶å­å›¾", 20)

        # ç›´æ¥è°ƒç”¨å­å›¾ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£çš„æ–¹å¼ï¼‰
        updated_state = await call_intelligent_research_subgraph(state)

        # æ£€æŸ¥å¤„ç†ç»“æœ
        if updated_state.get("content_creation_completed"):
            completed_sections = updated_state.get("sections", [])
            total_words = updated_state.get("performance_metrics", {}).get("total_words", 0)

            writer.step_complete(
                "æ™ºèƒ½ç ”ç©¶å®Œæˆ",
                completed_sections=len(completed_sections),
                total_sections=len(sections),
                total_words=total_words
            )

            logger.info(f"æ™ºèƒ½ç ”ç©¶å®Œæˆ: {len(completed_sections)} ä¸ªç« èŠ‚, æ€»å­—æ•°: {total_words}")
            return updated_state
        else:
            writer.step_progress("å­å›¾å¤„ç†æœªå®Œæˆï¼Œè¿”å›åŸçŠ¶æ€", 50)
            logger.warning("å­å›¾å¤„ç†æœªå®Œæˆ")
            return state

    except Exception as e:
        logger.error(f"æ™ºèƒ½ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}")
        writer.error(f"ç« èŠ‚å¤„ç†å¤±è´¥: {str(e)}", "IntelligentSectionProcessingError")

        state["error_log"] = state.get("error_log", []) + [f"æ™ºèƒ½ç« èŠ‚å¤„ç†é”™è¯¯: {str(e)}"]
        return state

# ============================================================================
# å¤§çº²ç”ŸæˆèŠ‚ç‚¹
# ============================================================================

async def outline_generation_node(state: DeepResearchState, config=None) -> DeepResearchState:
    """å¤§çº²ç”ŸæˆèŠ‚ç‚¹"""
    # ä½¿ç”¨æ‰å¹³åŒ–å¤„ç†å™¨ - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
    processor = create_workflow_processor("outline_generation", "å¤§çº²ç”Ÿæˆå™¨")
    processor.writer.step_start("å¼€å§‹ç”Ÿæˆæ·±åº¦ç ”ç©¶å¤§çº²")
    
    try:
        start_time = time.time()
        llm = create_llm()
        parser = JsonOutputParser(pydantic_object=ReportOutline)
        
        # æ„å»ºé«˜çº§å¤§çº²ç”Ÿæˆæç¤º
        outline_prompt = ChatPromptTemplate.from_messages([
            ("system", """ä½ æ˜¯ä¸“ä¸šçš„æŠ¥å‘Šå¤§çº²è®¾è®¡ä¸“å®¶ã€‚è¯·ç”Ÿæˆè¯¦ç»†çš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ã€‚
            
            è¦æ±‚ï¼š
            1. å¤§çº²ç¬¦åˆ{report_type}æŠ¥å‘Šçš„æ ‡å‡†ç»“æ„
            2. é’ˆå¯¹{target_audience}è¯»è€…ç¾¤ä½“è®¾è®¡
            3. ç ”ç©¶æ·±åº¦ä¸º{depth_level}çº§åˆ«
            4. ç›®æ ‡å­—æ•°çº¦{target_length}å­—
            5. æ¯ä¸ªç« èŠ‚åŒ…å«ç ”ç©¶æŸ¥è¯¢å…³é”®è¯
            6. ç« èŠ‚ä¼˜å…ˆçº§åˆç†åˆ†é…
            
            {format_instructions}"""),
            ("human", """
            è¯·ä¸ºä»¥ä¸‹ä¸»é¢˜ç”Ÿæˆä¸“ä¸šçš„æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š
            
            ç ”ç©¶ä¸»é¢˜ï¼š{topic}
            æŠ¥å‘Šç±»å‹ï¼š{report_type}
            ç›®æ ‡è¯»è€…ï¼š{target_audience}
            æ·±åº¦çº§åˆ«ï¼š{depth_level}
            
            è¯·ç¡®ä¿å¤§çº²ç»“æ„å®Œæ•´ã€é€»è¾‘æ¸…æ™°ã€ä¾¿äºæ·±åº¦ç ”ç©¶ã€‚
            """)
        ])
        
        input_data = {
            "topic": state["topic"],
            "report_type": state["report_type"],
            "target_audience": state["target_audience"],
            "depth_level": state["depth_level"],
            "target_length": state["target_length"],
            "format_instructions": parser.get_format_instructions()
        }
        
        processor.writer.step_progress("æ­£åœ¨ç”Ÿæˆä¸“ä¸šå¤§çº²...", 30)
        
        # åˆ›å»ºLLMé“¾å¹¶æµå¼æ‰§è¡Œ
        llm_chain = outline_prompt | llm | parser
        
        outline_data = None
        chunk_count = 0
        current_outline_display = ""
        
        async for chunk in llm_chain.astream(input_data, config=config):
            outline_data = chunk
            chunk_count += 1
            # å®æ—¶æ˜¾ç¤ºå¤§çº²å†…å®¹ï¼ˆæ¯5ä¸ªchunkæ›´æ–°ä¸€æ¬¡ä»¥å‡å°‘é¢‘ç‡ï¼‰
            if chunk_count % 5 == 0:
                # æ„å»ºå½“å‰å¤§çº²çš„æ˜¾ç¤ºæ–‡æœ¬
                if hasattr(chunk, 'title'):
                    current_outline_display = f"ğŸ¯ æ ‡é¢˜ï¼š{chunk.title}\n"
                if hasattr(chunk, 'sections') and chunk.sections:
                    current_outline_display += f"ğŸ“š ç« èŠ‚({len(chunk.sections)}ä¸ª):\n"
                    for i, section in enumerate(chunk.sections[:3], 1):  # åªæ˜¾ç¤ºå‰3ä¸ªç« èŠ‚
                        if hasattr(section, 'title'):
                            current_outline_display += f"  {i}. {section.title}\n"
                            if hasattr(section, 'description'):
                                current_outline_display += f"     {section.description}\n"
                    if len(chunk.sections) > 3:
                        current_outline_display += f"  ... è¿˜æœ‰{len(chunk.sections)-3}ä¸ªç« èŠ‚"
                
                processor.writer.step_progress(
                    str(chunk),
                    min(90, 30 + (chunk_count // 5) * 10),
                    current_outline=current_outline_display,
                    chunk_count=chunk_count
                )
                
                # å¦‚æœå¤§çº²åŸºæœ¬å®Œæ•´ï¼Œæå‰æ˜¾ç¤º
                if hasattr(chunk, 'title') and hasattr(chunk, 'sections') and len(chunk.sections) >= 3:
                    processor.writer.step_progress(
                        "å¤§çº²ç»“æ„å·²ç”Ÿæˆï¼Œæ­£åœ¨å®Œå–„ç»†èŠ‚...",
                        85,
                        partial_outline=chunk,
                        streaming_content=current_outline_display
                    )
        
        # å¤„ç†ç”Ÿæˆç»“æœ
        if not outline_data:
            # åˆ›å»ºé»˜è®¤å¤§çº²
            outline_data = ReportOutline(
                title=f"{state['topic']} - æ·±åº¦ç ”ç©¶æŠ¥å‘Š",
                executive_summary=f"æœ¬æŠ¥å‘Šå¯¹{state['topic']}è¿›è¡Œå…¨é¢æ·±å…¥çš„ç ”ç©¶åˆ†æï¼Œä¸º{state['target_audience']}æä¾›ä¸“ä¸šæ´å¯Ÿã€‚",
                sections=[
                    ReportSection(
                        id="background",
                        title="ç ”ç©¶èƒŒæ™¯ä¸ç°çŠ¶",
                        description="åˆ†æç ”ç©¶èƒŒæ™¯ã€å‘å±•å†ç¨‹å’Œå½“å‰çŠ¶æ€",
                        key_points=["å†å²å‘å±•", "ç°çŠ¶åˆ†æ", "å…³é”®ç‰¹å¾"],
                        research_queries=[f"{state['topic']} å‘å±•å†å²", f"{state['topic']} ç°çŠ¶åˆ†æ", f"{state['topic']} å¸‚åœºè§„æ¨¡"],
                        priority=5
                    ),
                    ReportSection(
                        id="deep_analysis",
                        title="æ·±åº¦åˆ†æä¸æ´å¯Ÿ",
                        description="è¿›è¡Œæ·±å…¥åˆ†æï¼Œè¯†åˆ«å…³é”®è¶‹åŠ¿å’Œæ¨¡å¼",
                        key_points=["æ ¸å¿ƒé©±åŠ¨å› ç´ ", "å‘å±•è¶‹åŠ¿", "å½±å“å› ç´ "],
                        research_queries=[f"{state['topic']} è¶‹åŠ¿åˆ†æ", f"{state['topic']} å½±å“å› ç´ ", f"{state['topic']} å‘å±•é¢„æµ‹"],
                        priority=5
                    ),
                    ReportSection(
                        id="case_studies",
                        title="æ¡ˆä¾‹ç ”ç©¶ä¸åº”ç”¨",
                        description="åˆ†æå…¸å‹æ¡ˆä¾‹å’Œå®é™…åº”ç”¨æƒ…å†µ",
                        key_points=["æˆåŠŸæ¡ˆä¾‹", "åº”ç”¨åœºæ™¯", "å®æ–½ç»éªŒ"],
                        research_queries=[f"{state['topic']} æ¡ˆä¾‹ç ”ç©¶", f"{state['topic']} åº”ç”¨å®ä¾‹", f"{state['topic']} æœ€ä½³å®è·µ"],
                        priority=4
                    ),
                    ReportSection(
                        id="challenges_opportunities",
                        title="æŒ‘æˆ˜ä¸æœºé‡åˆ†æ",
                        description="è¯†åˆ«é¢ä¸´çš„æŒ‘æˆ˜å’Œå‘å±•æœºé‡",
                        key_points=["ä¸»è¦æŒ‘æˆ˜", "å‘å±•æœºé‡", "é£é™©è¯„ä¼°"],
                        research_queries=[f"{state['topic']} æŒ‘æˆ˜åˆ†æ", f"{state['topic']} å‘å±•æœºä¼š", f"{state['topic']} é£é™©è¯„ä¼°"],
                        priority=4
                    ),
                    ReportSection(
                        id="future_outlook",
                        title="æœªæ¥å±•æœ›ä¸å»ºè®®",
                        description="é¢„æµ‹æœªæ¥å‘å±•å¹¶æå‡ºä¸“ä¸šå»ºè®®",
                        key_points=["å‘å±•é¢„æµ‹", "æˆ˜ç•¥å»ºè®®", "è¡ŒåŠ¨è®¡åˆ’"],
                        research_queries=[f"{state['topic']} æœªæ¥å‘å±•", f"{state['topic']} å‘å±•å»ºè®®", f"{state['topic']} æˆ˜ç•¥è§„åˆ’"],
                        priority=3
                    )
                ],
                methodology="é‡‡ç”¨æ–‡çŒ®ç ”ç©¶ã€æ¡ˆä¾‹åˆ†æã€è¶‹åŠ¿é¢„æµ‹å’Œä¸“å®¶æ´å¯Ÿç›¸ç»“åˆçš„ç»¼åˆç ”ç©¶æ–¹æ³•",
                target_audience=state["target_audience"],
                estimated_length=state["target_length"]
            )
        
        # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
        if hasattr(outline_data, 'dict'):
            outline_dict = outline_data.dict()
        else:
            outline_dict = dict(outline_data) if outline_data else {}
        
        # æ›´æ–°çŠ¶æ€
        execution_time = time.time() - start_time
        update_performance_metrics(state, "outline_generator", execution_time)
        update_task_status(state, "outline_generation", TaskStatus.COMPLETED)
        
        state["outline"] = outline_dict
        state["current_step"] = "outline_generated"
        state["execution_path"] = state["execution_path"] + ["outline_generation"]
        
        # åˆ›å»ºå¤§çº²å±•ç¤ºæ¶ˆæ¯
        sections_text = "\n".join([
            f"  {i+1}. {section['title']}\n     {section['description']}\n     å…³é”®è¯: {', '.join(section['research_queries'][:3])}"
            for i, section in enumerate(outline_dict.get("sections", []))
        ])
        
        outline_message = f"""
        ğŸ“‹ æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆï¼š
        
        ğŸ¯ æŠ¥å‘Šæ ‡é¢˜ï¼š{outline_dict.get('title', 'æœªçŸ¥')}
        
        ğŸ“ æ‰§è¡Œæ‘˜è¦ï¼š
        {outline_dict.get('executive_summary', 'æ— æ‘˜è¦')}
        
        ğŸ“š ç ”ç©¶ç« èŠ‚ï¼š
        {sections_text}
        
        ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline_dict.get('methodology', 'æœªæŒ‡å®š')}
        ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline_dict.get('estimated_length', 0):,}å­—
        â±ï¸ ç”Ÿæˆæ—¶é—´ï¼š{execution_time:.2f}ç§’
        """
        
        state["messages"] = state["messages"] + [AIMessage(content=outline_message)]
        
        processor.writer.step_complete(
            "æ·±åº¦ç ”ç©¶å¤§çº²ç”Ÿæˆå®Œæˆ",
            sections_count=len(outline_dict.get("sections", [])),
            execution_time=execution_time,
            outline_data=outline_dict,
            display_text=outline_message
        )
        
        logger.info(f"å¤§çº²ç”Ÿæˆå®Œæˆ: {len(outline_dict.get('sections', []))}ä¸ªç« èŠ‚")
        return state
        
    except Exception as e:
        logger.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}")
        processor.writer.error(f"å¤§çº²ç”Ÿæˆå¤±è´¥: {str(e)}", "OutlineGenerationError")
        
        state["error_log"] = state["error_log"] + [f"å¤§çº²ç”Ÿæˆé”™è¯¯: {str(e)}"]
        state["current_step"] = "outline_generation_failed"
        update_task_status(state, "outline_generation", TaskStatus.FAILED)
        return state

# ============================================================================
# äº¤äº’ç¡®è®¤èŠ‚ç‚¹
# ============================================================================

def create_interaction_node(interaction_type: InteractionType):
    """åˆ›å»ºäº¤äº’ç¡®è®¤èŠ‚ç‚¹çš„å·¥å‚å‡½æ•°"""
    
    def interaction_node(state: DeepResearchState) -> DeepResearchState:
        """é€šç”¨äº¤äº’ç¡®è®¤èŠ‚ç‚¹"""
        # äº¤äº’èŠ‚ç‚¹ - ä¸ä½¿ç”¨æ¨¡æ¿ï¼Œä¿æŒç®€æ´
        processor = create_workflow_processor(f"interaction_{interaction_type.value}", f"{interaction_type.value}_äº¤äº’")
        
        interaction_config = get_interaction_config(interaction_type)
        mode = state["mode"]
        
        processor.writer.step_start(f"å¤„ç†{interaction_config['title']}")
        processor.writer.step_progress(f"å¤„ç†{interaction_config['title']}", 0, 
                                      interaction_type=interaction_type.value,
                                      mode=mode.value)
        
        # Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡
        if mode == ReportMode.COPILOT:
            state["approval_status"][interaction_type.value] = True
            state["user_feedback"][interaction_type.value] = {"approved": True, "auto": True}
            
            processor.writer.step_complete(f"Copilotæ¨¡å¼è‡ªåŠ¨é€šè¿‡", auto_approved=True)
            
            state["messages"] = state["messages"] + [
                AIMessage(content=f"ğŸ¤– Copilotæ¨¡å¼ï¼š{interaction_config['copilot_message']}")
            ]
            
            return state
        
        # äº¤äº’æ¨¡å¼éœ€è¦ç”¨æˆ·ç¡®è®¤
        message_content = format_interaction_message(state, interaction_type, interaction_config)
        
        processor.writer.step_progress("ç­‰å¾…ç”¨æˆ·ç¡®è®¤", 50, awaiting_user_input=True)
        
        # ä½¿ç”¨interruptç­‰å¾…ç”¨æˆ·è¾“å…¥
        user_response = interrupt({
            "type": interaction_type.value,
            "title": interaction_config["title"],
            "message": message_content,
            "options": interaction_config["options"],
            "default": interaction_config.get("default", "continue")
        })
        
        # å¤„ç†ç”¨æˆ·å“åº”
        approved = user_response.get("approved", True) if isinstance(user_response, dict) else True
        state["approval_status"][interaction_type.value] = approved
        state["user_feedback"][interaction_type.value] = user_response
        
        # è®°å½•äº¤äº’å†å²
        add_user_interaction(state, interaction_type.value, user_response)
        
        processor.writer.step_complete(
            "ç”¨æˆ·ç¡®è®¤å®Œæˆ",
            user_response=user_response,
            approved=approved
        )
        
        # æ·»åŠ ç¡®è®¤æ¶ˆæ¯
        status_emoji = "âœ…" if approved else "âŒ"
        confirmation_message = f"{status_emoji} {interaction_config['title']}ï¼š{'ç¡®è®¤é€šè¿‡' if approved else 'è¢«æ‹’ç»'}"
        
        state["messages"] = state["messages"] + [AIMessage(content=confirmation_message)]
        
        return state
    
    return interaction_node

def get_interaction_config(interaction_type: InteractionType) -> Dict[str, Any]:
    """è·å–äº¤äº’é…ç½®"""
    configs = {
        InteractionType.OUTLINE_CONFIRMATION: {
            "title": "å¤§çº²ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤æŠ¥å‘Šå¤§çº²ï¼Œç»§ç»­æ‰§è¡Œç ”ç©¶",
            "options": ["ç¡®è®¤ç»§ç»­", "ä¿®æ”¹å¤§çº²", "é‡æ–°ç”Ÿæˆ"],
            "default": "ç¡®è®¤ç»§ç»­"
        },
        InteractionType.RESEARCH_PERMISSION: {
            "title": "ç ”ç©¶æƒé™ç¡®è®¤",
            "copilot_message": "è‡ªåŠ¨å…è®¸è¿›è¡Œæ·±åº¦ç ”ç©¶å’Œæ•°æ®æ”¶é›†",
            "options": ["å…è®¸ç ”ç©¶", "è·³è¿‡ç ”ç©¶", "é™åˆ¶èŒƒå›´"],
            "default": "å…è®¸ç ”ç©¶"
        },
        InteractionType.ANALYSIS_APPROVAL: {
            "title": "åˆ†æç»“æœå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨ç¡®è®¤åˆ†æç»“æœï¼Œç»§ç»­å†…å®¹ç”Ÿæˆ",
            "options": ["ç¡®è®¤åˆ†æ", "é‡æ–°åˆ†æ", "è°ƒæ•´æ–¹å‘"],
            "default": "ç¡®è®¤åˆ†æ"
        },
        InteractionType.CONTENT_REVIEW: {
            "title": "å†…å®¹å®¡æŸ¥",
            "copilot_message": "è‡ªåŠ¨é€šè¿‡å†…å®¹å®¡æŸ¥ï¼Œå‡†å¤‡æœ€ç»ˆæŠ¥å‘Š",
            "options": ["é€šè¿‡å®¡æŸ¥", "ä¿®æ”¹å†…å®¹", "é‡å†™ç« èŠ‚"],
            "default": "é€šè¿‡å®¡æŸ¥"
        },
        InteractionType.FINAL_APPROVAL: {
            "title": "æœ€ç»ˆå®¡æ‰¹",
            "copilot_message": "è‡ªåŠ¨å®Œæˆæœ€ç»ˆå®¡æ‰¹ï¼ŒæŠ¥å‘Šç”Ÿæˆå®Œæˆ",
            "options": ["æœ€ç»ˆç¡®è®¤", "å†æ¬¡ä¿®æ”¹", "ç”Ÿæˆæ–°ç‰ˆæœ¬"],
            "default": "æœ€ç»ˆç¡®è®¤"
        }
    }
    return configs.get(interaction_type, {})

def format_interaction_message(state: DeepResearchState, interaction_type: InteractionType, config: Dict[str, Any]) -> str:
    """æ ¼å¼åŒ–äº¤äº’æ¶ˆæ¯"""
    if interaction_type == InteractionType.OUTLINE_CONFIRMATION:
        outline = state.get("outline", {})
        sections_text = "\n".join([
            f"  {i+1}. {section.get('title', 'æœªçŸ¥ç« èŠ‚')}\n     {section.get('description', 'æ— æè¿°')}"
            for i, section in enumerate(outline.get("sections", []))
        ])
        return f"""
            è¯·ç¡®è®¤ä»¥ä¸‹æ·±åº¦ç ”ç©¶æŠ¥å‘Šå¤§çº²ï¼š

            ğŸ“‹ æ ‡é¢˜ï¼š{outline.get('title', 'æœªçŸ¥æ ‡é¢˜')}

            ğŸ“ æ‘˜è¦ï¼š{outline.get('executive_summary', 'æ— æ‘˜è¦')}

            ğŸ“š ç« èŠ‚ç»“æ„ï¼š
            {sections_text}

            ğŸ” ç ”ç©¶æ–¹æ³•ï¼š{outline.get('methodology', 'æœªæŒ‡å®š')}
            ğŸ“Š é¢„ä¼°å­—æ•°ï¼š{outline.get('estimated_length', 0):,}å­—
            ğŸ‘¥ ç›®æ ‡è¯»è€…ï¼š{outline.get('target_audience', 'æœªçŸ¥')}
        """
    elif interaction_type == InteractionType.RESEARCH_PERMISSION:
        return f"""
            æ˜¯å¦å…è®¸å¯¹ä¸»é¢˜ã€Œ{state['topic']}ã€è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Ÿ

            ç ”ç©¶å°†åŒ…æ‹¬ï¼š
            - å¤šæºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ”¶é›†
            - ç›¸å…³æ¡ˆä¾‹å’Œæ•°æ®åˆ†æ
            - è¶‹åŠ¿è¯†åˆ«å’Œæ¨¡å¼å‘ç°
            - ä¸“ä¸šæ´å¯Ÿç”Ÿæˆ

            é¢„è®¡ç ”ç©¶æ—¶é—´ï¼š5-10åˆ†é’Ÿ
        """
    else:
        return f"è¯·ç¡®è®¤{config['title']}ç›¸å…³è®¾ç½®"

# åˆ›å»ºäº¤äº’èŠ‚ç‚¹å®ä¾‹
outline_confirmation_node = create_interaction_node(InteractionType.OUTLINE_CONFIRMATION)

# ============================================================================
# å›¾æ„å»ºå’Œè·¯ç”±é€»è¾‘
# ============================================================================


# åˆ é™¤äº†analysis_generation_node - ç”±updateå­å›¾å¤„ç†åˆ†æåŠŸèƒ½

# ============================================================================
# è·¯ç”±å‡½æ•° - ç®€åŒ–ç‰ˆæœ¬
# ============================================================================

def route_after_outline_confirmation(state: DeepResearchState) -> str:
    """å¤§çº²ç¡®è®¤åçš„è·¯ç”± - ç®€åŒ–ç‰ˆæœ¬"""
    if not state["approval_status"].get("outline_confirmation", True):
        return "outline_generation"  # é‡æ–°ç”Ÿæˆå¤§çº²
    return "content_creation"  # ç›´æ¥è¿›å…¥å†…å®¹åˆ›å»ºï¼ˆé›†æˆäº†å­å›¾ï¼‰

# ============================================================================
# å›¾æ„å»ºå‡½æ•°
# ============================================================================

def create_deep_research_graph():
    """åˆ›å»ºæ·±åº¦ç ”ç©¶æŠ¥å‘Šç”Ÿæˆå›¾ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾"""
    workflow = StateGraph(DeepResearchState)

    # æ·»åŠ ç®€åŒ–çš„æ ¸å¿ƒèŠ‚ç‚¹ - é›†æˆæ™ºèƒ½ç« èŠ‚ç ”ç©¶å­å›¾
    workflow.add_node("outline_generation", outline_generation_node)
    workflow.add_node("outline_confirmation", outline_confirmation_node)
    # ç›´æ¥ä½¿ç”¨å­å›¾è°ƒç”¨å‡½æ•°ä½œä¸ºèŠ‚ç‚¹ï¼ˆæŒ‰ç…§å®˜æ–¹æ–‡æ¡£æ–¹å¼ï¼‰
    workflow.add_node("content_creation", call_intelligent_research_subgraph)
    
    # è®¾ç½®ç®€åŒ–çš„æµç¨‹ï¼šå¤§çº²ç”Ÿæˆ â†’ å¤§çº²ç¡®è®¤ â†’ å†…å®¹åˆ›å»º
    workflow.add_edge(START, "outline_generation")
    workflow.add_edge("outline_generation", "outline_confirmation")

    # å¤§çº²ç¡®è®¤åçš„æ¡ä»¶è·¯ç”±
    workflow.add_conditional_edges(
        "outline_confirmation",
        route_after_outline_confirmation,
        {
            "outline_generation": "outline_generation",
            "content_creation": "content_creation"
        }
    )
    
    workflow.add_edge("content_creation", END)
    
    return workflow
