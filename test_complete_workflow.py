#!/usr/bin/env python3
"""
å®Œæ•´çš„å†™ä½œåŠ©æ‰‹å·¥ä½œæµç¨‹æµ‹è¯•è„šæœ¬ - çœŸæ­£çš„å®Œæ•´æµå¼ç‰ˆæœ¬
æµ‹è¯•å®Œæ•´å·¥ä½œæµï¼šå¤§çº²ç”Ÿæˆ â†’ ç¡®è®¤ â†’ RAGå¢å¼º â†’ æœç´¢ â†’ æ–‡ç« ç”Ÿæˆ
ç¡®ä¿æ¯ä¸ªæ­¥éª¤éƒ½æœ‰æµå¼è¾“å‡ºå’Œæ­£ç¡®çš„ä¸­æ–­å¤„ç†
"""

import asyncio
import time
from typing import Dict, Any, cast
from graph import create_writing_assistant_graph, initialize_writing_state
from tools import load_knowledge_bases
from langgraph.types import Command
from langgraph.checkpoint.memory import MemorySaver


def print_streaming_content(chunk: Any, step_name: str = ""):
    """
    ä¼˜åŒ–çš„æµå¼å†…å®¹æ‰“å°å‡½æ•° - æ˜¾ç¤ºçœŸæ­£çš„tokenæµå¼æ•ˆæœ
    """
    # ğŸ¯ æ­£ç¡®å¤„ç†tupleæ ¼å¼çš„chunk
    if isinstance(chunk, tuple) and len(chunk) > 0:
        message_chunk = chunk[0]  # è·å–AIMessageChunk
        
        # æ£€æŸ¥æ˜¯å¦æœ‰content
        if hasattr(message_chunk, 'content'):
            content = message_chunk.content
            if content and isinstance(content, str):
                # ğŸ”¤ å®æ—¶æ‰“å°æ¯ä¸ªtoken - æ‰“å­—æœºæ•ˆæœï¼
                print(content, end="", flush=True)
                return True
    return False


async def complete_streaming_workflow():
    """
    å®Œæ•´çš„æµå¼å·¥ä½œæµç¨‹ - ç¡®ä¿èµ°å®Œæ‰€æœ‰æ­¥éª¤
    å¤§çº²ç”Ÿæˆ â†’ ç¡®è®¤ â†’ RAGå¢å¼º â†’ æœç´¢ â†’ æ–‡ç« ç”Ÿæˆ
    """
    print("\nğŸš€ å®Œæ•´æµå¼å†™ä½œåŠ©æ‰‹å·¥ä½œæµç¨‹æµ‹è¯•")
    print("ğŸ¯ ç›®æ ‡ï¼šç¡®ä¿å®Œæ•´æµç¨‹ + çœŸæ­£æµå¼è¾“å‡º")
    print("=" * 60)
    
    # åˆå§‹åŒ–
    try:
        load_knowledge_bases()
        print("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ")
    except Exception as e:
        print(f"âš ï¸ çŸ¥è¯†åº“åˆå§‹åŒ–è­¦å‘Š: {e}")
    
    graph = create_writing_assistant_graph()
    print("âœ… å†™ä½œåŠ©æ‰‹å›¾åˆ›å»ºæˆåŠŸ")
    
    # éªŒè¯checkpointer
    if hasattr(graph, 'checkpointer') and graph.checkpointer:
        print("âœ… Checkpointeré…ç½®æ­£ç¡®")
    else:
        print("âš ï¸ è­¦å‘Šï¼šCheckpointerå¯èƒ½æœªæ­£ç¡®é…ç½®")
    
    # å®Œæ•´å·¥ä½œæµç¨‹é…ç½®
    initial_state = initialize_writing_state(
        topic="Pythonå¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ",
        user_id="complete_streaming_test",
        max_words=600,
        style="technical",
        language="zh",
        enable_search=True  # ğŸ”¥ å¯ç”¨å®Œæ•´åŠŸèƒ½
    )
    
    config: Dict[str, Any] = {"configurable": {"thread_id": "complete_streaming_001"}}
    
    start_time = time.time()
    total_tokens = 0
    step_count = 0
    
    try:
        current_input = initial_state
        
        # å®Œæ•´å·¥ä½œæµç¨‹å¾ªç¯
        while step_count < 15:  # å¢åŠ æ­¥éª¤é™åˆ¶ï¼Œç¡®ä¿å®Œæ•´æµç¨‹
            step_count += 1
            print(f"\nğŸ“ æ­¥éª¤ {step_count}: æ‰§è¡Œå·¥ä½œæµç¨‹")
            print("-" * 50)
            
            # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨astreamæ£€æµ‹ä¸­æ–­
            interrupted = False
            result = None
            
            try:
                # ä½¿ç”¨astream(updates)æ¨¡å¼æ£€æµ‹ä¸­æ–­
                async for chunk in graph.astream(current_input, cast(Any, config), stream_mode=""):
                    print(f"   ğŸ“¦ æ”¶åˆ°æ›´æ–°: {list(chunk.keys())}")
                    print(chunk)
                    # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­æ ‡è®°
                    if "__interrupt__" in chunk:
                        interrupted = True
                        print(f"â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­")
                        break
                    else:
                        # ä¿å­˜æœ€åçš„ç»“æœç”¨äºçŠ¶æ€æ›´æ–°
                        for node_name, node_result in chunk.items():
                            if isinstance(node_result, dict):
                                result = node_result
                
                if interrupted:
                    # è·å–å½“å‰çŠ¶æ€ç”¨äºç¡®å®šä¸­æ–­ç±»å‹
                    if result:
                        current_step = result.get("current_step", "unknown")
                        print(f"   å½“å‰æ­¥éª¤: {current_step}")
                    else:
                        current_step = "unknown"
                    
                    # ğŸ¤– è‡ªåŠ¨å¤„ç†ä¸­æ–­
                    if "outline" in current_step or "confirmation" in current_step:
                        user_response = "yes"
                        print("   ğŸ“‹ è‡ªåŠ¨ç¡®è®¤å¤§çº²")
                    elif "rag" in current_step or "knowledge" in current_step:
                        user_response = "skip"  # è·³è¿‡RAGå¢å¼ºä»¥ç®€åŒ–æµç¨‹
                        print("   ğŸ§  è·³è¿‡RAGå¢å¼º")
                    elif "search" in current_step:
                        user_response = "yes"
                        print("   ğŸ” è‡ªåŠ¨åŒæ„æœç´¢")
                    else:
                        user_response = "yes"
                        print(f"   â¡ï¸  è‡ªåŠ¨ç»§ç»­ ({current_step})")
                    
                    print(f"   å“åº”: {user_response}")
                    
                    # ğŸŒŠ æ¢å¤æ‰§è¡Œå¹¶è·å–æµå¼è¾“å‡º
                    print(f"\nğŸŒŠ æ¢å¤æ‰§è¡Œï¼Œå¼€å§‹æµå¼è¾“å‡º...")
                    print("-" * 30)
                    
                    step_tokens = 0
                    async for chunk in graph.astream(Command(resume=user_response), cast(Any, config), stream_mode="messages"):
                        if print_streaming_content(chunk):
                            step_tokens += 1
                            total_tokens += 1
                            
                            # æ¯30ä¸ªtokenæ˜¾ç¤ºè¿›åº¦
                            if total_tokens % 30 == 0:
                                elapsed = time.time() - start_time
                                print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
                    
                    print(f"\nâœ… æ­¥éª¤å®Œæˆ ({step_tokens} tokens)")
                    
                    # å‡†å¤‡ä¸‹ä¸€æ­¥
                    current_input = Command(resume=user_response)
                    
                else:
                    # æ²¡æœ‰ä¸­æ–­ï¼Œå·¥ä½œæµç¨‹å¯èƒ½å®Œæˆ
                    print("ğŸ‰ å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
                    
                    # æœ€ç»ˆæµå¼è¾“å‡ºæ£€æŸ¥
                    print(f"\nğŸŒŠ æ£€æŸ¥æœ€ç»ˆæµå¼è¾“å‡º...")
                    print("-" * 30)
                    
                    final_tokens = 0
                    async for chunk in graph.astream(current_input, cast(Any, config), stream_mode="messages"):
                        if print_streaming_content(chunk):
                            final_tokens += 1
                            total_tokens += 1
                            
                            if total_tokens % 30 == 0:
                                elapsed = time.time() - start_time
                                print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
                    
                    if final_tokens > 0:
                        print(f"\nâœ… æœ€ç»ˆè¾“å‡ºå®Œæˆ ({final_tokens} tokens)")
                    
                    print(f"\nğŸ å®Œæ•´æµç¨‹ç»“æŸï¼æ€»è®¡ {total_tokens} tokens")
                    break
                    
            except Exception as e:
                if "interrupt" in str(e).lower():
                    print(f"â¸ï¸  é€šè¿‡å¼‚å¸¸æ£€æµ‹åˆ°ä¸­æ–­: {str(e)[:100]}")
                    interrupted = True
                    # åœ¨è¿™ç§æƒ…å†µä¸‹ä¹Ÿéœ€è¦å¤„ç†ä¸­æ–­
                    user_response = "yes"
                    current_input = Command(resume=user_response)
                    continue
                else:
                    print(f"âŒ æ­¥éª¤ {step_count} å¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    break
        
        total_time = time.time() - start_time
        
        # è·å–æœ€ç»ˆç»“æœ
        try:
            final_result = await graph.ainvoke(initial_state, cast(Any, config))
        except:
            final_result = None
        
        return {
            "success": True,
            "total_time": total_time,
            "total_tokens": total_tokens,
            "steps": step_count,
            "final_result": final_result
        }
        
    except Exception as e:
        print(f"âŒ æµå¼å·¥ä½œæµç¨‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None


async def handle_workflow_interrupts(graph, config: Dict[str, Any]):
    """
    å¤„ç†å·¥ä½œæµç¨‹çš„ä¸­æ–­å’Œæµå¼è¾“å‡º
    """
    # åˆå§‹åŒ–çŠ¶æ€
    initial_state = initialize_writing_state(
        topic="Pythonå¼‚æ­¥ç¼–ç¨‹æœ€ä½³å®è·µ",
        user_id="workflow_test",
        max_words=600,
        style="technical",
        language="zh",
        enable_search=True
    )
    
    print(f"ğŸ“ ä¸»é¢˜: {initial_state['topic']}")
    print(f"ğŸ¯ å­—æ•°: {initial_state['max_words']}")
    print("=" * 60)
    
    current_input = initial_state
    step_count = 0
    total_tokens = 0
    start_time = time.time()
    
    while step_count < 15:  # å®‰å…¨é™åˆ¶
        step_count += 1
        print(f"\nğŸ“ æ­¥éª¤ {step_count}: æ‰§è¡Œå·¥ä½œæµç¨‹")
        print("-" * 40)
        
        try:
            # æ£€æµ‹ä¸­æ–­
            result = await graph.ainvoke(current_input, cast(Any, config))
            
            # æ£€æŸ¥æ˜¯å¦æœ‰ä¸­æ–­
            if hasattr(result, '__contains__') and "__interrupt__" in result:
                print(f"â¸ï¸  æ£€æµ‹åˆ°ä¸­æ–­")
                current_step = result.get("current_step", "unknown")
                print(f"   å½“å‰æ­¥éª¤: {current_step}")
                
                # è‡ªåŠ¨å¤„ç†ä¸­æ–­
                if "outline" in current_step or "confirmation" in current_step:
                    user_response = "yes"
                    print("   ğŸ“‹ è‡ªåŠ¨ç¡®è®¤å¤§çº²")
                elif "rag" in current_step or "knowledge" in current_step:
                    user_response = "skip"  # è·³è¿‡RAGå¢å¼ºä»¥ç®€åŒ–æµç¨‹
                    print("   ğŸ§  è·³è¿‡RAGå¢å¼º")
                elif "search" in current_step:
                    user_response = "yes"
                    print("   ğŸ” è‡ªåŠ¨åŒæ„æœç´¢")
                else:
                    user_response = "yes"
                    print(f"   â¡ï¸  è‡ªåŠ¨ç»§ç»­ ({current_step})")
                
                print(f"   å“åº”: {user_response}")
                
                # æ¢å¤æ‰§è¡Œå¹¶è·å–æµå¼è¾“å‡º
                print(f"\nğŸŒŠ æ¢å¤æ‰§è¡Œï¼Œå¼€å§‹æµå¼è¾“å‡º...")
                print("-" * 30)
                
                step_tokens = 0
                async for chunk in graph.astream(Command(resume=user_response), cast(Any, config), stream_mode="messages"):
                    if print_streaming_content(chunk):
                        step_tokens += 1
                        total_tokens += 1
                        
                        if total_tokens % 30 == 0:
                            elapsed = time.time() - start_time
                            print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
                
                print(f"\nâœ… æ­¥éª¤å®Œæˆ ({step_tokens} tokens)")
                current_input = Command(resume=user_response)
                
            else:
                # æ²¡æœ‰ä¸­æ–­ï¼Œå·¥ä½œæµç¨‹å®Œæˆ
                print("ğŸ‰ å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆ")
                
                # æœ€ç»ˆæµå¼è¾“å‡º
                print(f"\nğŸŒŠ è·å–æœ€ç»ˆæµå¼è¾“å‡º...")
                print("-" * 30)
                
                final_tokens = 0
                async for chunk in graph.astream(current_input, cast(Any, config), stream_mode="messages"):
                    if print_streaming_content(chunk):
                        final_tokens += 1
                        total_tokens += 1
                        
                        if total_tokens % 30 == 0:
                            elapsed = time.time() - start_time
                            print(f"\n[âš¡ {total_tokens} tokens, {elapsed:.1f}s]", end="", flush=True)
                
                if final_tokens > 0:
                    print(f"\nâœ… æœ€ç»ˆè¾“å‡ºå®Œæˆ ({final_tokens} tokens)")
                
                print(f"\nğŸ å®Œæ•´æµç¨‹ç»“æŸï¼æ€»è®¡ {total_tokens} tokens")
                break
                
        except Exception as e:
            if "interrupt" in str(e).lower():
                print(f"â¸ï¸  é€šè¿‡å¼‚å¸¸æ£€æµ‹åˆ°ä¸­æ–­: {str(e)[:100]}")
                user_response = "yes"
                current_input = Command(resume=user_response)
                continue
            else:
                print(f"âŒ æ­¥éª¤ {step_count} å¤±è´¥: {e}")
                break
    
    # è·å–æœ€ç»ˆç»“æœ
    try:
        final_result = await graph.ainvoke(initial_state, cast(Any, config))
        return final_result
    except:
        return None


async def test_streaming_writing_workflow():
    """
    æµ‹è¯•å®Œæ•´çš„æµå¼å†™ä½œå·¥ä½œæµç¨‹
    """
    print("ğŸ­ LangGraph æµå¼å†™ä½œåŠ©æ‰‹æµ‹è¯•")
    print("ğŸ¯ ç›®æ ‡ï¼šéªŒè¯å®Œæ•´å·¥ä½œæµçš„çœŸæ­£æµå¼è¾“å‡ºæ•ˆæœ")
    

    load_knowledge_bases()
    print("âœ… çŸ¥è¯†åº“åˆå§‹åŒ–æˆåŠŸ")
    
    graph = create_writing_assistant_graph()

    config: Dict[str, Any] = {"configurable": {"thread_id": "streaming_test_001"}}
    
    start_time = time.time()
        
    final_result = await handle_workflow_interrupts(graph, config)
        
    total_time = time.time() - start_time
        
    print(f"\n" + "="*60)
    print(f"ğŸ“Š æµå¼å†™ä½œå·¥ä½œæµç¨‹æµ‹è¯•ç»“æœ:")
    print(f"   æ€»æ‰§è¡Œæ—¶é—´: {total_time:.3f}s")
        
    if final_result:
        # åˆ†ææœ€ç»ˆç»“æœ
        article = final_result.get("article", "")
        word_count = final_result.get("word_count", 0)
        outline = final_result.get("outline", {})
        search_results = final_result.get("search_results", [])
        rag_status = final_result.get("rag_enhancement", "")
            
        print(f"\nğŸ“‹ å·¥ä½œæµç¨‹å®Œæˆåº¦æ£€æŸ¥:")
        print(f"   âœ… å¤§çº²ç”Ÿæˆ: {'å®Œæˆ' if outline else 'æœªå®Œæˆ'}")
        print(f"   âœ… æœç´¢åŠŸèƒ½: {'å®Œæˆ' if search_results else 'è·³è¿‡'} ({len(search_results)}ä¸ªç»“æœ)")
        print(f"   âœ… RAGå¢å¼º: {rag_status if rag_status else 'æœªæ‰§è¡Œ'}")
        print(f"   âœ… æ–‡ç« ç”Ÿæˆ: {'å®Œæˆ' if article else 'æœªå®Œæˆ'}")
            
        if article:
            print(f"\nğŸ“„ ç”Ÿæˆæ–‡ç« ä¿¡æ¯:")
            print(f"   å­—æ•°: {word_count}å­—")
            print(f"   é•¿åº¦: {len(article)}å­—ç¬¦")
                
            print(f"\nğŸ“– æ–‡ç« é¢„è§ˆ:")
            print("-" * 40)
            preview = article[:200] + "..." if len(article) > 200 else article
            print(preview)
            print("-" * 40)
            
            # æµå¼æ•ˆæœè¯„ä¼°
    if total_time > 0:
            if word_count > 0:
                chars_per_second = len(article) / total_time
                print(f"\nâš¡ æµå¼æ•ˆæœè¯„ä¼°:")
                print(f"   å¹³å‡è¾“å‡ºé€Ÿåº¦: {chars_per_second:.1f} å­—ç¬¦/ç§’")
                
                if chars_per_second > 10:
                    print("   ğŸ‰ æµå¼æ•ˆæœ: ä¼˜ç§€ï¼ˆå®æ—¶å“åº”ï¼‰")
                elif chars_per_second > 5:
                    print("   âœ… æµå¼æ•ˆæœ: è‰¯å¥½ï¼ˆæµç•…è¾“å‡ºï¼‰")
                else:
                    print("   âš ï¸ æµå¼æ•ˆæœ: ä¸€èˆ¬ï¼ˆå¯èƒ½æœ‰å»¶è¿Ÿï¼‰")
    
    return {"success": True, "total_time": total_time, "final_result": final_result}


async def main():
    """ä¸»æµ‹è¯•å‡½æ•° - ä½¿ç”¨å®Œæ•´æµå¼å·¥ä½œæµç¨‹"""
    print("ğŸŒŠ å¼€å§‹æµå¼å†™ä½œåŠ©æ‰‹å®Œæ•´å·¥ä½œæµç¨‹æµ‹è¯•")
    print("ğŸ¯ æµ‹è¯•ç›®æ ‡ï¼šå¤§çº²â†’ç¡®è®¤â†’RAGâ†’æœç´¢â†’æ–‡ç« ç”Ÿæˆ(æµå¼)")
    
    # ç›´æ¥ä½¿ç”¨å®Œæ•´çš„æµå¼å·¥ä½œæµç¨‹
    result = await complete_streaming_workflow()
    
    print(f"\nğŸ’¡ æµ‹è¯•æ€»ç»“:")
    print("=" * 60)
    
    if result and result.get("success"):
        print("ğŸ‰ æµå¼å†™ä½œå·¥ä½œæµç¨‹æµ‹è¯•æˆåŠŸ!")
        print(f"â±ï¸ æ€»ç”¨æ—¶: {result.get('total_time', 0):.3f}ç§’")
        print(f"ğŸ“Š æ€»Tokenæ•°: {result.get('total_tokens', 0)}")
        print(f"ğŸ”„ æ€»æ­¥éª¤æ•°: {result.get('steps', 0)}")
        print("ğŸŒŠ æµå¼è¾“å‡ºæ•ˆæœ: å·²éªŒè¯")
        print("ğŸ“ å®Œæ•´å·¥ä½œæµç¨‹: å·²å®Œæˆ")
        
        # æ˜¾ç¤ºæœ€ç»ˆç»“æœä¿¡æ¯
        final_result = result.get('final_result')
        if final_result:
            article = final_result.get("article", "")
            if article:
                word_count = final_result.get("word_count", 0)
                print(f"\nğŸ“„ ç”Ÿæˆæ–‡ç« ä¿¡æ¯:")
                print(f"   å­—æ•°: {word_count}å­—")
                print(f"   å­—ç¬¦æ•°: {len(article)}")
                
                print(f"\nğŸ“– æ–‡ç« é¢„è§ˆ:")
                print("-" * 40)
                preview = article[:200] + "..." if len(article) > 200 else article
                print(preview)
                print("-" * 40)
    else:
        error = result.get('error', 'æœªçŸ¥é”™è¯¯') if result else 'æµ‹è¯•å¤±è´¥'
        print(f"âŒ æµå¼å·¥ä½œæµç¨‹æµ‹è¯•å¤±è´¥: {error}")
    
    print(f"\nğŸ¯ å®Œæ•´æµå¼å·¥ä½œæµç¨‹æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())